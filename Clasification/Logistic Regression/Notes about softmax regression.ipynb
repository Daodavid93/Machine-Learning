{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes about softmax regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax regression aslo known as Mutinomial Logistic,Maximum entropy Clasifier, or just Multi-class logistic regression is a generelazation of logistic regression that we can use for multi-class classification(under the assumption that the classes are mutualy exclusive).<br>\n",
    "<br>\n",
    "In softmax regression we replace sigmoid function by so-called softmax function $\\phi_{softmax}(.).$ <br> <br>\n",
    "$$ P(y=j|z^{(i)}) = \\phi_{softmax}(z^{(i)}) = \\frac{e^{z^{(i)}}} {\\sum_i^k e^{z_k^{(i)}}}$$ \n",
    "<br> <br>\n",
    "Where we difine the net input as $$ z = w_1 x_1 + ...+w_nx_n +b = \\sum_l^mw_lx_l +b = W^T +b$$\n",
    "<br>\n",
    "(W is the weight vector, X is is the feature of 1 training example, and b is bias unit )\n",
    "<br>\n",
    "\n",
    "Now,softmax function computes the probability that this training example $x^{(i)}$ belongs to class j given the weight and net input $z^{(i)}$. <br>\n",
    "So we compute the probability $p(y=j | x^{(i)}) =;W_j$)$ for each clas label in j=1,...,k..Note the normalization term in the denominator which causes these class probabilities to sum up one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ilustrate the concept of softmax, let us walk through a concrete example. let's assume we have a training set consisting of 4 samples from 3 different classes (0, 1 and 2)\n",
    "* $x_0 \\rightarrow class \\; 0 $ \n",
    "* $x_1 \\rightarrow class \\; 1 $ \n",
    "* $x_2 \\rightarrow class \\; 2 $ \n",
    "* $x_3 \\rightarrow class \\; 2 $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " y = np.array([0, 1, 2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to encode the class label into a format that we can more easily work with; we apply one-hot encoding :\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one hot encoding\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_enc = (np.arange(np.max(y)+1) == y [: , None]).astype(float)\n",
    "print('one hot encoding')\n",
    "y_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " (np.array([1,2])==np.array([3,2])).astype(float)   ###### see\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample that belongs to class 0 (the first row) has a 1 in the first cell, a sampe that belong to class 2 has 1 in the second row, and so forth.\n",
    "<br>\n",
    "Next, let us define the feature of our 4 trainning samples.Here, we assume that our dataset consist of 2 features;thus, we create (4,2) dimensinal matrix of our samples and features. Similarly, we create a (2,3) dimensinal weigh matrix (one row per feature and one column per each class) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs X:\n",
      " [[ 0.1  0.5]\n",
      " [ 1.1  2.3]\n",
      " [-1.1 -2.3]\n",
      " [-1.5 -2.5]]\n",
      "\n",
      "Weights W:\n",
      " [[0.1 0.2 0.3]\n",
      " [0.1 0.2 0.3]]\n",
      "\n",
      "bias:\n",
      " [0.01 0.1  0.1 ]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0.1, 0.5],\n",
    "              [1.1, 2.3],\n",
    "              [-1.1, -2.3],\n",
    "              [-1.5, -2.5]])\n",
    "\n",
    "W = np.array([[0.1, 0.2, 0.3],\n",
    "              [0.1, 0.2, 0.3]])\n",
    "\n",
    "bias = np.array([0.01, 0.1, 0.1])\n",
    "\n",
    "print('Inputs X:\\n', X)\n",
    "print('\\nWeights W:\\n', W)\n",
    "print('\\nbias:\\n', bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the net input, we mutiply feature vector with weight vector with the $2\\times 3$ (n_feature,n_classes) weight matrix W, wich yields a $4\\times 3 $ matrix (n_samples,n_classes) to wich we then add the bias unit.\n",
    "$$ Z = XW +b $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs X:\n",
      " [[ 0.1  0.5]\n",
      " [ 1.1  2.3]\n",
      " [-1.1 -2.3]\n",
      " [-1.5 -2.5]]\n",
      "\n",
      "Weights W:\n",
      " [[0.1 0.2 0.3]\n",
      " [0.1 0.2 0.3]]\n",
      "\n",
      "bias:\n",
      " [0.01 0.1  0.1 ]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0.1, 0.5],\n",
    "              [1.1, 2.3],\n",
    "              [-1.1, -2.3],\n",
    "              [-1.5, -2.5]])\n",
    "\n",
    "W = np.array([[0.1, 0.2, 0.3],\n",
    "              [0.1, 0.2, 0.3]])\n",
    "\n",
    "bias = np.array([0.01, 0.1, 0.1])\n",
    "\n",
    "print('Inputs X:\\n', X)\n",
    "print('\\nWeights W:\\n', W)\n",
    "print('\\nbias:\\n', bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net input:\n",
      " [[ 0.07  0.22  0.28]\n",
      " [ 0.35  0.78  1.12]\n",
      " [-0.33 -0.58 -0.92]\n",
      " [-0.39 -0.7  -1.1 ]]\n"
     ]
    }
   ],
   "source": [
    "def net_input(X, W, b):\n",
    "    return (X.dot(W) + b)\n",
    "\n",
    "net_in = net_input(X, W, bias)\n",
    "print('net input:\\n', net_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time to compute the softmax activation that we discussed earlier : <br> <br>\n",
    " $$ P(\\; y=j\\;|\\;z^{(i)}) = \\phi_{softmax}(z^{(i)}) = \\frac{e^{z^{(i)}}} {\\sum_i^k e^{z_k^{(i)}}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax:\n",
      " [[0.29450637 0.34216758 0.36332605]\n",
      " [0.21290077 0.32728332 0.45981591]\n",
      " [0.42860913 0.33380113 0.23758974]\n",
      " [0.44941979 0.32962558 0.22095463]]\n"
     ]
    }
   ],
   "source": [
    "def softmax(z):\n",
    "    return (np.exp(z.T) / np.sum(np.exp(z), axis=1)).T\n",
    "\n",
    "smax = softmax(net_in)\n",
    "print('softmax:\\n', smax)\n",
    "# def sofmax(x,Z,intercept):\n",
    "#     exp_K = np.exp(x.dot(Z.T)+intercept)\n",
    "#     sums=np.sum(exp_K, axis=1) # array contains sum of exp if every row\n",
    "#     result = (exp_K.T/sums).T  #return for every element e^{k_ij}/(e^{k_i1}+e^{k_i2}+e^{k_i3})\n",
    "#     return result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the values for each sample (row) nicely sum up 1 now.E.g. , we can say the first tsample\n",
    "$[0.29450637 0.34216758 0.36332605]$ has a $29.45 % $ probability to belong to class 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted class labels:  [2 2 0 0]\n"
     ]
    }
   ],
   "source": [
    "def to_classlabel(z):\n",
    "    return z.argmax(axis=1) #return the index of max number of every row  in list\n",
    "\n",
    "print('predicted class labels: ', to_classlabel(smax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted class labels:  [2 2 0 0]\n"
     ]
    }
   ],
   "source": [
    "def to_classlabel(z):\n",
    "    return z.argmax(axis=1)\n",
    "\n",
    "print('predicted class labels: ', to_classlabel(smax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our prediction are terribly wrong the correct class labels are $[0,1,2,2]$.Now, in order to train out logistic model(e.g via the optimization algorithm such as gradient descent), we need to cost function &J& that we want to minimize : <br> <br>\n",
    "$$J(W:b) = \\frac{1}{n}\\sum_i^nH(T_i,O_i), $$\n",
    "Here the T stands for 'target' i.e the true class labels. <br>\n",
    "\n",
    "$$ \\nabla w_j J(W;b) = \\frac{1}{n}\\sum_i^ a$$\n",
    "http://rasbt.github.io/mlxtend/user_guide/classifier/SoftmaxRegression/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in order to learn our sofmax model -- determening the weigth coefficient --via gradient descent, we then need to compute the derivative <br> <br>\n",
    "   $$\\nabla w_j J(W;b) $$\n",
    "   \n",
    "   \n",
    "   \n",
    "   final result look like this :\n",
    "   <br> <br>\n",
    "   \n",
    "   $$\\nabla w_j J(W;b) = frac{1}{n} \\sum_i=0^n[x(i)(O_i-T_i)] $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
