{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step by Step Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This note book is fucused in Binary logistic regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    <font color='black'>\n",
    "       The logistic regression starts with the assumption of the following equation.\n",
    "    </font>\n",
    "  \n",
    "  <font color='#3333cc'>\n",
    "  $$(1) \\; \\;  log(\\frac{p}{1-p}) = \\sum_i^n \\theta_n x_n + ... +\\theta_1 x_1 +\\theta_0$$\n",
    "  </font>\n",
    "That is the base point.What does this equation say ? <br> <br>\n",
    "The independent variables (feature variables) are linearly related to log-odds of the probability of some classification variable to be equal to 1.<br> <br>\n",
    "The main important properties of the log-odds function $F(p)=log(\\frac{p}{1-p})$ : <br> <br>\n",
    "<font color='#3333cc'>\n",
    "  $$1)\\; p \\in [0,1] ;(because\\; that\\; is\\; probability)$$ <br>\n",
    "  $$2)\\; F(p) \\in [-\\infty,-\\infty,] \\;(that\\; makes \\;it\\; capable\\; to\\; be\\; fited\\; to\\; linear\\; function )$$<br>\n",
    "  $$3) The\\;  probability\\;  p \\; refers \\;to \\;the \\;label(target) \\;variable\\; to \\;be\\; eaqual\\; to\\; 1$$ \n",
    "  </font> \n",
    "  <br>\n",
    " </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Taking the exponent of eq(1) followed by some basic algebraic handling we can achieve the :\n",
    " <font color='#3333cc'>\n",
    "  $$(2) h_{\\Theta}(X.\\Theta) =\\sigma(z) = \\frac{1}{1 +e^{-z}}  \\; where  \\; z= \\sum_i^n \\theta_n x_n + ... +\\theta_1 x_1 +\\theta_0\\;  $$  \n",
    "  </font>\n",
    "The eq(2) is called sigmoid function. $X$ is the feature vector $\\Theta$ is our estimator vector.This function is the hypothesis function, it is used in Logistic regression for predicting the classification variable.<br> <br>\n",
    "The l-regression deals with finding the best estimators parameters  $\\Theta(\\theta) = [\\theta_n x_n ,... ,\\theta_1 x_1 ,\\theta_0]$ by fitting it according to a given data set.\n",
    "</h4>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> \n",
    "    More details about logg-odds can be found here <a href=\"https://daodavid93.github.io/Machine-Learning/source/html/ML/logistic-regression/Cross-entropy%20function.Investigation%20and%20gradient%20descent.html\">Origin of Sigmoid</a>\n",
    "</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    " We need a loss function that expresses ,for an observation x ,how close the classifier output ($\\hat{y}=\\sigma(\\theta_i.x_i)$) is to the correct function output (which is 1 ,0 ).We will call this : <br> <br>\n",
    "    $$ L(\\hat{y},y)= How\\;much\\;\\hat{y}\\;is\\;different\\;from \\;true \\; y$$ <br>\n",
    "    We do this via a loss function that prefers the correct class labels of the training data exaples to be more quickly.<br> <br>\n",
    "    \n",
    "In order to be fitting the sigmoid function, the cost function is not  R-squared which is used in Linear regression, because the sigmoid is very complex and non-convex, contains many extrema. The cost function which is used is the so-called Cross-entropy function<br> <br>\n",
    "\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "  <font color='#3333cc'>\n",
    "       $$ \\;(3)\\; L(\\hat{y},y)= J(\\Theta) = \\sum_i^m Cost(h_{\\Theta}(x_i),y_i)=-\\frac{1}{m}\\big(\\sum_i^my_i.log(h_{\\Theta}(x_i)) + (y_i-1)log(1 - h_{\\Theta}(x_i))\\big)$$\n",
    "   </font>\n",
    "   <br>\n",
    "  For the optimization problem, we will use the well-known algorithm <a href=\"https://daodavid93.github.io/Machine-Learning/source/html/Linear-Algebra/gradient%20descent.html\">Gradient Descent</a>.<br> <br>\n",
    "  Applying Gradient descent into Cross-entropy function we achieve the folowing equations : \n",
    "  <br> \n",
    "</h4>  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "   \n",
    "  <font color='#3333cc'> \n",
    "      $$(4) \\; \\theta_i = \\theta_i - \\frac{1}{m}\\sum_i^m \\big( h_{\\Theta}(z) - y_i\\big )x_i = $$\n",
    "      $$\\theta_i - \\frac{1}{m}\\sum_i^m \\big( h_{\\Theta}(\\theta_n x_n + ... +\\theta_1 x_1 +\\theta_0) - y_i\\big )x_i $$\n",
    "  </font>\n",
    "</h1>  \n",
    "\n",
    "<h4>\n",
    "   for intersept $\\theta_0$\n",
    "  <font color='#3333cc'> \n",
    "      $$(5) \\;\\theta_0 = \\theta_0 - \\frac{1}{m}\\sum_i^m \\big( h_{\\Theta}(z) - y_i\\big )$$\n",
    "  </font>\n",
    "  <br>\n",
    " We will try to implement step by step the above eqations,In order to opimize (3) and to fited the best estimators <br>  <br>\n",
    " $$\\vec{\\Theta}(\\theta)= [\\theta_n,\\theta_{n-1},..,\\theta_1,\\theta_0]$$ <br>  by given dataset\n",
    "</h4>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### There is more mathematical details and proofs about cross-entropy and gradient descent here  : <a href=\"https://daodavid93.github.io/Machine-Learning/source/html/ML/logistic-regression/Cross-entropy%20function.Investigation%20and%20gradient%20descent.html\">Investigation of Cross-entropy function. Math proof of formula of gradient descent over Cross-entropy. Math–µmaticaly resolved </a>.<br> <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X, theta, intercept = 0 ):\n",
    "    \"\"\"\n",
    "    Target vector X,X can be the matrix of many vectors and numer as well\n",
    "    theta is an estimator vector\n",
    "    intercept is theta zero elemt\n",
    "    \n",
    "    \"\"\"\n",
    "    #convertions to ndarray\n",
    "    X = np.array(X)\n",
    "    theta = np.array(theta)\n",
    "    z = X.dot(theta.T)+intercept # scallar product : <X|theta^(-1)> + intercept\n",
    "    return 1/(1+np.exp(-z)) #sigmoid transformation of z\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1,2,3],\n",
    "         [2,2,3]])\n",
    "#x = np.array([1,2,3])\n",
    "theta = np.array([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lost(arg, y_target, x_i=1):\n",
    "    \"\"\"\n",
    "    takes arg ,that is the result of sigmoid it has to be array\n",
    "    y_target label variable which is 1 or 0\n",
    "    x_i lement is every i element from X vectors in one array [x[i][j]] j is constant refer to column related to j_estimator\n",
    "    \"\"\"\n",
    "    y = np.array(y_target)\n",
    "    x_i = np.array(x_i)\n",
    "    return (arg-y)*x_i\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(X, estimators, Y_label, intecept, x_i=1):\n",
    "    \"\"\"\n",
    "    takes:\n",
    "    X is Target vectors \n",
    "    estimators are our fitin parametes theta_i\n",
    "    Y_label is our target values zero or one\n",
    "    x_i is the i_th element (column) of target element related to Theta i_th estimator\n",
    "    \"\"\"\n",
    "    m = Y_label.shape[0]\n",
    "    n = np.array(x_i).shape[0]\n",
    "    if m != n :\n",
    "        print(n)\n",
    "        raise ValueError('x_i and Y_label must have same shape')\n",
    "    sigmoid_result = sigmoid(X, estimators, intecept)    \n",
    "    result = lost(sigmoid_result, Y_label, x_i)\n",
    "    return result.sum()   \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "#cost([[1,2],[1,3]],[3,2],np.array([1,0]),1,1)\n",
    "n = np.array(1)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X_data, Y_label, times_interaction=100, learning_rate=0.1, init_value=10):\n",
    "    x = np.array(X_data)\n",
    "    y_l = np.array(Y_label)\n",
    "    n = x.shape[1]\n",
    "    m = x.shape[0]\n",
    "    intercept = init_value\n",
    "    esimators = np.full(n, init_value)\n",
    "    \n",
    "    for i in range(times_interaction):\n",
    "        for i in range(len(esimators)):\n",
    "            esimators[i]-=cost(x, esimators, y_l, intercept, x[:,i] )\n",
    "            \n",
    "        intercept -=cost(x, esimators, y_l, intercept, np.full(m, 1) )     \n",
    "        \n",
    "    \n",
    "    print(\"coef:\",esimators)\n",
    "    print(\"intercept:\",intercept)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coef: [ 0 -2]\n",
      "intercept: 5.846070847119247\n"
     ]
    }
   ],
   "source": [
    "gradient_descent([[1,2],[2,3]], [1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let to generate out training data from back to front : \n",
    "1) We should define the training data taking into acout that the variables will be linear dependent to one hidden variable,wich will the argument of sigmoid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let to difine $z = a*x + b$ where $a=2$ and $b=4$ and use to define some sequence ,find the out from sigmoid, \n",
    "\n",
    "Then out main purpose will be to find a,b from back to front,and to see is approximatlyu equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#define z argument as lininear euation of z = 2*x + 4\n",
    "z_f = lambda x : 2*x+4\n",
    "x = np.linspace(-10,10,30)\n",
    "\n",
    "z_args = np.array([z_f(i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_prime = np.array( 1/(1+np.exp(z_args)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30,)\n",
      "(30,)\n",
      "(30,)\n"
     ]
    }
   ],
   "source": [
    "print(y_prime.shape)\n",
    "print(z_args.shape)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate labeled  from already define y_prime data given the sigmoid with args z = 2*x+4\n",
    "def generated_label(i):\n",
    "        if i < 0.5 :\n",
    "            return 0\n",
    "        elif i > 0.5 :\n",
    "            return 1\n",
    "        else :\n",
    "            return np.random.randint(0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_label = np.array([generated_label(i) for i in y_prime])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
