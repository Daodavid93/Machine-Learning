{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step by Step Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let to define step by step what must happen and why logistic works before finding our optimization function. Firstly optimization function will be the well-know  Gradient descent although, that is not the best choice one\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let to generate out training data from back to front : \n",
    "1) We should define the training data taking into acout that the variables will be linear dependent to one hidden variable,wich will the argument of sigmoid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let to difine $z = a*x + b$ where $a=2$ and $b=4$ and use to define some sequence ,find the out from sigmoid, \n",
    "\n",
    "Then out main purpose will be to find a,b from back to front,and to see is approximatlyu equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#define z argument as lininear euation of z = 2*x + 4\n",
    "z_f = lambda x : 2*x+4\n",
    "x = np.linspace(-10,10,30)\n",
    "\n",
    "z_args = np.array([z_f(i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    result = [1/(1+np.exp(-i)) for i in z ]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_prime = np.array( sigmoid(z_args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_prime.shape)\n",
    "print(z_args.shape)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate labeled  from already define y_prime data given the sigmoid with args z = 2*x+4\n",
    "def generated_label(i):\n",
    "        if i < 0.5 :\n",
    "            return 0\n",
    "        elif i > 0.5 :\n",
    "            return 1\n",
    "        else :\n",
    "            return np.random.randint(0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_label = np.array([generated_label(i) for i in y_prime])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_prime.shape)\n",
    "print(z_args.shape)\n",
    "print(y_label.shape)\n",
    "data = np.array([z_args,y_label]).T\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = data[:,1] #.reshape(-1, 1)\n",
    "trained_data = data[:,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"a = \",reg.coef_)\n",
    "print(\"b=\",reg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_f = lambda x : 1.47*x+4\n",
    "x = np.linspace(-10,10,30)\n",
    "z_1=  lambda x : 2*x+1.8\n",
    "z_args = np.array([z_1(i) for i in x])\n",
    "y_logi = np.array( sigmoid(z_args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://math.stackexchange.com/questions/477207/derivative-of-cost-function-for-logistic-regression\n",
    "def sigmoid(a,b,x_data):\n",
    "    return 1/(1+np.exp(-(a*x_data+b)))\n",
    "\n",
    "def cost_a(a,b,x_data,y_data):\n",
    "    return (1/len(x_data))*((y_data-sigmoid(a,b,x_data))*x_data).sum()\n",
    "     \n",
    "    \n",
    "def cost_b(a,b,x_data,y_data):\n",
    "    return (1/len(x_data))*((y_data-sigmoid(a,b,x_data))).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_gradient_descent(x_data,y_data,times_interaction=100,learning_rate=0.1):\n",
    "    a = 10\n",
    "    b = 10\n",
    "    a_args=[a]\n",
    "    b_args=[b]\n",
    "    for i in range(times_interaction):\n",
    "        a = a + cost_a(a,b,x_data,y_data)*learning_rate\n",
    "        b = b + cost_a(a,b,x_data,y_data)*learning_rate\n",
    "        a_args=np.append(a_args, a)\n",
    "        b_args=np.append(b_args, b)\n",
    "   \n",
    "    return a,b,a_args,b_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = data[:,1] #.reshape(-1, 1)\n",
    "trained_data = data[:,0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
