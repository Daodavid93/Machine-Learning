{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SoftMax function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "<br>\n",
    "<h7>\n",
    "    <font color='#263a61' > The softmax is a function that takes as input a vector with K real numbers, ant normalizes it into a probability distribution consisting of K probabilities proportional to the exponentials of the input numbers. That is, prior to applying the softmax function, some vector components could be negative, or greater than one and might not sum to one. Futhermore, the larger input components corresponds to larger  probabilities. The softmax function is often used in neural networks to map non normalized output of the network to a probability distribution over predicted output classes.\n",
    "             \n",
    "    \n",
    "   </font>\n",
    "</h7>         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h7>\n",
    "    <font color='#263a61' >\n",
    "        The standart softmax function $\\sigma: \\; \\Re^k \\; \\rightarrow \\; \\Re^k $ is defined by formula : <br>\n",
    "    \n",
    "\n",
    "   </font>    \n",
    "</h7>\n",
    "<h2><font color='#1c5cd9' > $$\\sigma(z)_i= \\frac{e^z}{\\sum_{j=1}^n e^{z}_{j}}$$  \n",
    "     </font> \n",
    "</h2>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h7>\n",
    "    <font color='#263a61' > begining by simple example : <br>\n",
    "        We have the numbers -1, 0, 3, and 5.First, we calculate the denomitator : <br>\n",
    "        $$Denominator = e^{-1} +e^{0}+e^{3}+e^{5} = 169.87$$\n",
    "   </font>\n",
    "</h7>  \n",
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "    <th>x</th>\n",
    "    <th>Numerator $(e^x)$</th>\n",
    "    <th>Probability $(\\frac{e^x}{169.87})$</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>-1</td>\n",
    "    <td>0.368</td>  \t\t\n",
    "    <td> 0.002</td>\n",
    "  </tr>\n",
    "  <tr>  \n",
    "   <td>0</td>\n",
    "    <td>1</td>  \t\t\n",
    "    <td> 0.006</td>\n",
    "  </tr> \n",
    "      <tr>\n",
    "   <td>3</td>\n",
    "    <td>20.09</td>  \t\t\n",
    "    <td> 0.112</td>\n",
    "  </tr>\n",
    "      <tr>\n",
    "   <td>5</td>\n",
    "    <td>148.41</td>  \t\t\n",
    "    <td>0.87</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h7>\n",
    "    <font color='#263a61' >\n",
    "        The Bigger x gives the higher probability.Also, notice that the probabilities all add up to 1, as mentioned before\n",
    "   </font>\n",
    "</h7>             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The softmax regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h7>\n",
    "    <font color='#263a61' >\n",
    "     What is Softmax regression and How is it related to Logistic Regression ?      <br>\n",
    "     Softmax regression also is known as Mutinomial regression, Maximum entropy Clasifier, or just multi-class regression is an generalziation of logistic regression that we can that we can use for multi-class clasification (under the assumption that the classes are mutaataly exclusive).  In contrast, we use the standart Logistic model in binary classification tasks. <br>\n",
    "        \n",
    "   $$P(y=j|z^{(i)}) = \\phi_{sofmax}(z^{(i)})=\\frac{e^{z^{(i)}}}{\\sum_{j=1}^n e^{z^{(i)}}_{j}}$$     \n",
    "   \n",
    "   where we define the net input $z$ as  <br>\n",
    "   $$z = w_0x_0 + w_1x_1 + ... w_mx_m =W^TX   $$\n",
    "    \n",
    "        \n",
    "   </font> \n",
    "</h7>           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## references :\n",
    "<a href = 'http://rasbt.github.io/mlxtend/user_guide/classifier/LogisticRegression/' >\n",
    "    Logistic Regression\n",
    "</a>\n",
    "<br>\n",
    "<a href = 'https://sebastianraschka.com/faq/docs/softmax_regression.html' >\n",
    "    Softmax\n",
    "</a>\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
