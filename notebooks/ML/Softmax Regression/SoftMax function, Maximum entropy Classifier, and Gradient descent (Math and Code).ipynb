{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax function,Cross Entropy Loss and Gradient descent.\n",
    "author: daodeiv (david stankov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>\n",
    "    <font color=' #263a61 ' >\n",
    "        The standart softmax function $\\sigma: \\; \\Re^k \\; \\rightarrow \\; \\Re^k $ is defined by formula : <br>\n",
    "    \n",
    "\n",
    "   </font>    \n",
    "</h5>\n",
    "<h2><font color='#1c5cd9' > $$\\sigma(z)_i= \\frac{e^z_i}{\\sum_{j=1}^n e^{z}_{j}}$$  \n",
    "     </font> \n",
    "</h2>\n",
    "<h7>\n",
    "    <font color='#263a61' >Before getting deeper into the above equation, we gonna generate our learning data. The data consist of records contains a different type of vehicles together their sizes. The dataset is very simple since we would be able to do prediction per our brain, without using any ML concept however in the beginning, our purpose is not to make a prediction our purpose is diving into the basic concepts behind softmax underlying on math.\n",
    "    \n",
    "   </font>\n",
    "</h7>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>\n",
    "    <font color=' #263a61 ' >\n",
    "        Let's generate our data : <br>\n",
    "    \n",
    "\n",
    "   </font>    \n",
    "</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_size</th>\n",
       "      <th>y_size</th>\n",
       "      <th>z_size</th>\n",
       "      <th>label_Bus</th>\n",
       "      <th>label_Car</th>\n",
       "      <th>label_Tractor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.203818</td>\n",
       "      <td>4.666909</td>\n",
       "      <td>4.086120</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.788860</td>\n",
       "      <td>4.522800</td>\n",
       "      <td>4.893825</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.587386</td>\n",
       "      <td>4.920355</td>\n",
       "      <td>4.176234</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.555912</td>\n",
       "      <td>4.420434</td>\n",
       "      <td>4.818734</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.923830</td>\n",
       "      <td>4.078491</td>\n",
       "      <td>4.329376</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.412069</td>\n",
       "      <td>1.273014</td>\n",
       "      <td>1.046412</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.599903</td>\n",
       "      <td>1.766137</td>\n",
       "      <td>1.354414</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.462978</td>\n",
       "      <td>1.003001</td>\n",
       "      <td>1.094916</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.110654</td>\n",
       "      <td>1.714280</td>\n",
       "      <td>1.715052</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.233482</td>\n",
       "      <td>1.348724</td>\n",
       "      <td>1.467063</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      x_size    y_size    z_size  label_Bus  label_Car  label_Tractor\n",
       "0   4.203818  4.666909  4.086120          0          0              1\n",
       "1   4.788860  4.522800  4.893825          0          0              1\n",
       "2   4.587386  4.920355  4.176234          0          0              1\n",
       "3   4.555912  4.420434  4.818734          0          0              1\n",
       "4   4.923830  4.078491  4.329376          0          0              1\n",
       "..       ...       ...       ...        ...        ...            ...\n",
       "25  1.412069  1.273014  1.046412          0          1              0\n",
       "26  1.599903  1.766137  1.354414          0          1              0\n",
       "27  1.462978  1.003001  1.094916          0          1              0\n",
       "28  1.110654  1.714280  1.715052          0          1              0\n",
       "29  1.233482  1.348724  1.467063          0          1              0\n",
       "\n",
       "[90 rows x 6 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generate tractor records with random size between [4,5] \n",
    "tractor_dataframe= pd.DataFrame(data=np.random.random((30, 3))+4,columns = ['x_size','y_size','z_size'])\n",
    "tractor_dataframe['label'] =pd.DataFrame( np.full((1,30 ), 'Tractor').T)\n",
    "\n",
    "#generate car records with random size between [3,4] \n",
    "car_dataframe= pd.DataFrame(data=np.random.random((30, 3)) + 1,columns = ['x_size','y_size','z_size'])\n",
    "car_dataframe['label'] =pd.DataFrame( np.full((1,30 ), 'Car').T)\n",
    "\n",
    "#generate bus records with random size between [2,3] \n",
    "bus_dataframe= pd.DataFrame(data=np.random.random((30, 3))+2,columns = ['x_size','y_size','z_size'])\n",
    "bus_dataframe['label'] =pd.DataFrame( np.full((1,30 ), 'Bus').T)\n",
    "\n",
    "# joint each data frame into  one\n",
    "data = tractor_dataframe.append(bus_dataframe).append(car_dataframe)\n",
    "data = pd.get_dummies(data) \n",
    "data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x21460304e48>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>\n",
    "    <br>\n",
    "    <font color='#454214' >\n",
    "       The above scatter plot shows the simplicity of the classification problem because it is obvious how they are separated into classes. <br> <br>\n",
    "     </font>    \n",
    "   <br>\n",
    "</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 3 record of x train [[4.87703442 4.08854767 4.71603313]\n",
      " [1.77165227 1.16707245 1.71898848]\n",
      " [4.77579936 4.24984133 4.01672021]\n",
      " [2.1497537  2.39400013 2.04626256]]\n",
      "first 3 record of y label  [[0 0 1]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n",
      "shape of x train (n.m) (60, 3)\n",
      "shape of y label (n,k) (60, 3)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.array(data.drop(['label_Bus','label_Car','label_Tractor'], axis = 1)) # gets the target label variables\n",
    "y_train = np.array(data[['label_Bus','label_Car','label_Tractor']]) # gets feature variables \n",
    "X_train, X_test, y_train, y_test = train_test_split(x_train,y_train, test_size=0.33, random_state=42) #separats into test and train samples \n",
    "print('first 3 record of x train', X_train[ : 4] )\n",
    "print('first 3 record of y label ', y_train[ : 4] )\n",
    "\n",
    "print('shape of x train (n.m)', X_train.shape)\n",
    "print('shape of y label (n,k)', y_train.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>\n",
    "    <br> <br> <br>\n",
    "    <font color='#053157'> $\\; \\; \\; $In softmax regression we replace sigmoid function by so-called softmax function \n",
    "          $\\phi_{softmax (.)}$. Where we difine the net input as :  \n",
    "          <br> <br>\n",
    "   </font>\n",
    "   <font color='#3b051d' >\n",
    "      $$ z = w_1 x_1 + ...+w_nx_n +b = \\sum_l^mw_lx_l +b = W^T +b$$\n",
    "   </font>\n",
    "   <br> <br>\n",
    "   <font color='#053b3b'>\n",
    "     (W is the weight vector, X is is the feature of 1 training example, and b is bias unit ) <br> <br>\n",
    "    \n",
    "Now,softmax function computes the probability that this training example $x^{(i)}$ belongs to class j given the weight and net input $z^{(i)}$. <br>\n",
    "So we compute the probability $p(y=j | x^{(i)};W_j)$ for each class label in j=1,...,k..Note the normalization term in the denominator which causes these class probabilities to sum up one. <br>\n",
    "<br> <br> \n",
    "Our training sample consist of $(n_{samples} ,m_{features})$ = (60,3) and label consist of  are ( k_{classes} ) =  (3)\n",
    "therefore  the weight matrix is $m\\times k = 9$ or  $(m_{features},k_{classes}(probabilities) = (3 ,3)$ dimensional matrix.  \n",
    "\n",
    "To compute the net input $z$, we mutiply feature matrix with weight matrix  $(n_{samples} ,m_{features})\\times (m_{features} ,k_{classes})\\; (60,3)\\times(3,3) $ wich yields a $60 \\times 3 $ to wich we then add the bias unit.Mathematicaly look like this:\n",
    " </font> \n",
    "</h5>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h5>\n",
    "    <font color='#3f450f' > weight vector : </font>\n",
    "  <font color='#053b32' > \n",
    "     <br>  \n",
    "    $$ W= \\begin{bmatrix} weight_1\\rightarrow class \\; 1(bus) \\\\ weight_2\\rightarrow class\\;  2(car) \\;   \\\\ weight_3\\rightarrow class \\;3(tractor) \\;  \\end{bmatrix} =\n",
    "    \\begin{bmatrix} \\vec w_1 \\\\  \\vec w_2\\  \\\\ \\vec w_3  \\end{bmatrix} =\\begin{bmatrix} w_{11} & w_{12} & w_{13} \\\\ w_{21} & w_{22} & w_{23} \\\\ w_{31} & w_{32} & w_{33} \\end{bmatrix}  $$ \n",
    "   <br>\n",
    "    \n",
    "\n",
    "   \n",
    " </font>\n",
    "<h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h5>\n",
    "    <font color='#454214' > net input $Z$ vector : </font>\n",
    "  <font color='#454214' > \n",
    "     $$ Z = \n",
    "    \\begin{bmatrix} x_{11} &  x_{12} & x_{13} \\\\  x_{21} &  x_{22} & x_{23} \\\\ ... & ... & ... \\\\ x_{n1} &  x_{n2} & x_{n3} \\end{bmatrix} \\times \\begin{bmatrix} w_{11} & w_{21} & w_{31}  \\\\ w_{12} & w_{22} & w_{32} \\\\  w_{13} & w_{23} & w_{33}\\end{bmatrix} + \\begin{bmatrix}  b_1 \\\\  b_2\\  \\\\  b_3  \\end{bmatrix}  =   \\begin{bmatrix} z_{11} &  z_{12} & z_{13} \\\\  z_{21} &  z_{32} & z_{33} \\\\ ... & ... & ...  \\\\ z_{n1} &  z_{n2} & z_{n3}  \\end{bmatrix} $$ <br>\n",
    "        \n",
    "   <br>\n",
    "       $$z_{ij} = \\sum_l^3 x_{ip} w_{jp} + b_j $$\n",
    "       $$ Z = XW^{T} +b $$\n",
    "    \n",
    "\n",
    "   \n",
    " </font>\n",
    "<h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h7>\n",
    "    <br> <br> <br>\n",
    "    <font color='#053157'> \n",
    "Now, it's time to compute the softmax activation that we discussed earlier : <br> <br>\n",
    " $$ P(\\; y=j\\;|\\;z^{(i)}) = \\phi_{softmax}(z^{(i)}) = \\frac{e^{z^{(i)}}} {\\sum_i^k e^{z_k^{(i)}}} $$\n",
    " <br> <br>\n",
    " \n",
    " \n",
    "$\\; \\; $In softmax regression settings, we are interested in multi-class classification and so the label y can take many values rather then only two. Thus in our training  we have $k_i\\ in {1,2,3}$.\n",
    "Given a test input x, we want our hypotesis to estimate the probability  $P(y=k|x_i)$ (y=k_i), i.e prediction of one label ,accoring to one samle(row) x_i) for each value of $k = {1,2,3}$ i.e we want to estimate the probability of class label taking on each of the $K$ different posible values. Thus, our hypotesis will output a K-dimensianal (3) vectors (whose element sum up to 1) giving us our K estimated probabilities. Concretely, our score(hypotesis) function $f(x,W)$ takes the form : <br>\n",
    "  </font> \n",
    "</h7> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> <br>\n",
    " <h5>\n",
    "   <font color='#454214'>\n",
    "         $$ f(x|W) = \\begin{bmatrix} P(y=1 |x;W) \\\\  P(y=2 |x;W)  \\\\ ... \\\\ P(y=n |x;W)  \\end{bmatrix} = \n",
    " \\frac{1}{\\sum_{j=1}^k exp(W_j^Tx)} = \\begin{bmatrix} exp(W_1^Tx) \\\\  exp(W_2^Tx)  \\\\ ... \\\\ exp(W_n^Tx)  \\end{bmatrix}$$\n",
    "  <br> <br> \n",
    "   </font>\n",
    " </h5>       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <h5>\n",
    " </font>\n",
    " According to our dataset\n",
    "  <font color='#454214'>\n",
    " $$ f(x|W) = P = \\begin{bmatrix} P(y=1 |x_{1j} ; w_{1j}) &  P(y=2 |x_{2j} ; w_{2j})  & P(y=3 |x_{1j} ; w_{3j}) \n",
    "        \\\\ ... &  ...  & ...\n",
    "        \\\\ P(y=1 |x_{nj} ; w_{1j}) &  P(y=2 |x_{2j} ; w_{2j})  & P(y=3 |x_{1j} ; w_{3j})\n",
    "        \\end{bmatrix} = \n",
    "        \\begin{bmatrix} \\frac{e^{z_{11}}}{\\sum_{1j}e^{z_{1j}}} & \\frac{e^{z_{12}}}{\\sum_{1j}e^{z_{1j}}} & \\frac{e^{z_{13}}}{\\sum_{1j}e^{z_{1j}}} \n",
    "        \\\\   \\\\ ... & ... & ...  \\\\\n",
    "        \\\\    \\frac{e^{k_{n1}}}{\\sum_{j}^3e^{z_{nj}}} & \\frac{e^{z_{n2}}}{\\sum_{j}^3e^{z_{nj}}} & \\frac{e^{z_{n3}}}{\\sum_{j}^3e^{z_{nj}}}\\end{bmatrix} $$ \n",
    "  </font> \n",
    "   <h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h6>\n",
    " <br> <br>\n",
    "        <font color='#1a403c' >\n",
    "        The elements of $P$ we will called predictors $p_{ij}$ wich can be interpretated as probability i_th row element\n",
    "        to be j_th categorical value.If $p_{1,2}$ means what is the probability that record one belongs to the label_ bus?\n",
    "        Ofcourse, $p_{ij} \\in [0,1]$ <br>\n",
    "        Another way to express $p_{ij}$ : <br>  <br> \n",
    "        For example, the probability record one $X_1$ from dataset to be from class label $Y_2$ can be calculated in this way :\n",
    "         </font> \n",
    "      <br> \n",
    " </h6>  \n",
    "      \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h3>\n",
    "     <font color='#454214'>\n",
    "        $$softmax(Z)_{12}=P(y=2 |x_{1j};\\; w_{2j})=  \\;p_{12}\\;  = \\frac{ e^{ ^{z_{12}} } }{ \\sum_p^3 e^{z_{1p}}}=\\frac{ e^{ (^{\\sum_v^3 x_{1v}.w_{vj} + b_v }} )}{ \\sum_p^3 e^{ ^ ({\\sum_v^3 x_{1v}.w_{2v}} + b_2})}$$\n",
    "  </font>\n",
    "</h3>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h5>\n",
    " <br> <br>\n",
    "        <font color='#263a61' >\n",
    "         Let to see how an above considering can be applied concretely in our training dataset :\n",
    "             First, let to define a weight vector $W$ <br> <br>\n",
    "            $$W = \\begin{bmatrix} w_{11} & w_{12} & w_{13} \\\\ w_{21} & w_{22} & w_{23} \\\\ w_{31} & w_{32} & w_{33} \\end{bmatrix}  =\n",
    "            \\begin{bmatrix} 0.75776811 & 0.6690284 & -0.02646755 \\\\\n",
    " -2.94775864 &  -3.06439323 & -2.14043833\\\\\n",
    "  2.18999053 & 2.39536483 &  2.16690588 \\end{bmatrix}$$\n",
    "  <br> <br>\n",
    "    $$B = \\begin{bmatrix}  b_1 \\\\  b_2\\  \\\\  b_3  \\end{bmatrix} = \\begin{bmatrix}  -0.15425504 \\\\ 18.81781451 \\\\ -18.66355947 \\end{bmatrix} $$ <br> <br>\n",
    "     I've prepared weight vector $W$ and bias $B$ in advance, how? We will see later. <br>\n",
    "            \n",
    "            \n",
    "          \n",
    " </font>\n",
    "</h5>     \n",
    "="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "W= np.array([[ 0.75776811,  0.6690284 , -0.02646755],\n",
    " [-2.94775864, -3.06439323, -2.14043833],\n",
    " [ 2.18999053 , 2.39536483,  2.16690588]])\n",
    "\n",
    "B = intercept = np.array([ -0.15425504 , 18.81781451, -18.66355947])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sofmax(x,W,intercept):\n",
    "    '''\n",
    "    soft max product \n",
    "    takes :\n",
    "    x feature matrix (train data)\n",
    "    W weight matrix\n",
    "    intercept Bias vector\n",
    "    '''\n",
    "    exp_z = np.exp(x.dot(W.T)+intercept) # calculate each z_ij\n",
    "    sums=np.sum(exp_z, axis=1) # array consist\n",
    "    result = (exp_z.T/sums).T \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original data label Y :  \n",
      "[[0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]]\n",
      "\n",
      "predicted data label Y' :\n",
      "[[0.01 0.99 0.  ]\n",
      " [0.01 0.   0.99]\n",
      " [0.9  0.09 0.  ]\n",
      " [0.91 0.09 0.  ]\n",
      " [0.01 0.   0.99]\n",
      " [0.86 0.13 0.  ]\n",
      " [0.89 0.11 0.  ]\n",
      " [0.01 0.   0.99]\n",
      " [0.03 0.97 0.  ]]\n"
     ]
    }
   ],
   "source": [
    "print('original data label Y :  ')\n",
    "print(y_train[1:10])\n",
    "print('')\n",
    "print(\"predicted data label Y' :\")\n",
    "print(np.around(sofmax(X_train,W,intercept)[1:10],2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h7>\n",
    " <br> <br>\n",
    "        <font color='#263a61' >\n",
    "            From $Y'$ output let to consider the first row : <br> <br>\n",
    "            $Y'_{11} = 0.01 \\rightarrow  $ has  0.1% chance that row one belongs to class 1(BUS) <br> \n",
    "            $Y'_{12} = 0.98 \\rightarrow  $ has 98% chance that row one belongs to class 2(CAR) <br> \n",
    "            $Y'_{13} = 0.01 \\rightarrow  $ has 0.1% chance that row one belongs to class 3(TRACTOR) <br> <br>\n",
    "            That means record $X_1$ belongs to class Car, because has the 90% chance, which is the largest. <br> <br> <br> \n",
    "            \n",
    "            \n",
    " If we compare     $Y$ and $Y'$ we will notice that our prediction has 100% accuracy. <br>\n",
    "But, how have I  found a weight matrix $W$ and bias &B&.I just used the LogisticRegression from scikit-learn, and took the coefficients, but now we gonna define a Loss function ant its optimization in order to find the weght $W$ and bias $\\vec b$. \n",
    "         </font>\n",
    " </h7>        \n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h7>\n",
    " <br> <br>\n",
    "        <font color='#263a61' >\n",
    "        Now we are going to Loss function  for : \n",
    "    </font>\n",
    "</h7>\n",
    "</h3>\n",
    "     <font color='#454214'>\n",
    "            $$(1) \\; \\; \\; softmax(Z)_{ij}=p_{ij}=\\frac{e^{z_{ij}}}{\\sum_l^k e^{z_{il}}} $$ <br>\n",
    "            $$z_{ij} = \\sum_p^3 x_{ip} w_{jp} + b_j $$\n",
    "   </font>\n",
    "   \n",
    "</h3>\n",
    " <h7>\n",
    " <br> <br>\n",
    "        <font color='#263a61' >\n",
    "        The loss function for Softmax regression is called Cross-Entropy Loss or Sofmax Loss and it is defined like that:    \n",
    "    </font>\n",
    "</h7>\n",
    "<h3>\n",
    "     <font color='#454214'>\n",
    "            $$ (2) \\;  \\; \\;L(W,b)=-\\sum_i^n\\sum_j^k y_{ij} \\log (sofmax(Z)_{ij})$$ <br> <br>\n",
    "\n",
    "   </font>\n",
    " </h3>  \n",
    " <h7>\n",
    " <br> <br>\n",
    "        <font color='#4561bf' >\n",
    "       where n is a count of records , $y_{ij}$ is label value of i_th record j_th class, $softmax(Z)_{ij}$ predicited label value  $Y'_{ij}$ <br>\n",
    "     and $W$ is weight matrix and $b$ bias. <br> \n",
    "     Our purpose in to fit parameters  $w_{ij}\\in W$ and $b_j$ according to \n",
    " a training data $X$  and a label data $Y$ in order to make the best prediction.To do that we will apply Gradient descent as a regression technique over Softmax loss function.The eq.(2) is the function of all weights $w_{ij}$, bias $b_j$ all training data x and label data y.       \n",
    "    </font>\n",
    "</h7>   \n",
    "  </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h7>\n",
    " <br> <br>\n",
    "        <font color='#263a61' >\n",
    "            First, We gonna introduce some math technics which will make our work easier, and we will find derivates of some function which will be extremely useful in the process of finding of $\\nabla L(W,b)$. <br> <br>\n",
    "            For simplicity in the summation process of indices, we will introduce a <a href='http://physics.csusb.edu/~prenteln/notes/vc_notes.pdf'>Kronecker symbol</a> .<br>\n",
    "    </font>    \n",
    "</h7> \n",
    "\n",
    " <h3>\n",
    "     <font color='#454214'>\n",
    "       $$\\delta_{ij} =    \n",
    "         \\begin{equation}\n",
    "   \\begin{Bmatrix} \n",
    "   1 & if \\; i=j  \\\\\n",
    "   0 & if \\; i\\ne j  \n",
    "    \\end{Bmatrix} \n",
    "\\end{equation}$$\n",
    "  <br><br>\n",
    "          $$ \\delta_{ij} = \\begin{bmatrix} 1 & 0 & 0  \\\\ 0 & 1 & 0 \\\\  0 & 0 & 1 \\end{bmatrix}$$\n",
    "  </font>\n",
    " </h3>    \n",
    " <h7>\n",
    " <br> <br>\n",
    "        <font color='#263a61' >\n",
    "           In many places in the coming sum operations over indexes we will miss the $\\sum$ symbol, just it will be avoided(hidden) according to the .<a href='https://en.wikipedia.org/wiki/Einstein_notation'>Einstein summation convention</a> .<br>\n",
    "       For example, the eq. $$z_{ij} = \\sum_l^3 x_{ip} w_{jp} + b_j $$ by applying Enstein convetion we could rewrite it as :\n",
    "         $$z_{ij} = x_{ip} w_{jp} + b_j$$  \n",
    "   The sign $\\sum_p$ is miss.The sumation over p  is implied(by default) because p is repeated twice.Every time when there are repeatable indices that is the indicator for exist of $\\sum$  which is just missing(The sum sign  is not written).\n",
    "    </font>    \n",
    "</h7> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h7>\n",
    " <br> <br>\n",
    "        <font color='#263a61' >\n",
    "            Let find some necessary derivatives: \n",
    "         </font> \n",
    "</h7>    \n",
    " <h2>\n",
    "     <font color='#454214'>\n",
    "         $\\;\\;\\;\\;\\;\\;\\;\\;  \\frac{\\partial z_{mv}}{\\partial w_{ij}} =\\frac{\\partial (x_{mp} w_{vp} + b_v)}{\\partial w_{ij}}$ <br> <br>\n",
    "       $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;  =  \\frac{ x_{mp} \\partial w_{vp} }{\\partial w_{ij}} +\\frac{\\partial b_v}{\\partial w_{ij}} $\n",
    "  </font>\n",
    "</h2>\n",
    " <h7>\n",
    "     <br>\n",
    "     <font color='#1c5cd9' >\n",
    "          $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\;\\;\\;\\;\\;\\;\\;  \\;\\;\\;\\;\\;\\;\\;due\\; to \\; \\frac{ \\partial w_{vp} }{\\partial w_{ij}} =1\\; when \\; (v=i\\;and\\; p=j) \\;and \\;\\frac{ \\partial w_{jp} }{\\partial w_{it}}=0  \\;when \\; (v \\ne i\\;or\\; p \\ne j) \\;  we \\; could \\;introduce\\; a\\; symbol \\; \\delta\\; and \\; write \\; \\frac{ \\partial w_{jp} }{\\partial w_{it}}=\\delta_{vi}\\delta_{pj} $ \n",
    "     </font>\n",
    " </h7> \n",
    "  <h2>\n",
    "     <font color='#454214'>\n",
    "        $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;  =   x_{mp}\\delta_{vi}\\delta_{pj} +\\frac{\\partial b_v}{\\partial w_{ij}} $ <br> <br>\n",
    "  </font>\n",
    "</h2>\n",
    "<h7>\n",
    "     <br>\n",
    "     <font color='#1c5cd9' >\n",
    "          $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\;\\;\\;\\;\\;\\;\\;  \\;\\;\\;\\;\\;\\;\\;using\\; kronicker \\;properties \\; p \\; is\\;  replaced by\\;  j \\; and\\;v \\; by\\;i  $ \n",
    "     </font>\n",
    " </h7> \n",
    "  <h2>\n",
    "     <font color='#454214'>\n",
    "        $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;  =   x_{mj}\\delta_{ii}\\delta_{jj} +\\frac{\\partial b_v}{\\partial w_{ij}} $ <br> <br>\n",
    "       $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;  =  \\frac{ x_{ip} \\partial w_{jp} }{\\partial w_{ij}}\\delta_{ii}\\delta_{jj} +0 $\n",
    "  </font>\n",
    "</h2>\n",
    "<h7>\n",
    "     <br>\n",
    "     <font color='#1c5cd9' >\n",
    "          $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\;\\;\\;\\;\\;\\;\\;  \\;\\;\\;\\;\\;\\;\\; i  \\; is\\;  a\\; part\\; only \\;of \\;  \\delta_{ii} therefore \\;it\\; can \\; be \\;removed \\;and \\;\\frac{\\partial b_v}{\\partial w_{ij}}=0 $ \n",
    "   </font>\n",
    " </h7> \n",
    "  <h2>\n",
    "     <font color='#454214'>\n",
    "        $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;  =   x_{mj}\\delta_{jj} + 0 $ <br> <br>\n",
    "      $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;  =   x_{mj} $ <br> <br>\n",
    "  </font>\n",
    "</h2>\n",
    "<h7>\n",
    "     <br>\n",
    "     <font color='#263a61' >\n",
    "         !!! The result can be proof using direct verification as well  If someone is not able to understand proof using Kronecker notations.\n",
    "   </font>\n",
    " </h7> \n",
    "  <h2>\n",
    "     <font color='#454214'>\n",
    "       $$ \\;\\;(3)\\;\\;\\;\\;\\;\\frac{\\partial z_{mv}}{\\partial w_{ij}} = x_{mj}$$\n",
    "  </font>\n",
    "</h2>\n",
    "<h7>\n",
    "     <br>\n",
    "     <font color='#263a61' >\n",
    "         In the same way can be proof for bias:\n",
    "   </font>\n",
    " </h7> \n",
    "  <h2>\n",
    "     <font color='#454214'>\n",
    "       $$ \\;\\;(4)\\;\\;\\;\\;\\;\\frac{\\partial z_{mv}}{\\partial b_{j}} = \\delta_{vj}$$\n",
    "  </font>\n",
    "</h2>\n",
    "<h7>\n",
    "     <br>\n",
    "     <font color='#263a61' >\n",
    "         Go on!!!\n",
    "   </font>\n",
    " </h7>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    <br> <br>\n",
    "     <font color='#454214'>\n",
    "         $\\;\\;\\;\\;\\;\\;\\;\\; \\frac{\\partial p_{mv}}{\\partial w_{ij}} =\\frac{\\partial p_{mv}}{\\partial z_{ij}}\\times\\frac{\\partial z_{ij}}{\\partial w_{ij}} $\n",
    "    </font>\n",
    "</h2>\n",
    "<h7> \n",
    "    <br>\n",
    "     <font color='#1c5cd9' >\n",
    "          $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\;\\;\\;\\;\\;\\;\\;  \\;\\;\\;\\;\\;\\;\\;applying\\; eq.(3)\\; we \\;ahcieve  $ \n",
    "     </font>\n",
    " </h7> \n",
    " <h2>\n",
    "     <font color='#454214'>\n",
    "         $\\;\\;\\;\\;\\;\\;\\;\\; \\;\\;\\;\\;\\;\\; =\\frac{\\partial p_{mv}}{\\partial z_{ij}}.x_{ij}$\n",
    "    </font>\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    <br> <br>\n",
    "     <font color='#454214'>\n",
    "         $\\frac{\\partial p_{mv}}{\\partial z_{ij}} = \\frac{\\partial \\Big(  \\frac{e^{^{z_{mv}}}}{\\sum_{k=1}^K e^{^{z_{mk}}}}\\Big)}   {\\partial z_{ij}}$\n",
    "   <br> <br>\n",
    "         $\\;\\;\\;\\;\\; = \\frac{1}{(\\sum_{k=1}^K e^{^{z_{mk}}})^2}\\times\\Big( \\frac{\\partial e^{z_{mv}}}{\\partial z_{ij}}\\times(\\sum_{k=1}^K e^{^{z_{mk}}}) - \\frac{\\partial (\\sum_{k=1}^K e^{^{z_{mk}}})}{\\partial z_{ij}}\\times e^{z_{mk}}\\Big) $\n",
    "   <br> <br>\n",
    "   $\\;\\;\\;\\;\\; = \\frac{1}{\\sum_{k=1}^K e^{^{z_{mk}}}}\\times\\frac{\\partial e^{z_{mv}}}{\\partial z_{ij}} - \n",
    "\\frac{1}{(\\sum_{k=1}^K e^{^{z_{mk}}})^2}\\times  \\Big(\\sum_{k=1}^K \\frac{\\partial e^{^{z_{mk}}}}{\\partial z_{ij}}\\Big)\\times e^{z_{mv}} $   $\\;\\;\\;\\;\\;$   \\\\ line 1     \n",
    "  </font>\n",
    " </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    <br> <br>\n",
    "     <font color='#454214'>\n",
    "$\\;\\;\\;\\;\\; = \\frac{1}{\\sum_{k=1}^K e^{^{z_{mk}}}}\\times\\frac{\\partial e^{z_{mv}}}{\\partial z_{ij}} - \n",
    "\\frac{1}{(\\sum_{k=1}^K e^{^{z_{mk}}})^2}\\times  (\\sum_{k=1}^K  e^{^{z_{mk}}}.\\delta_{mi}\\delta_{kj})\\times e^{z_{mv}} $   $\\;\\;\\;\\;\\;$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    <br> <br>\n",
    "     <font color='#454214'>\n",
    "$\\;\\;\\;\\;\\; = \\frac{1}{\\sum_{k=1}^K e^{^{z_{mk}}}}\\times e^{z_{mv}}\\delta_{mi}\\delta_{kj} - \n",
    "\\frac{1}{(\\sum_{k=1}^K e^{^{z_{mk}}})^2}\\times  (\\sum_{k=1}^K  e^{^{z_{mk}}}.\\delta_{mi}\\delta_{kj})\\times e^{z_{mv}} $   $\\;\\;\\;\\;\\;$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    <br> <br>\n",
    "     <font color='#454214'>\n",
    "$\\;\\;\\;\\;\\; = \\frac{1}{\\sum_{k=1}^K e^{^{z_{mk}}}}\\times e^{z_{mv}}\\delta_{mi}\\delta_{vj}- \\frac{\\sum_{k=1}^K  e^{^{z_{mk}}}.\\delta_{mi}\\delta_{kj}}{\\sum_{k=1}^K e^{^{z_{mk}}}}\\times \\frac{e^{z_{mv}}}{\\sum_{k=1}^K e^{^{z_{mk}}}}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2>\n",
    "    <br> <br>\n",
    "     <font color='#454214'>\n",
    "$\\;\\;\\;\\;\\; = \\frac{1}{\\sum_{k=1}^K e^{^{z_{ik}}}}\\times e^{z_{ij}}\\delta_{ii}\\delta_{jj}- \\frac{\\sum_{k=1}^K  e^{^{z_{mk}}}.\\delta_{mi}\\delta_{kj}}{\\sum_{k=1}^K e^{^{z_{mk}}}}\\times \\frac{e^{z_{mv}}}{\\sum_{k=1}^K e^{^{z_{mk}}}}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    <br> <br>\n",
    "     <font color='#454214'>\n",
    "$ \\frac{\\sum_{k=1}^K  e^{^{z_{ij}}}.\\delta_{ii}\\delta_{jj}}{\\sum_{k=1}^K e^{^{z_{ik}}}}\\times \\frac{e^{z_{mv}}}{\\sum_{k=1}^K e^{^{z_{mk}}}} = p_{ij}p_{mv}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial p_{mv}}{\\partial z_{ij}}=p_{ij}(1-p_{mv})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " <h7>\n",
    "     <font color='#1c5cd9'>\n",
    "         <br>\n",
    "         $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$  if $j=t$ then $\\frac{e^{z_{ij}}}{z_{it}} = z_{ij}$ if $j\\ne t$ then  $\\frac{e^{z_{ij}}}{z_{it}}=0$ (it is simple for proving)  <br> <br>\n",
    "        $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ For simplicity  as a continue we will separate the function in two cases when $j=t$ and $j\\ne t$ <br> <br>\n",
    "          $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ When $j=t$ we have : <br> <br>\n",
    "     </font>\n",
    " </h7>  \n",
    " \n",
    " <h3>\n",
    "     <font color='#454214'>\n",
    "          $\\;\\;\\;\\;\\; = \\frac{z_{ij}}{\\sum_{k=1}^K e^{^{z_{ik}}}} - \\frac{1}{(\\sum_{k=1}^K e^{^{z_{ik}}})^2}\\times \\frac{\\partial e^{z_{it}}}{\\partial z_{it}}\\times e^{z_{ij}}$\n",
    "  </font>\n",
    " </h3>  \n",
    " \n",
    "  <h7>\n",
    "     <font color='#1c5cd9'>\n",
    "         <br>\n",
    "        $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$  According to eq(1) we could write: \n",
    "        <br> <br>\n",
    "     </font>\n",
    " </h7>  \n",
    "  <h3>\n",
    "     <font color='#454214'>\n",
    "          $\\;\\;\\;\\;\\;  = p_{ij}(1 -p_{it})$\n",
    "  </font>\n",
    " </h3>  \n",
    "  \n",
    "  <h7>\n",
    "     <font color='#1c5cd9'>\n",
    "         <br>\n",
    "        $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ if $j\\ne t$ and going on from line 1, we achieve\n",
    "        <br> <br>\n",
    "     </font>\n",
    " </h7> \n",
    "  <h3>\n",
    "     <font color='#4542141'>\n",
    "          $\\;\\;\\;\\;\\; = \\frac{1}{\\sum_{k=1}^K e^{^{z_{ik}}}}\\times 0 - \n",
    "\\frac{1}{(\\sum_{k=1}^K e^{^{z_{ik}}})^2}\\times \\frac{ e^{z_{it}} }{\\partial z_{it}}\\times e^{z_{ij}} $\n",
    "          <br> <br>\n",
    "          $\\;\\;\\;\\;\\; = - \n",
    "\\frac{e^{z_{it}}}{\\sum_{k=1}^K e^{^{z_{ik}}}}\\times \\frac{e^{z_{ij}}}{\\sum_{k=1}^K e^{^{z_{ik}}}} $\n",
    "          <br> <br>\n",
    "  </font>\n",
    "  </h3>\n",
    "    <h7>\n",
    "     <font color='#1c5cd9'>\n",
    "         <br>\n",
    "         $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ Again according to eq(1) we could write: \n",
    "        <br> <br>\n",
    "     </font>\n",
    " </h7> \n",
    "  <h3>\n",
    "     <font color='#454214'>\n",
    "        $\\;\\;\\;\\;\\; = -p_{ij}p_{it}$\n",
    "  </font>\n",
    "  </h3> \n",
    "  \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h7>\n",
    "     <font color='#263a61'>\n",
    "         <br>\n",
    "         We can write : \n",
    "     </font>\n",
    " </h7>  \n",
    "  <h3>\n",
    "     <br> <br> \n",
    "    <font color='#454214'>\n",
    "$$\\frac{\\partial p_{ij}}{\\partial z_{it}} = \n",
    "\\begin{pmatrix}\n",
    "p_{ij}(1 -p_{it}) & \\;j=t  \\\\\n",
    "-p_{ij}p_{it} & j \\ne t \n",
    "\\end{pmatrix} $$\n",
    "             \n",
    "  </font>\n",
    "  </h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#263a61'>\n",
    "         <br>\n",
    "         the eqution above can  be rewritten : <br> \n",
    "     </font>\n",
    " </h7>  \n",
    "   <h3>\n",
    "     <br> <br> \n",
    "    <font color='#454214'>\n",
    "        $$\\;  (4)\\; \\;\\frac{\\partial p_{ij}}{\\partial z_{it}} = p_{ij}(\\delta_{jt} - p_{it})$$ \n",
    "        <br> <br>\n",
    "        $ \\frac{\\partial z_{ij}}{\\partial w_{it}}$ \n",
    "        $=\\frac{\\partial \\sum_p^3 x_{ip} w_{jp} + b_j}{\\partial w_{it}}=x_{ij}$\n",
    "         $=\\frac{\\sum_l^3  x_{ip}\\partial w_{jp}}{\\partial w_{it}} +\\frac{\\partial b_j}{\\partial w_{it}}=x_{ij}$\n",
    "    <br> <br>\n",
    "    $$  (5)\\;\\;\\frac{\\partial z_{ij}}{\\partial w_{it}}=x_{ij}$$\n",
    "   </font>\n",
    "  </h3>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h7>\n",
    "     <font color='#263a61'>\n",
    "         <br> <br>\n",
    "         So far so good!!! <br>\n",
    "         In order to apply gradient descent we have to find all \n",
    "         Now let to find derivate of eq.(2) $ L(W,b)=-\\sum_i^n\\sum_j^k y_{ij} \\log (sofmax(Z)_{ij})$ <br> <br>\n",
    "         we have to find all $$\\nabla_{bl}L = \\frac{\\partial L}{\\partial w_{bl}}  = \n",
    " - \\sum_k\\frac{\\partial y_{ij} Log(p_{ij}) }{\\partial w_{bl}}$$\n",
    "       Applying eq (3) and using the Eistent index row we can rewrite euqtion (2) to <br>\n",
    "         $$\\nabla_{it}L = \\frac{\\partial L}{\\partial w_{it}}  = \n",
    " - \\frac{\\partial y_{ij} Log(p_{ij}) }{\\partial w_{it}}$$\n",
    "         \n",
    "  </font>\n",
    "  <h7>\n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "     <br> <br> \n",
    "    <font color='#454214'>\n",
    "$\\nabla_{it}L = \\frac{\\partial L}{\\partial w_{it}}  \n",
    "  =\\frac{\\partial y_{ij} Log(p_{ij}) }{\\partial w_{it}}  $\n",
    "  <br> <br>\n",
    "   $\\;\\;\\;\\;\\; \\;\\;\\;\\;\\;\\;\\;\\; \\;\\;\\; =\n",
    " \\frac{ y_{ij}\\partial Log(p_{ij}) }{\\partial w_{it}} = \n",
    "  \\frac{1}{p_{ij}} \\times \\frac{\\partial p_{ij}}{\\partial z_{it}} \\times \\frac{\\partial z_{ij}}{\\partial w_{it}}$\n",
    "  <br> <br>\n",
    "  $\\;\\;\\;\\;\\; \\;\\;\\;\\;\\;\\;\\;\\; \\;\\;\\; = \\frac{y_{ij}}{p_{ij}} \\times \\frac{\\partial p_{ij}}{\\partial z_{it}}\\times \\frac{\\partial z_{ij}}{\\partial w_{it}} $\n",
    "  </font>\n",
    "  </h3>  \n",
    "   <h7>\n",
    "     <font color='#1c5cd9'>\n",
    "         <br>\n",
    "         $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ By applying eq.(5) and eq.(5) we will achieve: \n",
    "     </font>\n",
    " </h7> \n",
    " <h3>\n",
    "     <br> \n",
    "    <font color='#454214'>\n",
    "        $\\;\\;\\;\\;\\; \\;\\;\\;\\;\\;\\;\\;\\; \\;\\;\\; = \\frac{y_{ij}}{p_{ij}} \\times p_{ij}(\\delta_{jt} - p_{it})x_{ij} $\n",
    "      \n",
    "   <br><br>\n",
    "    $\\;\\;\\;\\;\\; \\;\\;\\;\\;\\;\\;\\;\\; \\;\\;\\; = y_{ij} \\times p_{ij}(\\delta_{jt} - p_{it}) = y_{ij}(\\delta_{jt} - p_{it})x_{ij} $\n",
    "  </font>\n",
    "  </h3>\n",
    "  <h7>\n",
    "     <font color='#1c5cd9'>\n",
    "         <br>\n",
    "         $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ using Kronecker propeties we can replace symbol $t$ with $j$: then we achieve \n",
    "     </font>\n",
    " </h7> \n",
    "  <h3>\n",
    "    <br><br>\n",
    "    $\\;\\;\\;\\;\\; \\;\\;\\;\\;\\;\\;\\;\\; \\;\\;\\;  = y_{ij}(\\delta_{jj} - p_{ij})x_{ij} $\n",
    "   <br><br>\n",
    "    $\\;\\;\\;\\;\\; \\;\\;\\;\\;\\;\\;\\;\\; \\;\\;\\;  = y_{ij}(1 - p_{ij})x_{ij}$\n",
    "  </font>\n",
    "  </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h7>\n",
    "     <font color='#263a61'>\n",
    "         We achived extrmly simple eqation :\n",
    "         \n",
    "   <font>\n",
    "</h7>    \n",
    "<h2>\n",
    "    <br><br>\n",
    "      $$ \\frac{\\partial L}{\\partial w_{ij}} = y_{ij}(1 - p_{ij})x_{ij}$$ \n",
    "  </font>\n",
    "  </h2>\n",
    "<h7>\n",
    "     <font color='#263a61'>\n",
    "         For bias is just :\n",
    "         \n",
    "   <font>\n",
    "</h7> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                         <h2>\n",
    "    <br><br>\n",
    "      $$ \\frac{\\partial L}{\\partial b_{j}} = \\sum_i y_{ij}(1 - p_{ij})$$ \n",
    "  </font>\n",
    "  </h2>\n",
    "  <h7>\n",
    "     <font color='#263a61'>\n",
    "         The bias can not be fitted :\n",
    "         \n",
    "   <font>\n",
    "</h7>  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
