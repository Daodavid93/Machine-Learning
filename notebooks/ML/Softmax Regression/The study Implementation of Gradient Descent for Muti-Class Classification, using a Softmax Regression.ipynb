{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The study Implementation of Gradient Descent for Muti-Class Classification, using a Softmax Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our approach has a two major componets; ascore function that maps the raw data to class scores, abd los function that that quatifiers the aggreement between the predicted scores and the ground thruth label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In softmax regression settings, we are interested in muti-class clasification and so the laebel y can take many values rather then only two. Thus in our training set ${(x_1,y_1), (x_2,y_2),..., (x_n,y_n)}$ we have $i\\ in {1,2,...,K}$.\n",
    "Given a test input x, we want out hypotesis to estimate the porbability the $P(y=k|x)$ (y=k, i.e prediction of one label equal to 1,accoring to one samle(row) x) for each value of $k = {1,2,3 ...,K}$ i.e we want to estimate the probability of class label taking on each of the $K$ different posible values. Thus, our hypotesis will output a K-dimensianal vector(whose element sym up to 1) giving us our K estimated probabilities. Concretely, out score(hypotesis) function $f(x,W)$ takes the form : <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$ f(x|W) = \\begin{bmatrix} P(y=1 |x;W) \\\\  P(y=2 |x;W)  \\\\ ... \\\\ P(y=n |x;W)  \\end{bmatrix} = \n",
    " \\frac{1}{\\sum_{j=1}^k exp(W_j^Tx)} = \\begin{bmatrix} exp(W_1^Tx) \\\\  exp(W_2^Tx)  \\\\ ... \\\\ exp(W_n^Tx)  \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here $W_1, W_2, ..., W_k$ are the parameters and term $\\sum_{j=1}^k exp(W_j^Tx) $  normilized the distribution such that it sums to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now describe the loss function that we will use in softmax regression. In the eqation below $I{.}$ is the indicator function, so that $I{a true statetment}  = 1 $, and $I{a false statetment}  = 0 $ . Out cross-entropy function loss function will be : <br> <br>\n",
    "$$L(W) = - \\sum_{i=1}^m\\sum_{k=1}^K I{(y_i=k)} log\\frac{exp(W_k^Tx_i)}{\\sum_{j=1}^k exp(W_j^Tx_i)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> <br>\n",
    "\n",
    "# Let $z_j = W_k^Tx_j + b$ and $p_j=\\frac{e^{^{z_j}}}{\\sum_{k=1}^K e^{^{z_k}}}$ ,then <br>\n",
    "<br> if $i=j$ <br>\n",
    "# $ \\frac{\\partial \\frac{e^{^{z_j}}}{\\sum_{k=1}^K e^{^{z_k}}}}{\\partial z_j} = \\frac{e^{z_i}(\\sum_{k=1}^K e^{z_k}-e^{z_j})}{(\\sum_{k=1}^K e^{^{z_k}})^2}=   \\frac{e^{^{z_j}}}{\\sum_{k=1}^K e^{^{z_k}}} \\times \\frac{\\sum_{k=1}^K e^{z_k}-e^{z_j}}{\\sum_{k=1}^K e^{^{z_k}}}=p_i(1-p_j) $\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# $\\frac{\\partial \\frac{e^{^{z_i}}}{\\sum_{k=1}^K e^{^{z_k}}}}{\\partial z_j} = \\frac{0-e^{z_j}e^{z_i}}{\\sum_{k=1}^K e^{^{z_k}}} = -\\frac{e^{^{z_i}}}{\\sum_{k=1}^K e^{^{z_k}}}\\times \\frac{e^{^{z_j}}}{\\sum_{k=1}^K e^{^{z_k}}} = \n",
    "-p_ip_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when $i\\ne$ \n",
    "then $\\frac{\\partial p_j}{\\partial z_i} = -p_i p_j$\n",
    "if we use the kronicker symbol $ \\delta_ij$\n",
    "then we can write \n",
    "$$\\frac{\\partial p_j}{\\partial z_i} = p_j(\\delta_{ij} - p_i)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let to investigate the lost function : <br> <br>\n",
    "    $$L = - \\sum_iy_i Log(p_i)$$\n",
    " <br> <br>\n",
    " \n",
    " $$\\frac{\\partial L}{\\partial a_i}  = - \\sum_k\\frac{\\partial y_k Log(p_k) }{\\partial a_k} \\times \\frac{\\partial p_k}{\\partial a_i} = \\sum_k y_k \\frac{1}{p_k}\\frac{\\partial p_k}{\\partial a_i}$$ <br> <br> <br>\n",
    " using abobe eqution : <br> <br>\n",
    " \n",
    " $$ \\sum_k y_k \\frac{1}{p_k}\\frac{\\partial p_k}{\\partial a_i} = \\sum_k y_k \\frac{1}{p_k}p_k(\\delta_{ik} - p_i) $$\n",
    " <br>\n",
    " $$= \\sum_k y_k.\\delta_{ik} - p_i = -(y_i - p_i) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br> <br> <br>\n",
    " $$\\nabla w_j J(W;b) = \\frac{1}{n} \\sum_{i=0}^n [x(i)(p_i-y_i)] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://rstudio-pubs-static.s3.amazonaws.com/337306_79a7966fad184532ab3ad66b322fe96e.html\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
