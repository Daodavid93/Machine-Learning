{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display, Markdown , Math \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printmd(string): display(Markdown(string))\n",
    "def latex(out): printmd(f'{out}')  \n",
    "def pr(string): printmd('***{}***'.format(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "   <font size=\"5\" face = \"Times New Roma\" color='#270336'>\n",
    "     The optimization of Softmax function by  Cross-entropy Loss \n",
    "   </font> \n",
    " </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "  <p>\n",
    "    <a href =   \"https://github.com/daodavid\" > \n",
    "       <img src=\"https://cdn.thenewstack.io/media/2014/12/github-octocat.png\" align=\"left\" width=\"120\"  alt=\"daodavid\" >\n",
    "    </a>\n",
    "    <font face = \"Times New Roma\" size=\"4\"  color='#270336'>\n",
    "        author: daodeiv (David Stankov) \n",
    "    </font>\n",
    "</p>      \n",
    "</h2>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>\n",
    "  <font size=\"4\" face = \"Times New Roma\" color='#3f134f' > \n",
    "    <ul style=\"margin-left: 30px\">\n",
    "      <li><a href='#abstract'>Abstract </a> </li> <br>\n",
    "      <!--<li><a href='#int-1'>Introduction </a> </li><br> -->\n",
    "      <li><a href='#deff_softmax'>Softmaxt definition and  how it works?</a> </li><br>\n",
    "      <li><a href='#cross_entropy'>Cross-entropy Loss and Gradient Descent</a> </li><br>  \n",
    "      <li><a href='#optimization'>Optimization by Cross-entropy Loss and derivation of Gradient descent formula </a> </li><br>\n",
    "       <li><a href='#gradient'>Implementation of Gradient descent algorithm </a> </li><br> \n",
    "      <li><a href='#reg'>Regularization by learning rate and max iterations</a> </li><br>     \n",
    "        \n",
    "</ul>    \n",
    " </font>\n",
    "  </h6>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"1\" id=\"abstract\">... </font> \n",
    "<h2 face = \"Times New Roma\" color='#270336' >&nbsp; Abstract</h2>\n",
    "<br>\n",
    "<font face = \"Times New Roma\" size=\"4.5\"  color='#270336' style=\"margin-right: 45px; margin-left: 45px\" >\n",
    "  &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; This paper contains a mathematical proof and implementation of Gradient Descent formula used as optimization alogithm in Cross-entropy Loss. We will use the Cross-entropy Loss or so-called Softmax Loss as an error function in the process of fitting estimator parameters. This notebook is focused on the mathematics behind softmax regression rather than its application in multi-class classification.We will use the Iris dataset in order to represent the training data process using the softmax implementation. In addition, we will see the experiments related to leaning step and max iterations  </font> <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5 id=\"int-1\" style=\"margin-right: 45px; margin-left: 45px\">\n",
    "<font face=\"Times New Roma\" size=\"4\" color='#270336' >\n",
    "      &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;The Softmax function maybe is the most popular Machine learning algorithm basically, it turns arbitrary real values into probabilities, by using the exponential function.  It could be considered a generalization of the sigmoid function that we could use for multi-class classification problems yet the softmax function is met in many various fields of science as statistical physics (Gibbs distributions), Quantum statistic, Information theory, and Neural networks. Softmax is much attractive in classification problems since it has a simple implementation and in most cases gives satisfying results.\n",
    "    </font>\n",
    "</h5>      \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size=\"4\" color='#270336' face = \"Times New Roma\">\n",
    "  &nbsp;&nbsp; For our purpose, we gonna use the Iris dataset.It is comparatively simple however, it is verry convinient  in Machine Learning styding. \n",
    "<h6> \n",
    "    <font size=\"3\" color='#270336' face = \"Times New Roma\" >\n",
    "        &nbsp;&nbsp; Loading data : <br>\n",
    "</font>    \n",
    "</h6>  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width      species\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = pd.read_csv(\"../../../resources/data/IRIS.csv\")  \n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "labels value :['Iris-setosa' 'Iris-versicolor' 'Iris-virginica']"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "printmd('labels value :' +str(iris['species'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6> \n",
    "    <font size=\"3\" color='#270336' face = \"Times New Roma\" >\n",
    "        &nbsp;&nbsp; Visualizing  data: <br>\n",
    "</font>    \n",
    "</h6>  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x237f54829c8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 463.5x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.FacetGrid(iris,hue='species',height=5).map(plt.scatter,'sepal_length','sepal_width').add_legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font size=\"4\" color='#270336' face = \"Times New Roma\">\n",
    "&nbsp;&nbsp;The class (target)  variable takes a label among 'Iris-setosa', 'Iris-versicolor', 'Iris-virginica'.We will express the label values as one-hot encoding variables or so-called dummies. Representing a non-numerical data into numerical one allows us to use the data in a mathematical equation.\n",
    "</font>    \n",
    "<br> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "***One hode encoding representation***"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iris-setosa</th>\n",
       "      <th>Iris-versicolor</th>\n",
       "      <th>Iris-virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Iris-setosa  Iris-versicolor  Iris-virginica\n",
       "0            1                0               0\n",
       "1            1                0               0\n",
       "2            1                0               0\n",
       "3            1                0               0\n",
       "4            1                0               0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = iris.drop('species', axis=1)\n",
    "y_train = pd.get_dummies(iris['species'])\n",
    "pr('One hode encoding representation')\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_train,y_train, test_size=0.33, random_state=42) #separats into test and train samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "shape of X test data $ X_{M\\times N} =(50, 4)$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "shape of Y test data $ Y_{M\\times N} =(50, 3)$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "shape of X train data $ X_{M\\times N} =(100, 4)$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "shape of Y train data $ Y_{M\\times N} =(100, 3)$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out = \"shape of X test data \" +r'$ X_{{M\\times N}} ={}$'.format(X_test.shape)\n",
    "latex(out)\n",
    "out =\"shape of Y test data \" +r'$ Y_{{M\\times N}} ={}$'.format(y_test.shape)\n",
    "latex(out)\n",
    "    \n",
    "out =\"shape of X train data \" +r'$ X_{{M\\times N}} ={}$'.format(X_train.shape)\n",
    "latex(out)\n",
    "out =\"shape of Y train data \" +r'$ Y_{{M\\times N}} ={}$'.format(y_train.shape)\n",
    "latex(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size=\"3\" color='#270336' face = \"Times New Roma\">  \n",
    "The dataset is split into $X_{M\\times N}$ feature matrix and $Y_{M\\times K}$ target one.Where $M$ is the number of feature vectors match the rows of  dataset. $N$ is the number of components match the columns in dataset.They are also called predictors or attributes.$K$ is equal to the number of  class labels  of target vector.\n",
    "   For our tran data $M = 100$, $N=4$ and $K=3$.<br>\n",
    "</font>\n",
    "<br>\n",
    "<h5 style=\"margin-right: 5px; margin-left: 5px\">\n",
    "<font face=\"Times New Roma\" size=\"3\" color='#270336' >\n",
    "   !!!  Index convetions. When we write $Q_{lh}$ or $q_{ih}$ we will mean the one element of matrix $Q_{M\\times N}$ where $l$ is row $h$ is column.When we write $Q^{i}$  or  $q_{i}$ we  will assume as vector row  $\\vec Q^{i}= [q_{i1},q_{i2},...q_{in}]\\in Q_{m\\times n}$\n",
    "</font>\n",
    "</h5>      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"1\" id=\"deff_softmax\">... </font> \n",
    "<h2 face = \"Times New Roma\" color='#270336' >&nbsp; Softmaxt definition and  how it works?</h2>\n",
    "\n",
    "<br> \n",
    "<font  size=\"4\" color='#270336' face = \"Times New Roma\">\n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;  The softmax function $\\sigma: \\; \\Re^k \\; \\rightarrow \\; \\Re^k $  could be define by formula :\n",
    "</font>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font size=\"5\" color='#4a3e20'>\n",
    "    $$ \\sigma_{softmax (W,b,X^{i})_{ij}}=\\frac{e^{ \\sum_p x_{ip} w_{pj} + b_j}}{\\sum_j^k e^{ \\sum_p x_{ip} w_{pj} + b_j}} $$ \n",
    "</font>   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h5>\n",
    "<font size=\"3\" color='#270336' face = \"Times New Roma\">\n",
    "  if we write :\n",
    "</font>    \n",
    "</h5>\n",
    "\n",
    "<font  size=\"3\" color='#4a3e20' >  \n",
    " $\\; \\; z_{ij} = \\sum_p x_{ip} w_{jp} + b_j $ \n",
    " <br>\n",
    " $\\; \\; \\vec{z_i} = z^i= [z_{i1},...z_{ik} ] $\n",
    "</font>  \n",
    "\n",
    "<h5 >\n",
    "<font size=\"3\" color='#270336' face = \"Times New Roma\">\n",
    " We can rewrite the eq. as  :\n",
    "</font> \n",
    "</h5>    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font size=\"5\" id=\"deff_softmax\" color='#4a3e20'>\n",
    "    $$ (1) \\;\\; \\sigma_{softmax}({z^i})_{ij} =\\frac {e^{z_{ij}} }{ \\sum_i^k e^{z_{ik}} } $$ \n",
    "  </font>\n",
    "  <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\" color='#270336' face = \"Times New Roma\" > \n",
    "  The softmax function  takes as an input a vector $z^i$ with $K$ number of component $z_{i1},...,z{ik}$, and normalized it into  a  probability distribution $p^{i}$ consisting of  $K$ number of probabilities  $p_{i1},...,{ik}$ proportional to exponentials of input values.That is, prior to applying softmax function some vector components could be negative or greater than one and  might not sum up to 1.\n",
    "Furthermore more the larger input components correspond to larger probabilities and $\\sum_j p_{ij}=1$, $w_{jp}$ are the wights or estimators  $w_{pj}\\in W^{K\\times N}$ In matrix  $W^{K\\times N}$ $K$ coresponds to number of class labels and  $N$ coresponds to number of atrribute(feature) of training data, and $ b = [b_1, ...b_k]$ is bais term with component coresponds to class labels. \n",
    " </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "<font size=\"4\" color='#270336'  face = \"Times New Roma\">\n",
    "    According to our data set, we can write the folowing expression.\n",
    "  </font>\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font size=\"3\" color='#4a3e20' >  <br>  \t\n",
    "      &nbsp;&nbsp;&nbsp;&nbsp;   $ W= \\begin{bmatrix} \n",
    "      weight^1\\rightarrow class \\; 1(Iris-setosa)\n",
    "      \\\\ weight^2\\rightarrow class\\;  2(Iris-versicolor) \\;  \n",
    "      \\\\ weight^{3}\\rightarrow class \\;3(Iris-virginica) \\;\n",
    "      \\end{bmatrix} =\n",
    "    \\begin{bmatrix}\n",
    "    \\vec W^1 \\\\  \\vec W^2\\  \\\\ \\vec W^3  \\end{bmatrix} =\n",
    "    \\begin{bmatrix} \n",
    "    w_{11} & w_{12} & w_{13} & w_{14}\n",
    "    \\\\ w_{21} & w_{22} & w_{23}  & w_{24}\n",
    "    \\\\ w_{31} & w_{32} & w_{33} & w_{34}\n",
    "    \\end{bmatrix}  $ &nbsp;&nbsp;&nbsp;&nbsp;  $ B= \\begin{bmatrix}  b_1 \\\\  b_2\\  \\\\  b_3  \\end{bmatrix}\\;\\;\\;$\n",
    "   </font>\n",
    "   \n",
    "<h5 style=\"margin-right: 45px; margin-left: 45px\">\n",
    "<font size=\"2\" color='#270336' face = \"Times New Roma\">\n",
    "  &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; vector $\\vec W^i=[w_{i1},..w_{in}]$ is estimator vecotor for class $i$ , $n$ coresponds to the feature of $X$ (attribute).\n",
    "</font>   \n",
    "</h5>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size=\"3\" color='#270336'  face = \"Times New Roma\">\n",
    "     &nbsp;&nbsp; and the $Z$ in matrix is written as : $Z = XW^T$ \n",
    " </font>\n",
    " <br> <br>\n",
    "  <font size=\"3\" color='#4a3e20' >\n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;  $Z = \n",
    "    \\begin{bmatrix} x_{11} &  x_{12} & x_{13}  & x_{14} \\\\  x_{21} &  x_{22} & x_{23}  & x_{24}\\\\ ... & ... & ... & ...\\\\ x_{m1} &  x_{m2} & x_{m3}  & x_{m4} \\end{bmatrix} \\times \\begin{bmatrix}\n",
    "    w_{11} & w_{21} & w_{31} \n",
    "    \\\\ w_{12} & w_{22} & w_{32} \n",
    "    \\\\  w_{13} & w_{23} & w_{33}\n",
    "     \\\\  w_{14} & w_{24} & w_{34}\n",
    "    \\end{bmatrix} + \\begin{bmatrix}  b_1 \\\\  b_2\\  \\\\  b_3  \\end{bmatrix}  =   \\begin{bmatrix} z_{11} &  z_{12} & z_{13} \\\\  z_{21} &  z_{32} & z_{33} \\\\ ... & ... & ...  \\\\ z_{m1} &  z_{m2} & z_{m3}  \\end{bmatrix} $ <br>   \n",
    "   </font>\n",
    "</h1>   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h7>\n",
    "<font size=\"3\" color='#270336'>  \n",
    "   &nbsp;&nbsp; The softmax function computes the probability that a training example $X^{(i)}$  belongs to class $y^{j}$ given \n",
    "    the weight  matrix $W$ and bias $\\vec b$ . <br> \n",
    "    So we compute the probability : <br>\n",
    "  </font>\n",
    "  <font size=\"3\" color='#4a3e20' >\n",
    "      $$p_{ij}=P(y^{j} \\;| \\;z^i) = \\sigma_{softmax (z)_{ij}}=\\frac{e^{z_{ij}}}{\\sum_p^K e^{z_{ip}}} $$\n",
    "   </font>   \n",
    "</h7>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\" color='#270336' > \n",
    "    &nbsp;&nbsp; In a matrix for all $P_{ij}$ given our data $Y = [y^1 -Iris-setosa,y^2 -Iris-versicolor,y^3- Iris-virginica]$\n",
    " </font> <br>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<font size=\"3\" color='#4a3e20' >\n",
    "   $ P = \\begin{bmatrix} p(y_{1} |z^{1})_{11} &  p(y_{2} |z_{2} )_{12}  &  p(y^{3}  |z^{3} )_{13} \n",
    "        \\\\ ... &  ...  & ...\n",
    "        \\\\ P(y_{1} |z^{m}  )_{m1} &  P(y_{m}  |z^{m}  )_{m2}  & P(y_{3}  |z^{m} )_{m3}\n",
    "        \\end{bmatrix} = \n",
    "        \\begin{bmatrix} \\frac{e^{z_{11}}}{\\sum_{1j}e^{z_{1j}}} & \\frac{e^{z_{12}}}{\\sum_{1j}e^{z_{1j}}} & \\frac{e^{z_{13}}}{\\sum_{1j}e^{z_{1j}}} \n",
    "        \\\\   \\\\ ... & ... & ...  \\\\\n",
    "        \\\\    \\frac{e^{k_{n1}}}{\\sum_{j}^3e^{z_{nj}}} & \\frac{e^{z_{n2}}}{\\sum_{j}^3e^{z_{nj}}} & \\frac{e^{z_{n3}}}{\\sum_{j}^3e^{z_{nj}}}\\end{bmatrix} $\n",
    "</h1>        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\" color='#270336' face = \"Times New Roma\"> \n",
    "  for example, the probability record $x^1$ from dataset to belongs to class  $y_2$ (Iris-versicolor) will be calculated as:\n",
    "  </font>\n",
    "  <br> <br> \n",
    "  <font size=\"4\" color='#270336'>\n",
    "     $$p_{12} =P(y=2 |x^{1}) = \\frac{ e^{ ^{z_{12}} } }{ \\sum_p^3 e^{z_{1p}}}=\\frac{ e^{ (^{\\sum_v^3 x_{1v}.w_{vj} + b_v }} )}{ \\sum_k^3 e^{ ^ ({\\sum_v^3 x_{1v}.w_{vk}} + b_k})}$$\n",
    "  </font>\n",
    "  <h5 style=\"margin-right: 45px; margin-left: 45px\">\n",
    "<font size=\"2\" color='#270336' face = \"Times New Roma\">\n",
    "    always the $p_{ij} \\in [0,1]$   and $\\sum_j p_{ij}= 1$\n",
    "    </font>\n",
    "<h5>    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\" color='#270336'  face = \"Times New Roma\">\n",
    "    &nbsp; Let to see how the softmax function  can be applied concretely in our training dataset :\n",
    "             First, let to define a weight matrix $W$ and bias $\\vec b$ <br> <br>\n",
    " <font>  \n",
    "   <font size=\"3\" color='#4a3e20' >\n",
    "   &nbsp;&nbsp;    $W =\\begin{bmatrix} \n",
    "    w_{11} & w_{12} & w_{13} & w_{14}\n",
    "    \\\\ w_{21} & w_{22} & w_{23}  & w_{24}\n",
    "    \\\\ w_{31} & w_{32} & w_{33} & w_{34}\n",
    "    \\end{bmatrix} \n",
    "    =  \\begin{bmatrix} 1.13912853 & 1.36806065 & 0.44595227 &0.74652812\n",
    "              \\\\1.018221  & 0.85803142& 1.15116108 &1.00726296\n",
    "              \\\\0.84265048 & 0.77390793& 1.40288666& 1.24620892\\end{bmatrix}$\n",
    "     &nbsp;&nbsp; $B = \\begin{bmatrix}  b_1 \\\\  b_2\\  \\\\  b_3  \\end{bmatrix} = \\begin{bmatrix} 1.07521097 \\\\1.00883369 \\\\0.91595534 \\end{bmatrix} $\n",
    "</font>     \n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5 style=\"margin-right: 45px; margin-left: 45px\">\n",
    "<font size=\"3\" color='#270336' face = \"Times New Roma\">\n",
    " I've prepared weight vector $W$ and bias $B$ in advance, how? We will see later.\n",
    "</font>    \n",
    "</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.array([[1.13912853, 1.36806065, 0.44595227, 0.74652812],\n",
    "              [1.018221,   0.85803142, 1.15116108, 1.00726296],\n",
    "              [0.84265048, 0.77390793, 1.40288666, 1.24620892]]) # define a weight matrix\n",
    "\n",
    "b = np.array([1.07521097, 1.00883369, 0.91595534])  #bias vector (intercept) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5 style=\"margin-right: 45px; margin-left: 45px\">\n",
    "<font size=\"3\" color='#270336' face = \"Times New Roma\">\n",
    "         The implementation of softmax function: <br>\n",
    "</font>    \n",
    "</h5>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X, weight, b):\n",
    "    '''\n",
    "    perform softmax function\n",
    "    \n",
    "    Parameters :\n",
    "    X : ndarray\n",
    "       train data\n",
    "    weight : ndarray\n",
    "       weght matrix\n",
    "    b : ndarray \n",
    "        bias vector        \n",
    "\n",
    "    Returns \n",
    "       ndarray \n",
    "    '''\n",
    "    if type(X) != np.ndarray or type(W) != np.ndarray or type(b) != np.ndarray: \n",
    "        raise ValueError('X and y must be ndarray')\n",
    "    #dot product between X_data matrix  and tranposed Weight_ matrix added to Bias  gives matrix each z_ij\n",
    "    Z = X.dot(weight.T)\n",
    "\n",
    "    #return matrix cosist of exponentials Z input net\n",
    "    exp_z = np.exp(Z)\n",
    "\n",
    "    #array contains sum  of every row  (e^z_{ik})\n",
    "    sums=np.sum(exp_z, axis=1) \n",
    "    \n",
    "    #return softmax(Z)_{ij}\n",
    "    return (exp_z.T/sums).T \n",
    "\n",
    "\n",
    "def accuracy(Y, P):\n",
    "    '''\n",
    "    evualate accuracy of dummies variable\n",
    "    \n",
    "    Parameters :\n",
    "    Y_target : ndarray\n",
    "      actual real values \n",
    "    P : ndarray\n",
    "     predicted values (probability)\n",
    "    Return\n",
    "      float\n",
    "    '''\n",
    "    if type(Y) != np.ndarray or type(P) != np.ndarray: \n",
    "        raise ValueError('X and y must be ndarray')\n",
    "        \n",
    "    C =  np.argmax(Y, axis=1)==np.argmax(P, axis=1)\n",
    "    D = np.where(C==True)\n",
    "    return len(D[0])/len(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "original data label $ Y_{M\\times N} :$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iris-setosa</th>\n",
       "      <th>Iris-versicolor</th>\n",
       "      <th>Iris-virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Iris-setosa  Iris-versicolor  Iris-virginica\n",
       "71             0                1               0\n",
       "106            0                0               1\n",
       "14             1                0               0\n",
       "92             0                1               0\n",
       "102            0                0               1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = \"original data label \" +r'$ Y_{{M\\times N}} :$'.format(X_test.shape)\n",
    "latex(out)\n",
    "y_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "predicted data label $ P_{M\\times N} :$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iris-setosa</th>\n",
       "      <th>Iris-versicolor</th>\n",
       "      <th>Iris-virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.155403</td>\n",
       "      <td>0.419975</td>\n",
       "      <td>0.424622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.062757</td>\n",
       "      <td>0.360841</td>\n",
       "      <td>0.576402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.822160</td>\n",
       "      <td>0.130184</td>\n",
       "      <td>0.047656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.138449</td>\n",
       "      <td>0.418586</td>\n",
       "      <td>0.442965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.036041</td>\n",
       "      <td>0.366646</td>\n",
       "      <td>0.597314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Iris-setosa  Iris-versicolor  Iris-virginica\n",
       "95     0.155403         0.419975        0.424622\n",
       "96     0.062757         0.360841        0.576402\n",
       "97     0.822160         0.130184        0.047656\n",
       "98     0.138449         0.418586        0.442965\n",
       "99     0.036041         0.366646        0.597314"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latex(\"predicted data label \" +r'$ P_{{M\\times N}} :$'.format(X_test.shape))\n",
    "predict = softmax(np.array(X_train),W,b)\n",
    "predict = pd.DataFrame(predict,columns = ['Iris-setosa', 'Iris-versicolor','Iris-virginica'])\n",
    "predict.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "***accuracy : 0.68***"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a =accuracy(np.array(y_train),np.array(predict))\n",
    "pr('accuracy : '+str(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font  size=\"3\" color='#270336'  face = \"Times New Roma\">\n",
    "  From $P_{{M\\times N}}$ output let to consider the first row : <br> <br>\n",
    "            $P'_{71,1} = 0.17 \\rightarrow  $ has  $17$% chance that record $x^{71}$ belongs to class 1 'iris-setosa' <br> \n",
    "            $P'_{71,2} = 0.43 \\rightarrow  $ has $43$% chance that record row $x^{71}$ belongs to class 2 'Iris-versicolor' <br> \n",
    "            $Y'_{71,3} = 0.39 \\rightarrow  $ has $39$% chance that record $x^{71}$ belongs to class 3 'Iris-virginica'<br> \n",
    "            <br>\n",
    "      From above result we can conclude : <br>\n",
    "      We cannot be too sure which is the class that record belongs $X_{71}$. Its second column has $43$% chance which is the biggest one therefore, we would assume that record belongs to class 'Iris-versicolor'in fact, that is actually correct .By applying the evaluation process for all results we can validate that the weight matrix $W_{M \\times N}$ have given the $80$% accuracy.\n",
    "</font>\n",
    "<h6> \n",
    "  <font size=\"3\" color='#270336' face = \"Times New Roma\" >\n",
    "     &nbsp;&nbsp; &nbsp;&nbsp;How have I  found the weight matrix $W$ and bias $B$ ? \n",
    "</font>  \n",
    " </h6>\n",
    " <font size=\"3\" color='#270336'  face = \"Times New Roma\">\n",
    "Just I used the <mark>LogisticRegression</mark> from <mark>scikit-learn</mark> and took the coefficients, but our main purpose is to understand the way   of finding the weight $W$ and bias $\\vec b$ . \n",
    "</font>\n",
    "\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"1\" id=\"cross_entropy\">... </font> \n",
    "<\n",
    "<h2 color='#270336' face = \"Times New Roma\" >&nbsp; Cross-entropy Loss and Gradient Descent</h2>\n",
    "<br>\n",
    "<font size=\"3\" color='#270336' face = \"Times New Roma\">\n",
    "      &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;In order to find the $W$ matrix we will introduce the error function, wich we minimaize.The error function \n",
    "        wich we will use in Softmax regression is called Cross-Entropy Loss or Sofmax Loss  defined as: \n",
    " <font>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> <br>   \n",
    "  <font size=\"5\" color='#4a3e20' >\n",
    "    $$ (2)\\;\\; L(W,b)=-\\sum_i^m\\sum_j^k y_{ij} \\log (p(Z)_{ij})$$ <br> <br>  \n",
    "  </font>\n",
    " <h5 style=\"margin-right: 45px; margin-left: 45px\">\n",
    "<font size=\"2\" color='#270336' face = \"Times New Roma\">\n",
    "  &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "    Where $m$ is a count of records , $k$ is  count of classes $y_{ij}$ is label values , $p_{ij}=\\phi_{softmax}(Z)_{ij}$ are  $Y'_{ij}$ predicited class  values, $W$ is weight matrix and $b$ bias. \n",
    "  </font>   \n",
    "</h5>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\" color='#270336' face = \"Times New Roma\">\n",
    "    Why will we use these Loss function is a question which brings by itself much reading and investigations.Here <a href='https://daodavid.github.io/Machine-Learning/pages/html/ML/logistic-regression/Cross-entropy%20function.Investigation%20and%20gradient%20descent.html'>cross-entropy</a> there are some examples and investigations related to Minimum cross-entropy used in  Sigmoid regression. That can give you some intuitions.<br>\n",
    "Out goal is to mimimaize the eq.(2) in order to find best estimators   $w_{ij}\\in W$ and $b_j$ given \n",
    " a training data $X$  and a label data $Y$. <br>\n",
    "    To do that we will apply well known Gradient descent as a optimization model. \n",
    "    Note that, the eq.(2) is the function of all weights $w_{ij}$, bias $b_j$ all training data X and label data Y.        \n",
    " </font>\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\" color='#270336' face = \"Times New Roma\"> \n",
    "  &nbsp;&nbsp;  Gradient descent is defined as  : \n",
    "</font>\n",
    "<br>    \n",
    "<font size=\"3\" color='#4a3e20' >   \n",
    "    $$ \\;  \\; \\;  \\; \\; \\; \\;\\begin{matrix} w_{ij} = w_{ij} - \\lambda \\nabla w_{ij}L(W,b) \\\\  \\\\ b_{j} = b_j - \\lambda\\nabla b_{j}L(W,b)\n",
    "       \\end{matrix} $$ \n",
    "</font>\n",
    "\n",
    "<h5 >\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font size=\"3\" color='#270336' face = \"Times New Roma\">\n",
    "where  $\\lambda$ is learning rate or step size \n",
    "  </font>   \n",
    "</h5>   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h7>\n",
    "    <font size=\"3\" color='#270336' face = \"Times New Roma\">\n",
    "&nbsp;&nbsp;  Plug in eq.(2) in  gradient descent we achive the general formula : \n",
    "</font>\n",
    "    <br>\n",
    "    <font size=\"4\" color='#4a3e20' >   \n",
    "     $$(3) \\;  \\; \\;  \\; \\; \\; \n",
    "   \\begin{matrix} \\nabla w_{ij}L(W,b)   &=  \\nabla w_{ij}L(W,b) -\\frac{\\partial}{\\partial w_{ij}}\\Big(\\sum_k^m\\sum_n^n y_{mn} \\log {p_{mn}}\\Big) \\\\  \\\\\n",
    "     \\nabla b_{j}L(W,b) & = \\nabla b_{j}L(W,b)  - \\frac{\\partial}{\\partial b_{j}}\\Big(\\sum_k^m\\sum_n^n y_{mn} \\log{p_{mn}}\\Big)\n",
    "       \\end{matrix} $$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\" color='#270336' face = \"Times New Roma\"> \n",
    "   Before we take up  with  $\\nabla w_{ij}L(W,b)$. We gonna introduce some math technics which will make our work easier.\n",
    "   <br>\n",
    "   For simplicity in the summation process of indices, we will introduce a <a href='http://physics.csusb.edu/~prenteln/notes/vc_notes.pdf'>Kronecker symbol</a> .<br>   <br>\n",
    "</font>\n",
    "<font size=\"3\" color='#4a3e20' >  \n",
    "       $$\\delta_{ij} =    \n",
    "         \\begin{equation}\n",
    "   \\begin{Bmatrix} \n",
    "   1 & if \\; i=j  \\\\\n",
    "   0 & if \\; i\\ne j  \n",
    "    \\end{Bmatrix} \n",
    "\\end{equation}$$\n",
    "  <br><br>\n",
    "          $$ \\delta_{ij} = \\begin{bmatrix} 1 & 0 & 0  \\\\ 0 & 1 & 0 \\\\  0 & 0 & 1 \\end{bmatrix}$$\n",
    "   </font>\n",
    "<font size=\"3\" color='#270336' face = \"Times New Roma\">  \n",
    "   In many places in the coming sum operations over indexes we will miss the $\\sum$ symbol, just it will be avoided(hidden) according to the .<a href='https://en.wikipedia.org/wiki/Einstein_notation'>Einstein summation convention</a> .<br>\n",
    "       For example, the equation. \n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h7>\n",
    "    <font size=\"2\" color='#270336'>  \n",
    "   $$z_{ij} = \\sum_p^3 x_{ip} w_{jp} + b_j $$ \n",
    "</font>\n",
    "<font size=\"3\" color='#270336' face = \"Times New Roma\">  \n",
    "     by applying the  Enstein convetion we could rewrite it as : <br>\n",
    "</font>\n",
    "</h7>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h7>\n",
    "    <font size=\"2\" color='#270336'> \n",
    "   $$z_{ij} = x_{ip} w_{jp} + b_j$$  \n",
    "</font>\n",
    "<font size=\"3\" color='#270336' face = \"Times New Roma\">   \n",
    "     The sign $\\sum_p^3$ is miss.The sumation over p  is implied(by default) because p is repeated twice.Every time when there are repeatable indices that is the indicator for exist of $\\sum$  which is just missing(The sum sign  is not written).\n",
    "</font>\n",
    "</h7>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5 style=\"margin-right: 45px; margin-left: 45px\">\n",
    "<font size=\"4\" color='#270336' face = \"Times New Roma\">\n",
    " &nbsp;&nbsp; Now we gonna to resolve the eqs.(3)\n",
    "  </font>   \n",
    "</h5>   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"1\" id=\"optimization\">... </font> \n",
    "<h2 color='#270336' face = \"Times New Roma\" >&nbsp; Optimization of Cross-entropy Loss and derivation of Gradient descent formula</h2>\n",
    "<br><br>\n",
    "<font size=\"4\" color='#270336' face = \"Times New Roma\">\n",
    "      &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;In order to minimize the entropy of data, we must to take up with minimizatin of Cross entropy loss respect to $w_{ij}$.The cross-etropy Loss is a function of all feature vectors $X_{M\\times N}$ ,all labels $X_{M\\times K}$ , weight $W_{K\\times N}$ and bias $B_K$ <br> $L = L(X,T,W,B)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"4\" color='#4a3e20' >    \n",
    " &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;$\\frac{\\partial L}{\\partial w_{ij}}=-\\frac{\\partial}{\\partial w_{ij}}\\Big(\\sum_k\\sum_n y_{mn} \\log {p_{mn}}\\Big)$\n",
    " $=\\sum_k\\sum_n y_{mn}\\frac{\\partial \\log {p_{mn}}}{\\partial w_{ij}}$ \n",
    "  </font>\n",
    "</h1>     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"4\" color='#4a3e20' >    \n",
    " &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;$=-\\sum_m\\sum_n\\frac{y_{mn}}{p_{mn}}\\frac{\\partial  p_{mn}}{\\partial w_{ij}} $ \n",
    "            $=\\sum_m\\sum_n \\frac{y_{mn}}{p_{mn}}\\frac{\\partial  p_{mn}}{\\partial z_{vp}}\\frac{\\partial  z_{vp}}{\\partial w_{ij}} $\n",
    " </font>\n",
    "</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5 style=\"margin-right: 50px; margin-left: 160px\">\n",
    "<font face=\"Times New Roma\" size=\"3\" color='#270336' >\n",
    "since  $z_{vp} = f(w_{pi}) \\Rightarrow \\frac{\\partial  z_{vp}}{\\partial w_{ij}} = 0\\;$ if $\\; p\\ne i$ then  we can  write $ \\frac{\\partial  z_{vp}}{\\partial w_{ij}}=\\delta_{pi}\\frac{\\partial  z_{vp}}{\\partial w_{ij}}$ \n",
    "      and  $p_{mn} = f(z_{mv}) \\Rightarrow \\frac{\\partial  z_{vp}}{\\partial w_{ij}} = 0$ if $ m\\ne v $ then  we  can  write  $\\frac{\\partial p_{mn}}{\\partial z_{vp}}=\\delta_{mv}\\frac{\\partial  p_{mn}}{\\partial z_{vp}}  $ \n",
    "   plug in our equation we will achieve \n",
    "    </font>\n",
    "</h5>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"4\" color='#4a3e20' >  \n",
    "     &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;$ =-\\sum_m\\sum_n \\frac{y_{mn}}{p_{mn}}\\delta_{mv}\\frac{\\partial  p_{mn}}{\\partial  z_{vp}} \\delta_{pi}\\frac{\\partial  z_{vp}}{\\partial w_{ij}} $\n",
    "    </font>\n",
    "</h1>     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5 style=\"margin-right: 50px; margin-left: 160px\">\n",
    "<font face=\"Times New Roma\" size=\"3\" color='#270336' >\n",
    "    using common Kronicker $\\delta$ proprties   \n",
    "  </font>\n",
    "</h5>    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"4\" color='#4a3e20' >  \n",
    "     &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;$=-\\sum_m\\sum_n \\frac{y_{mn}}{p_{mn}}\\delta_{mm}\\delta_{ii}\\frac{\\partial  p_{mn}}{\\partial z_{mi}}\\frac{\\partial  z_{mi}}{\\partial w_{ij}} $ $=-\\sum_m\\sum_n \\frac{y_{mn}}{p_{mn}}\\frac{\\partial  p_{mn}}{\\partial z_{mi}}\\frac{\\partial  z_{mi}}{\\partial w_{ij}} $ \n",
    "    </font>\n",
    "</h1>     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\" color='#270336' face = \"Times New Roma\">\n",
    "    We've successfully reduced the count of sum operations, using Einstein's convention and Kronecker symbol and\n",
    "    achieved\n",
    "</font>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"4\" color='#4a3e20' >    \n",
    " &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;$\\frac{\\partial L}{\\partial w_{ij}}=-\\sum_m\\sum_n \\frac{y_{mn}}{p_{mn}}\\frac{\\partial  p_{mn}}{\\partial z_{mi}}\\frac{\\partial  z_{mi}}{\\partial w_{ij}}$ \n",
    "  </font>\n",
    "</h1>     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h7>\n",
    "    <font size=\"3\" color='#270336'>\n",
    "  Let to focus on terms $\\frac{\\partial  p_{mn}}{\\partial z_{mi}}$ and $\\frac{\\partial  z_{mi}}{\\partial w_{ij}}$\n",
    "    </font>\n",
    "</h7>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"4\" color='#4a3e20' >  \n",
    "      &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;\n",
    "     $\\frac{\\partial  p_{mn}} {\\partial z_{mi}}=\\frac{\\partial\\frac { e^{z_{mn}} }{ \\sum_ke^{z_{mk}}} }{\\partial z_{mi}}$\n",
    "     $=\\frac{1}{(\\sum_ke^{z_{mk}})^2}\\times \\Big(\\frac{\\partial e^{z_{mn}} }{\\partial z_{mi}}\\times(\\sum_ke^{z_{mk}}) - e^{z_{mn}}\\times\\frac{\\partial (\\sum_ke^{z_{mk}})}{\\partial z_{mi}}  \\Big)$\n",
    "   </font>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"4\" color='#4a3e20' >  \n",
    "     &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;\n",
    "     $=\\frac{e^{z_{mn}}\\times\\frac{\\partial z_{mn}}{\\partial z_{mi}}}{\\sum_ke^{z_{mk}}} - \\frac{e^{z_{mn}}}{\\sum_ke^{z_{mk}}}\\times\\frac{ \\sum_k e^{z_{mk}} \\frac{ \\partial z_{mk}}{\\partial z_{mi}}}  {\\sum_ke^{z_{mk}}}$\n",
    "   </font>\n",
    "</h1>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5 style=\"margin-right: 50px; margin-left: 160px\">\n",
    "<font face=\"Times New Roma\" size=\"3\" color='#270336' >\n",
    "      since $\\frac{\\partial z_{mk}}{\\partial z_{mi}}=0$ if $k\\ne i\\;\\frac{\\partial z_{mk}}{\\partial z_{mi}}=1\\;ifk = i\\;\\Rightarrow \\frac{\\partial z_{mk}}{\\partial z_{mi}}=\\delta_{ki} $ plug in we achieve\n",
    "  </font>\n",
    "</h5>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"4\" color='#4a3e20' >  \n",
    "     &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;\n",
    "     $=\\frac{e^{z_{mn}}\\times \\delta_{ni} }{\\sum_ke^{z_{mk}}} - \\frac{e^{z_{mn}}}{\\sum_ke^{z_{mk}}}\\times\\frac{ \\sum_k e^{z_{mk}}\\delta_{ki}}  {\\sum_ke^{z_{mk}}}$ \n",
    "      $=\\frac{e^{z_{mn}}\\times \\delta_{ni} }{\\sum_k e^{z_{mk}}} - \\frac{e^{z_{mn}}}{\\sum_ke^{z_{mk}}}\\times\\frac{ \\sum_k e^{z_{mk}}\\delta_{ki}}  {\\sum_ke^{z_{mk}}}$ \n",
    "   </font>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"4\" color='#4a3e20' >  \n",
    "      &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;\n",
    "      $=\\frac{e^{z_{mn}}\\times \\delta_{ni} }{\\sum_ke^{z_{mk}}} - \\frac{e^{z_{mn}}}{\\sum_ke^{z_{mk}}}\\times\\frac{ \\sum_k e^{z_{mi}}\\delta_{ii}}  {\\sum_ke^{z_{mk}}}$ \n",
    "          $=\\frac{e^{z_{mn}}\\times \\delta_{ni} }{\\sum_ke^{z_{mk}}} - \\frac{e^{z_{mn}}}{\\sum_ke^{z_{mk}}}\\times \\frac{ e^{z_{mi}}}{\\sum_ke^{z_{mk}}}$ \n",
    "   </font>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5 style=\"margin-right: 50px; margin-left: 160px\">\n",
    "<font face=\"Times New Roma\" size=\"3\" color='#270336' >\n",
    "from eq.(1)  $\\Rightarrow$ $\\frac{ e^{z_{mn}} }{ \\sum_k e^{z_{mk} } }=p_{mn}$ and $\\frac{ e^{z_{mi}} }{ \\sum_k e^{z_{mk} } }=p_{mi}$  when we apply it, we will achieve \n",
    " </font>\n",
    "</h5>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"4\" color='#4a3e20' >  \n",
    "      &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;\n",
    "       $=p_{mn}\\times \\delta_{ni} - p_{mn}\\times p_{mn}p_{mi}$ <br>\n",
    "        &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;\n",
    "          $=p_{mn}(\\delta_{ni} -  p_{mi})$ \n",
    "   </font>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\" color='#270336' face = \"Times New Roma\">\n",
    " For term  $\\frac{\\partial  p_{mn}} {\\partial z_{mi}}$ we achieve :\n",
    "</font>\n",
    "<br> <br>\n",
    "<font size=\"4\" color='#4a3e20' >  \n",
    "      &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;\n",
    "     $$4) \\; \\; \\; \\; \\frac{\\partial  p_{mn}} {\\partial z_{mi}}=p_{mn}(\\delta_{ni} -  p_{mi})$$\n",
    "   </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\" color='#270336' face = \"Times New Roma\">\n",
    " For $\\frac{\\partial  z_{mi}}{\\partial w_{ij}} $ we have :\n",
    "</font>\n",
    "<br><br>\n",
    " <font size=\"4\" color='#4a3e20' >    \n",
    " &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;\n",
    "     $\\frac{\\partial  z_{mi}}{\\partial w_{ij}} =\\frac{\\partial ( \\sum_k  x_{mk}w_{ki})}{\\partial w_{ij}}= \\frac{ \\sum_k z_{mi} x_{mk}\\partial w_{ki}}{\\partial w_{ij}}$ \n",
    "  </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5 style=\"margin-right: 50px; margin-left: 160px\">\n",
    "<font face=\"Times New Roma\" size=\"3\" color='#270336' >\n",
    "      $\\frac{\\partial w_{ki}}{\\partial w_{ij}} = \\delta_{kj}$ only a direct verification can  proof it\n",
    "  </font>\n",
    "</h5>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"4\" color='#4a3e20' >  \n",
    "      &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;\n",
    "     $=  \\sum_k  x_{mk} \\delta_{kj} =\\sum_k  x_{mj}\\delta_{jj}=x_{mj}$  \n",
    "   </font>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\" color='#270336' face = \"Times New Roma\">\n",
    " and for $\\frac{\\partial  z_{mi}}{\\partial w_{ij}}$ we achive :\n",
    "</font>\n",
    "<br> <br>\n",
    "\n",
    " <font size=\"4\" color='#4a3e20' >  \n",
    "      $$(5) \\;\\;\\;\\;\\;\\frac{\\partial  z_{mi}}{\\partial w_{ij}} = x_{mj} $$   \n",
    "   </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h7>\n",
    "    <font size=\"3\" color='#270336'> \n",
    "     &nbsp;&nbsp;\n",
    "     Applying  eqs.(4) (5) in : $\\frac{\\partial L}{\\partial w_{ij}}=-\\sum_m\\sum_n \\frac{y_{mn}}{p_{mn}}\\frac{\\partial  p_{mn}}{\\partial z_{mi}}\\frac{\\partial  z_{mi}}{\\partial w_{ij}}$   we have  :\n",
    "    </font>\n",
    "</h7>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"4\" color='#4a3e20' >    \n",
    " &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;$\\frac{\\partial L}{\\partial w_{ij}}=-\\sum_m\\sum_n \\frac{y_{mn}}{p_{mn}}p_{mn}(\\delta_{ni} -  p_{mi})x_{mj}=--\\sum_m\\sum_n y_{mn}(\\delta_{ni} -  p_{mi})x_{mj} $ \n",
    "  </font>\n",
    "</h1>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"4\" color='#4a3e20' >  \n",
    "      &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;\n",
    "     $ =-\\sum_m\\sum_n y_{mn}\\delta_{ni} + \\sum_m\\sum_n y_{mn} p_{mi}x_{mj} $ \n",
    "   </font>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5 style=\"margin-right: 50px; margin-left: 160px\">\n",
    " <font face=\"Times New Roma\" size=\"3\" color='#270336' >\n",
    " we can  replace an index $n$ with $i$ (!There is no $\\sum_i$)  \n",
    " </font>    \n",
    "</h5>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1>\n",
    " <font size=\"4\" color='#4a3e20' >  \n",
    "      &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;\n",
    "    $ =-\\sum_m\\sum_n y_{mi}\\delta_{ii}x_{mj} + \\sum_m\\sum_n y_{mn} p_{mi}x_{mj} $ \n",
    "   </font>\n",
    "</h1>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1>\n",
    " <font size=\"4\" color='#4a3e20' >  \n",
    "      &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;\n",
    "   $ =-\\sum_m y_{mi}x_{mj} + \\sum_m\\sum_n y_{mn} p_{mi}x_{mj} $\n",
    "   </font>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5 style=\"margin-right: 50px; margin-left: 160px\">\n",
    " <font face=\"Times New Roma\" size=\"3\" color='#270336' >\n",
    "      &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "      $\\sum_n y_{in}=1$ (the sum of all probability) applying it\n",
    " </font> \n",
    "</h5>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"4\" color='#4a3e20' >  \n",
    "      &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;\n",
    "  $ =-\\sum_m y_{mi}x_{mj} + \\sum_m 1. p_{mi}x_{mj} = \\sum_m  p_{mi}x_{mj}-\\sum_m y_{mi}x_{mj}$\n",
    "   </font>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"4\" color='#4a3e20' >  \n",
    "      &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;\n",
    "  $ = \\sum_k^m( p_{mi}-y_{mi})x_{mj}$\n",
    "  </font>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\" color='#270336' face = \"Times New Roma\">\n",
    "     &nbsp;&nbsp;\n",
    "      Maybe we've achieved the most important result .The optimization  of Cross-entropy respect to $w_{ij}$  :\n",
    " </font>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"5\" color='#4a3e20' >  \n",
    "     $$6) \\;\\;\\;\\;\\nabla w_{ij}L(W,b)= \\sum_m( p_{mi}-y_{mi})x_{mj}$$\n",
    "   </font>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\" color='#270336' face = \"Times New Roma\">\n",
    "     &nbsp;&nbsp;\n",
    "      If we apply  the same   step for $\\nabla b_{i}L(W,b)$ (It is easier one)  we will achieve the minimization formula for bias ,wich looks like this<br>\n",
    " </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"5\" color='#4a3e20' >  \n",
    "     $$7)\\;\\;\\;\\;\\nabla b_{i}L(W,b)= \\sum_m( p_{mi}-y_{mi})$$\n",
    "   </font>\n",
    "</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\" color='#270336' face = \"Times New Roma\">\n",
    "     &nbsp;&nbsp;\n",
    "        Althought the eq.(6) seems so simple and elegant  it is written is tenzor form not in matrix one. Therefore  its implementation becomes more dificult, espesialy when we want to use our lovely library numpy.But we can write the equation in matrix form seeming like that :\n",
    "    </font>\n",
    "</h7>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " <font size=\"3\" color='#4a3e20' >  \n",
    "      &nbsp;&nbsp;\n",
    " $\\nabla_W L = \\begin{bmatrix}\n",
    "               \\nabla_{w_{11}} L  & \\nabla_{w_{12}}L &... &\\nabla_{ w_{1j}}L \\\\\n",
    "               \\nabla_{ w_{21}}L & \\nabla_{ w_{22}}L  &... &\\nabla_{ w_{2j}}L \\\\\n",
    "                ...   &  ...  & ... & ...  \\\\\n",
    "                \\nabla_{ w_{i1}}L  & \\nabla_{ w_{i2}}L  & ...& \\nabla_{ w_{ij}}L \\end{bmatrix} $\n",
    "$  =\\begin{bmatrix}\n",
    "p_{11} -y_{11}  &  p_{21}-y_{21}  & ... &  p_{m1}-y_{m1}\\\\\n",
    "p_{12} -y_{12}  &  p_{22}-y_{22}  & ... &  p_{m2}-y_{m2}\\\\\n",
    "\\;\\;...\\;\\;\\;   &  \\;\\;...\\;\\;\\;  &\\;\\;...\\;\\;\\; &\\;\\;...\\;\\;\\; \\\\\n",
    "p_{1i} -y_{1i}  &  p_{2i}-y_{2i}  & ... &  p_{mi}-y_{mi}\n",
    " \\end{bmatrix}$\n",
    "$\\begin{bmatrix} x_{11} &  x_{12} &...&  x_{1j} \\\\  x_{21} &  x_{22}  &...&  x_{2j}  \\\\ ... & ... & ... &... \\\\ x_{m1} &  x_{m2} &...&  x_{mj}\\end{bmatrix}$\n",
    "   </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " <font size=\"4\" color='#4a3e20' >  \n",
    "      $$8)\\;\\;\\;\\;\\nabla_W L = (P-Y)^T.X$$\n",
    "   </font>\n",
    "\n",
    "\n",
    " <font size=\"4\" color='#4a3e20' >  \n",
    "      $$9)\\;\\;\\;\\;\\nabla_{b} L = \\sum_m (P-Y)_{mi}$$\n",
    "   </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5 style=\"margin-right: 50px; margin-left:10px\">\n",
    " <font face=\"Times New Roma\" size=\"3\" color='#270336' >\n",
    "      &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "      The eq(8) can be proved only with direct verification, you can try to  proof it !!! See why this is correct!!!\n",
    "  </font> \n",
    "<h5>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\" color='#270336' face = \"Times New Roma\">\n",
    "     We've  written eq(6) and(7) in matrix form, and  result is surprisingly simple and  so easy for implementation.If we plug the eq(8) and (9) in gradient descent eqaution <br> \n",
    "    </font>\n",
    "    <font size=\"3\" color='#4a3e20' >   \n",
    "    $$ \\;  \\; \\;  \\; \\; \\; \\;\\begin{matrix} w_{ij} = w_{ij} - \\lambda \\nabla w_{ij}L(W,b) \\\\  \\\\ b_{j} = b_j - \\lambda\\nabla b_{j}L(W,b)\n",
    "       \\end{matrix} $$ \n",
    "</font>\n",
    "<font size=\"4\" color='#270336' face = \"Times New Roma\">\n",
    "    We've achieve our minimization alogithm for Cross-entopy Loss for find the best esitmitators $W_{ij}$.\n",
    "    The minimization alogorithm using gradient descent is deifined as :  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<font size=\"5\" color='#4a3e20' > \n",
    "    $$(10) \\;  \\; \\;  \\; \\; \\; \\;  W= W -\\lambda(P-Y)^T.X  $$\n",
    "  <br>\n",
    "    $$(11) \\;  \\; \\;  \\; \\; \\; \\;  b_i = b_i -\\lambda \\sum_m(P-Y)_{mi}   $$\n",
    "</font>      \n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "<h5 style=\"margin-right: 50px; margin-left:10px\">\n",
    " <font face=\"Times New Roma\" size=\"3\" color='#270336' >\n",
    "      where W is a weight matrix , $\\lambda$ is an leraning rate or step size, P is the the prediction values pruduced from sofmax   $Y$ is the target values $X$ is the training data (features vectors).\n",
    "    \n",
    "</font> \n",
    "<h5>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font id=\"gradient\" size=\"1\">....</font>\n",
    "<h2 color='#270336' face = \"Times New Roma\"> &nbsp; Implementation of Gradient descent algorithm<h2>\n",
    "  \n",
    "<font size=\"3\" color='#270336' face = \"Times New Roma\">\n",
    "   &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;The implentation  using <mark color='blue' size='5'>numpy</mark>\n",
    "     could be defined as : <br> <br>:\n",
    " <font size=\"3\" color='#4a3e20' >   \n",
    "  $$ W = W - gamma*(( softmax(W,b,X,Y) -Y )^T).dot(X)$$ <br>\n",
    "      $$ b = b_i - gamma*sum_m((softmax(W,b,X,Y)_i -Y_i)) $$\n",
    " </font> \n",
    " <br>\n",
    " </h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5 style=\"margin-right: 50px; margin-left:50px\">\n",
    " <font face=\"Times New Roma\" size=\"3\" color='#270336' >\n",
    "      where the arguments are considered: the W matrix our estimator coeficients , $gamma=$ $\\lambda* (1/m)$ or step size,   $Y$ is the target values $X$ is the training data (features vectors).\n",
    "      \n",
    "</font>\n",
    " </h5> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, W, b, step_size):\n",
    "    ''''\n",
    "    perform gradient descent one iteration\n",
    "    \n",
    "     Parameters :\n",
    "    X : ndarray\n",
    "       train data\n",
    "    y : ndarray\n",
    "       target data\n",
    "    W : ndarray\n",
    "     weight matrix\n",
    "    b : ndarray\n",
    "      bias \n",
    "    step_size : float\n",
    "       gradient descent setting\n",
    "    \n",
    "    '''\n",
    "    P_y = softmax(X,W,b)-y\n",
    "    W = W - step_size*(P_y.T).dot(X)\n",
    "    b = b - step_size*np.sum(P_y, axis=0)\n",
    "    return W,b\n",
    "    \n",
    "\n",
    "def train(X , y, max_iter=300,learning_rate=0.1,innitial_value =1, debug_W=None):\n",
    "    '''\n",
    "    Train by softmax regression \n",
    "    \n",
    "    Parameters :\n",
    "    X : ndarray\n",
    "       train data\n",
    "    y : ndarray\n",
    "       target data\n",
    "    max_iter : int \n",
    "        number of epoch (iterations)\n",
    "        \n",
    "    debug_W : tuple \n",
    "       index of weight parameter for debugging\n",
    "    Returns \n",
    "       W, b : ndarray\n",
    "         weight and bias\n",
    "       in debug mode\n",
    "       W, b, k : ndarray\n",
    "          weight and bias and debugind parameter \n",
    "         \n",
    "       \n",
    "    '''\n",
    "    \n",
    "    if type(X) != np.ndarray or type(y) != np.ndarray: \n",
    "        raise ValueError('X and y must be ndarray')\n",
    "        \n",
    "    #init weight and bias\n",
    "    b = np.full((y.shape[1],),innitial_value)\n",
    "    W = np.full((y.shape[1], X.shape[1]), innitial_value)\n",
    "    \n",
    "    m = X.shape[0] \n",
    "    step_size = (1/m)*learning_rate\n",
    "    \n",
    "    if debug_W is not None: \n",
    "        debug_mode=True \n",
    "        debug = W[(debug_W)]\n",
    "    else :\n",
    "        debug_mode=False\n",
    "        \n",
    "    for i in range(max_iter):\n",
    "        W,b = gradient_descent(X , y, W, b,step_size)\n",
    "        if debug_mode: debug = np.append(debug, W[debug_W])   \n",
    "    \n",
    "    \n",
    "    if debug_mode: return W,b,debug\n",
    "        \n",
    "    return W,b\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#W,b,k = train(X, y, max_iter= 10**3 ,learning_rate= 10,debug_W =(1,0) )   \n",
    "W,b,k = train(np.array(X_train), np.array(y_train),debug_W=(1,1))   \n",
    "#np.round(k,10)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = softmax(np.array(X_train), W,b)\n",
    "accuracy(np.array(y_train), v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font id=\"reg\" size=\"1\">....</font>\n",
    "<h2  color='#270336' face = \"Times New Roma\">Regularization by learning rate and max iterations</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
