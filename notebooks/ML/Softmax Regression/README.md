<h1>
   <font size="5" face = "Times New Roma" color='#270336'>
     The optimization of Softmax function by  Cross-entropy Loss 
   </font> 
 </h1>
<h6>


<font face = "Times New Roma" size="4.5"  color='#270336' style="margin-right: 45px; margin-left: 45px" >
  &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; This paper contains a mathematical proof and implementation of Gradient Descent formula used as optimization alogithm in Cross-entropy Loss. We will use the Cross-entropy Loss or so-called Softmax Loss as an error function in the process of fitting estimator parameters. This notebook is focused on the mathematics behind softmax regression rather than its application in multi-class classification.We will use the Iris dataset in order to represent the training data process using the softmax implementation. In addition, we will see the experiments related to leaning step and max iterations  
 </font>    
<br> <br>


<h2 face = "Times New Roma" size="4.5" >Project contains </h2>
<br>
  <font size="4" face = "Times New Roma" color='#3f134f' > 
    <ul style="margin-left: 30px">
      <li><a href='https://daodavid.github.io/Machine-Learning/pages/html/ML/softmax-regression/The%20optimization%20of%20Softmax%20function%20by%20%20Cross-entropy%20Loss.html#deff_softmax'>What is Softmaxt  and  how it works?</a></li>
        <br>
      <li><a href='https://daodavid.github.io/Machine-Learning/pages/html/ML/softmax-regression/The%20optimization%20of%20Softmax%20function%20by%20%20Cross-entropy%20Loss.html#cross_entropy'>Definition of Cross-entropy Loss</a></li>
        <br>  
      <li><a href='https://daodavid.github.io/Machine-Learning/pages/html/ML/softmax-regression/The%20optimization%20of%20Softmax%20function%20by%20%20Cross-entropy%20Loss.html#optimization'>Optimization of softmax by Cross-entropy Loss and derivation of Gradient descent formula </a> </li>
        <br>
       <li><a href='https://daodavid.github.io/Machine-Learning/pages/html/ML/softmax-regression/The%20optimization%20of%20Softmax%20function%20by%20%20Cross-entropy%20Loss.html#gradient'>Implementation of Gradient descent algorithm </a> </li>
        <br> 
      <li><a href='https://daodavid.github.io/Machine-Learning/pages/html/ML/softmax-regression/The%20optimization%20of%20Softmax%20function%20by%20%20Cross-entropy%20Loss.html#reg'>Regularization of gradient descent by manipulation of learning rate and max iterations</a></li><br>     
</ul>    
</font>
</h6>    
    
