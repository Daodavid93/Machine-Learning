<h1>
   <font size="5" face = "Times New Roma" color='#270336'>
     Softmax regression 
   </font> 
 </h1>
 <br> <br>
<h2>
   <font size="5" face = "Times New Roma" color='#270336'>
     The minimization Cross-entropy Loss used in Softmax regression 
   </font> 
 </h2>
 
 
<h6>


<font face = "Times New Roma" size="4.5"  color='#270336' style="margin-right: 45px; margin-left: 45px" >
  &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; This paper contains a mathematical proof and implementation of Gradient Descent formula used as optimization alogithm in Cross-entropy Loss. We will use the Cross-entropy Loss or so-called Softmax Loss as an error function in the process of fitting estimator parameters. This notebook is focused on the mathematics behind softmax regression and its implementation rather than its application in multi-class classification. We will use the Iris dataset in order to perform the process of training data using the softmax implementation. In addition, we will see  how are different learning rates and max iterations impact on training algorithm </font> <br>  
<br> <br>


<h2 face = "Times New Roma" size="4.5" >Project contains </h2>
<br>
  <font size="4" face = "Times New Roma" color='#3f134f' > 
    <ul style="margin-left: 30px">
      <li><a href='https://daodavid.github.io/maths-behind-ML/pages/html/ML/softmax-regression/The%20optimization%20of%20Softmax%20function%20by%20%20Cross-entropy%20Loss%20.html#deff_softmax'>What is Softmaxt  and  how it works?</a></li>
        <br>
      <li><a href='https://daodavid.github.io/maths-behind-ML/pages/html/ML/softmax-regression/The%20optimization%20of%20Softmax%20function%20by%20%20Cross-entropy%20Loss%20.html#cross_entropy'>Definition of Cross-entropy Loss</a></li>
        <br>  
      <li><a href='https://daodavid.github.io/maths-behind-ML/pages/html/ML/softmax-regression/The%20optimization%20of%20Softmax%20function%20by%20%20Cross-entropy%20Loss%20.html#optimization'>Optimization of softmax by Cross-entropy Loss and derivation of Gradient descent formula </a> </li>
        <br>
       <li><a href='https://daodavid.github.io/maths-behind-ML/pages/html/ML/softmax-regression/The%20optimization%20of%20Softmax%20function%20by%20%20Cross-entropy%20Loss%20.html#gradient'>Implementation of Gradient descent algorithm </a> </li>
        <br> 
      <li><a href='https://daodavid.github.io/maths-behind-ML/pages/html/ML/softmax-regression/The%20optimization%20of%20Softmax%20function%20by%20%20Cross-entropy%20Loss%20.html#reg'>Regularization of gradient descent by learning rate and max iterations</a></li><br>     
</ul>    
</font>
</h6> 

   <font face = "Times New Roma" size="3"  color='#270336' style="margin-right: 45px; margin-left: 45px">
 Step by step from math to implementation of multi-class classification.That could be used as a guide for implementation of logistic regression and some notes related to optimization could be useful for neural networks
    </font>
   
    
    
