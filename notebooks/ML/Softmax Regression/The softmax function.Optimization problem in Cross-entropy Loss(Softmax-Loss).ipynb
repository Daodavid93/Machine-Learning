{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "   <font size=\"5\" color='#1a092b'>\n",
    "     The softmax function.Optimization problem in Cross-entropy Loss\n",
    "   </font> \n",
    "    <br> \n",
    "</h1>\n",
    "<h6>\n",
    "  <font size=\"4\" color='#310959'>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; How the Softmax works? <br> <br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; Which is used as an error function in Softmax regression?  <br> <br>\n",
    "         &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; How we can find a maths expression of Gradient descent used in Cross-entropy Loss ? <br>     \n",
    "   </font>\n",
    "</h6> \n",
    "   <br><br><br>\n",
    "<h2>\n",
    "  <p>\n",
    "    <a href =   \"https://github.com/daodavid\" > \n",
    "       <img src=\"https://cdn.thenewstack.io/media/2014/12/github-octocat.png\" align=\"left\" width=\"120\"  alt=\"daodavid\" >\n",
    "    </a>\n",
    "    <font size=\"4\"  color='1a092b'>\n",
    "        author: daodeiv (David Stankov) \n",
    "    </font>\n",
    "</p>      \n",
    "<br> <br> <br>\n",
    "</h2>   \n",
    "<h7>\n",
    "   <font size=\"2\" color='#310959' >\n",
    "  &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; This paper contains a mathematical proof and implementation of Gradient Descent expression used as optimization alogithm in Cross-entropy Loss. We will use the Cross-entropy Loss or so-called Softmax Loss as an error function in the process of fitting estimator parameters(weights) in Softmax function. This notebook is focused on the mathematics behind softmax regression rather than its application in multi-class classification.In addition, there is an example of training data in order to represent a litle of practical part of Softmax regression.\n",
    "  </font>\n",
    "</h7>\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "   <font size=\"3\" color='#310959'>\n",
    "       &nbsp;&nbsp;&nbsp; The standart softmax function $\\sigma: \\; \\Re^k \\; \\rightarrow \\; \\Re^k $ is defined by formula : <br>\n",
    "  </font>   \n",
    "  <br>\n",
    "  <font size=\"5\" color='#4a3e20' >\n",
    "    $$\\sigma(z)_i=\\phi_{softmax (z)_i}=\\frac{e^z_i}{\\sum_{j=1}^n e^{z}_{j}}$$  \n",
    "  </font>\n",
    "  <br>\n",
    "  <font size=\"3\" color='#310959' > \n",
    "  The softmax function  takes as an input a vector $\\vec z$ with $K$ number of component, and normalized it into  a  probability distribution consisting of  $K$ number of probabilities proportional to exponentials of input values.That is, prior to applying softmax function some vector components could be negative or greater than one and  might not sum up to 1.\n",
    "Furthermore more the larger input components correspond to larger probabilities. \n",
    "  </font>    \n",
    "</h1>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h6> <font size=\"3\" color='#310959'>  $\\; \\; $ Before we getting deeper into the above equation, we gonna generate our learning data. The data consist of records contains different types of vehicle together with their physical sizes. Since our purpose is to diving into the concepts of Softmax (not to show great predictions) the dataset is very simple and we would be able to do prediction using only the power of our brain without any ML algorithms. The simplicity of the dataset will help us to easier understand the math concepts behind softmax easier. \n",
    "    </font> \n",
    "  </h6>\n",
    "<br>\n",
    "<h1>\n",
    "<font size=\"3\" color='#310959'>\n",
    "        &nbsp;&nbsp; Let to generate the training data : <br>\n",
    "</font>    \n",
    "</h1>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_size</th>\n",
       "      <th>y_size</th>\n",
       "      <th>z_size</th>\n",
       "      <th>label_Bus</th>\n",
       "      <th>label_Car</th>\n",
       "      <th>label_Tractor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.702559</td>\n",
       "      <td>3.065758</td>\n",
       "      <td>3.115231</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.929419</td>\n",
       "      <td>3.715173</td>\n",
       "      <td>3.468602</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.718284</td>\n",
       "      <td>3.799914</td>\n",
       "      <td>3.175256</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.131624</td>\n",
       "      <td>3.193786</td>\n",
       "      <td>3.931607</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.005559</td>\n",
       "      <td>3.023722</td>\n",
       "      <td>3.924982</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.618569</td>\n",
       "      <td>1.771616</td>\n",
       "      <td>1.741405</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.525551</td>\n",
       "      <td>1.605327</td>\n",
       "      <td>1.093550</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.939387</td>\n",
       "      <td>1.640885</td>\n",
       "      <td>1.073942</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.385039</td>\n",
       "      <td>1.505721</td>\n",
       "      <td>1.424187</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.920732</td>\n",
       "      <td>1.021843</td>\n",
       "      <td>1.037170</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      x_size    y_size    z_size  label_Bus  label_Car  label_Tractor\n",
       "0   3.702559  3.065758  3.115231          0          0              1\n",
       "1   3.929419  3.715173  3.468602          0          0              1\n",
       "2   3.718284  3.799914  3.175256          0          0              1\n",
       "3   3.131624  3.193786  3.931607          0          0              1\n",
       "4   3.005559  3.023722  3.924982          0          0              1\n",
       "..       ...       ...       ...        ...        ...            ...\n",
       "25  1.618569  1.771616  1.741405          0          1              0\n",
       "26  1.525551  1.605327  1.093550          0          1              0\n",
       "27  1.939387  1.640885  1.073942          0          1              0\n",
       "28  1.385039  1.505721  1.424187          0          1              0\n",
       "29  1.920732  1.021843  1.037170          0          1              0\n",
       "\n",
       "[90 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generate  records refer to tractor label with random physical size between [4,5] \n",
    "tractor_dataframe= pd.DataFrame(data=np.random.random((30, 3))+3,columns = ['x_size','y_size','z_size'])\n",
    "tractor_dataframe['label'] =pd.DataFrame( np.full((1,30 ), 'Tractor').T)\n",
    "\n",
    "#generate  records refer to car label with random physical size between [1,2]  \n",
    "car_dataframe= pd.DataFrame(data=np.random.random((30, 3)) + 1,columns = ['x_size','y_size','z_size'])\n",
    "car_dataframe['label'] =pd.DataFrame( np.full((1,30 ), 'Car').T)\n",
    "\n",
    "#generate  records refer to car label with random physical size between [2,3]  \n",
    "bus_dataframe= pd.DataFrame(data=np.random.random((30, 3))+2,columns = ['x_size','y_size','z_size'])\n",
    "bus_dataframe['label'] =pd.DataFrame( np.full((1,30 ), 'Bus').T)\n",
    "\n",
    "# joint each data frame into  one\n",
    "data = tractor_dataframe.append(bus_dataframe).append(car_dataframe)\n",
    "data = pd.get_dummies(data) \n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<font size=\"3\" color='#310959'>\n",
    "&nbsp;&nbsp; The label values label_car, label_bus, and label_tractor are represented as one-hot encoding variable (dummies) in order to be used in mathematical manipulation.\n",
    "<br> <br>\n",
    "  &nbsp;&nbsp;Let to visualize the data  using  Scatter plot :  \n",
    "</font>    \n",
    "</h1>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1de8f36fa08>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(tractor_dataframe['x_size'],tractor_dataframe['y_size'],label='Tractor',color='blue')\n",
    "plt.scatter(car_dataframe['x_size'],tractor_dataframe['y_size'],label='Car',color='red')\n",
    "plt.scatter(bus_dataframe['x_size'],tractor_dataframe['y_size'],label='Bus',color='yellow')\n",
    "plt.xlabel('x size')\n",
    "plt.ylabel('y size')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x train (m, n) (60, 3)\n",
      "shape of y label (m, k) (60, 3)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.array(data.drop(['label_Bus','label_Car','label_Tractor'], axis = 1)) # gets the target label variables\n",
    "y_train = np.array(data[['label_Bus','label_Car','label_Tractor']]) # gets feature variables \n",
    "X_train, X_test, y_train, y_test = train_test_split(x_train,y_train, test_size=0.33, random_state=42) #separats into test and train samples \n",
    "\n",
    "print('shape of x train (m, n)', X_train.shape)\n",
    "print('shape of y label (m, k)', y_train.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "  <br>\n",
    "<font size=\"3\" color='#310959'>     \n",
    " &nbsp;&nbsp; The dataset consist of $X^{m\\times n}$ features vectors and $Y^{m\\times  k}$ target (class) vectors.\n",
    "    Where $m = 60$ (observations), $n=3$ attributes (predictors), (k=3) target(class) variables. For example, $y_{32}=2$ that means a record 3 $x_{3i} $ belongs to class 2(Car). <br>\n",
    "    Given our data, the softmax function will be written as  : <br>\n",
    "</font>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font size=\"3\" color='#4a3e20' >\n",
    "   $$(1) \\; \\; \\; \\; \\; \\;p_{ij}=\\phi_{softmax (z)_{ij}}=\\frac{e^{z_{ij}}}{\\sum_l^k e^{z_{il}}} $$ <br>\n",
    "            $$\\; \\; \\;z_{ij} = \\sum_p^3 x_{ip} w_{jp} + b_j $$\n",
    "  </font>            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<font size=\"3\" color='#310959'>    \n",
    "     &nbsp;&nbsp; where $w_{jp}\\in W$ .$W$ is an weght matrix, where $w_{jp}$ is the estimators the prediction process of softmax function is based on $w_{jp}$.Acording to our dataset $W^{jp}=W^{33}$ because the dataset have $n=3$ attributes (predictors) , (k=3) class  <br> and $b_j $ is bias for vector, \n",
    "  </font>\n",
    " <br> <br> \n",
    "    <font size=\"3\" color='#4a3e20' >  <br> \n",
    "      &nbsp;&nbsp;&nbsp;&nbsp;   $ W= \\begin{bmatrix} weight^1\\rightarrow class \\; 1(bus) \\\\ weight^2\\rightarrow class\\;  2(car) \\;   \\\\ weight^{3}\\rightarrow class \\;3(tractor) \\;  \\end{bmatrix} =\n",
    "    \\begin{bmatrix} \\vec w^1 \\\\  \\vec w^2\\  \\\\ \\vec w^3  \\end{bmatrix} =\\begin{bmatrix} w_{11} & w_{12} & w_{13} \\\\ w_{21} & w_{22} & w_{23} \\\\ w_{31} & w_{32} & w_{33} \\end{bmatrix}  $ &nbsp;&nbsp;&nbsp;&nbsp;    $ B= \\begin{bmatrix}  b_1 \\\\  b_2\\  \\\\  b_3  \\end{bmatrix}\\;\\;\\;$\n",
    "   </font>\n",
    "   <br> <br> \n",
    "  <font size=\"3\" color='#310959'>  \n",
    " &nbsp;&nbsp; vector $\\vec w^1=[w_{11},w_{12},w_{13}]=w^{1j}$ is estimator vecotor for class 1 (bus), and $w_{12}$ is related to $x_{m2}$ attribute(predictor), written in matrix form : <br> <br>\n",
    "   </font>\n",
    "  </h1>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"3\" color='#4a3e20' >  <br> \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;  $Z = XW^T$ \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;  $Z = \n",
    "    \\begin{bmatrix} x_{11} &  x_{12} & x_{13} \\\\  x_{21} &  x_{22} & x_{23} \\\\ ... & ... & ... \\\\ x_{n1} &  x_{n2} & x_{n3} \\end{bmatrix} \\times \\begin{bmatrix} w_{11} & w_{21} & w_{31}  \\\\ w_{12} & w_{22} & w_{32} \\\\  w_{13} & w_{23} & w_{33}\\end{bmatrix} + \\begin{bmatrix}  b_1 \\\\  b_2\\  \\\\  b_3  \\end{bmatrix}  =   \\begin{bmatrix} z_{11} &  z_{12} & z_{13} \\\\  z_{21} &  z_{32} & z_{33} \\\\ ... & ... & ...  \\\\ z_{n1} &  z_{n2} & z_{n3}  \\end{bmatrix} $ <br>   \n",
    "   </font>\n",
    "</h1>   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "   <font size=\"3\" color='#310959'>  \n",
    "   &nbsp;&nbsp; The softmax function computes the probability that a training example $x^{(i)}=x_{in}$  belongs to class $y^{j}$ given \n",
    "    the weight  matrix $W$ and bias $\\vec b$ . <br> \n",
    "    So we compute the probability : \n",
    "  </font>\n",
    "  <font size=\"3\" color='#4a3e20' >\n",
    "      $$p_{ij}=P(y=j | x^{(i)};W) = \\phi_{softmax (z)_{ij}}=\\frac{e^{z_{ij}}}{\\sum_l^k e^{z_{il}}} $$\n",
    "   </font>   \n",
    "     \n",
    "   \n",
    "</h1>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<font size=\"3\" color='#4a3e20' >\n",
    "   $ \\phi_{softmax} = P = \\begin{bmatrix} p(y=1 |x^{1} ; W )_{11} &  p(y=2 |x^{1} ; W )_{12}  &  p(y=3 |x^{1} ; W )_{13} \n",
    "        \\\\ ... &  ...  & ...\n",
    "        \\\\ P(y=1 |x^{m} ; W ) &  P(y=2 |x^{m} ; W )  & P(y=3 |x^{m} ; W )\n",
    "        \\end{bmatrix} = \n",
    "        \\begin{bmatrix} \\frac{e^{z_{11}}}{\\sum_{1j}e^{z_{1j}}} & \\frac{e^{z_{12}}}{\\sum_{1j}e^{z_{1j}}} & \\frac{e^{z_{13}}}{\\sum_{1j}e^{z_{1j}}} \n",
    "        \\\\   \\\\ ... & ... & ...  \\\\\n",
    "        \\\\    \\frac{e^{k_{n1}}}{\\sum_{j}^3e^{z_{nj}}} & \\frac{e^{z_{n2}}}{\\sum_{j}^3e^{z_{nj}}} & \\frac{e^{z_{n3}}}{\\sum_{j}^3e^{z_{nj}}}\\end{bmatrix} $\n",
    "</h1>        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h1>\n",
    " <font size=\"3\" color='#310959'>  \n",
    "  for example, the probability record one $x^1$ from dataset to belongs to class  $y_2$ (car) we be calculated as:\n",
    "  </font>\n",
    "  <br> <br> \n",
    "  <font size=\"3\" color='#4a3e20' >\n",
    "     $$p_{12} =P(y=2 |x^{1};\\; W ) = \\frac{ e^{ ^{z_{12}} } }{ \\sum_p^3 e^{z_{1p}}}=\\frac{ e^{ (^{\\sum_v^3 x_{1v}.w_{vj} + b_v }} )}{ \\sum_p^3 e^{ ^ ({\\sum_v^3 x_{1v}.w_{2v}} + b_2})}$$\n",
    "</h1>\n",
    " <h1>\n",
    " <font size=\"3\" color='#310959'>  \n",
    "   &nbsp;&nbsp; Due to denominator $\\sum_v^3 x_{1v}.w_{2v} + b_2$ always the  $p_{12}=\\frac{e^{z_{ij}}}{\\sum_l^k e^{z_{il}}}\\in [0,1]$\n",
    "     and to all $p_{ij} $ as well.\n",
    "  </font>\n",
    " </h1>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"3\" color='#310959'>  \n",
    "    &nbsp;&nbsp; Let to see how the softmax function  can be applied concretely in our training dataset :\n",
    "             First, let to define a weight matrix $W$ and bias $\\vec b$ <br> <br>\n",
    " <font>  \n",
    "     <br> <br>\n",
    "  <font size=\"3\" color='#4a3e20' >\n",
    "   &nbsp;&nbsp;    $W = \\begin{bmatrix} w_{11} & w_{12} & w_{13} \\\\ w_{21} & w_{22} & w_{23} \\\\ w_{31} & w_{32} & w_{33}                      \\end{bmatrix} \n",
    "                   =  \\begin{bmatrix} 0.75776811 & 0.6690284 & -0.02646755 \\\\\n",
    "                                     -2.94775864 &  -3.06439323 & -2.14043833\\\\\n",
    "                                      2.18999053 & 2.39536483 &  2.16690588 \\end{bmatrix}$\n",
    "     &nbsp;&nbsp; $B = \\begin{bmatrix}  b_1 \\\\  b_2\\  \\\\  b_3  \\end{bmatrix} = \\begin{bmatrix}  -0.15425504 \\\\ 18.81781451 \\\\ -18.66355947 \\end{bmatrix} $\n",
    "  </font>     \n",
    "\n",
    "</h1>\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>  \n",
    "  <font size=\"3\" color='#310959'>  \n",
    "   &nbsp;&nbsp; I've prepared weight vector $W$ and bias $B$ in advance, how? We will see later. <br> <br>\n",
    "   </font>\n",
    "</h5>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.array([[ 0.75776811,  0.6690284 , -0.02646755],\n",
    "              [-2.94775864, -3.06439323, -2.14043833],\n",
    "              [ 2.18999053 , 2.39536483,  2.16690588]]) # define a weight matrix\n",
    "\n",
    "B = intercept = np.array([ -0.15425504 , 18.81781451, -18.66355947])  #bias vector (intercept) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X, W, B):\n",
    "    '''\n",
    "    softmax function  \n",
    "    takes :\n",
    "    X = training data\n",
    "    W =  weight matrix\n",
    "    b = bias vector (intercept) \n",
    "    return :\n",
    "      softmax for every z unit e^{k_ij}/Sum(e^{k_i1}+e^{k_i2}+e^{k_i3})\n",
    "    '''\n",
    "    #dot product between X_data matrix  and tranposed Weight_ matrix added to Bias  gives matrix each z_ij\n",
    "    Z = X.dot(W.T)+B \n",
    "    \n",
    "    #return matrix cosist of exponentials Z input net\n",
    "    exp_z = np.exp(X.dot(W.T)+B)\n",
    "    \n",
    "    #array contains sum  of every row  (e^z_{ik})\n",
    "    sums=np.sum(exp_z, axis=1) \n",
    "    \n",
    "    #return softmax(Z)_{ij}\n",
    "    return (exp_z.T/sums).T \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original data label Y :  \n",
      "[[0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]]\n",
      "\n",
      "predicted data label Y' :\n",
      "[[0.04 0.96 0.  ]\n",
      " [0.28 0.   0.72]\n",
      " [0.95 0.04 0.  ]\n",
      " [0.98 0.01 0.01]\n",
      " [0.41 0.   0.59]\n",
      " [0.98 0.   0.01]\n",
      " [0.98 0.   0.01]\n",
      " [0.34 0.   0.66]\n",
      " [0.01 0.99 0.  ]]\n"
     ]
    }
   ],
   "source": [
    "print('original data label Y :  ')\n",
    "print(y_train[1:10])\n",
    "print('')\n",
    "print(\"predicted data label Y' :\")\n",
    "print(np.around(softmax(X_train,W,intercept)[1:10],2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"3\" color='#310959'>  \n",
    "    &nbsp;&nbsp;From $Y'$ output let to consider the first row : <br> <br>\n",
    "            $Y'_{11} = 0.01 \\rightarrow  $ has  0.1% chance that $x^{1}$ belongs to class 1(BUS) <br> \n",
    "            $Y'_{12} = 0.98 \\rightarrow  $ has 98% chance that row $x^{1}$ belongs to class 2(CAR) <br> \n",
    "            $Y'_{13} = 0.01 \\rightarrow  $ has 0.1% chance that  $x^{1}$belongs to class 3(TRACTOR) <br> <br>\n",
    "            From above considering we do a conclusion record $X_1$ belongs to class Car because has the 90% chance and it is the largest one. <br> <br> <br>\n",
    "      If we compare reat target $Y$ and out prediction label $Y'$ we will notice that our prediction has 100% accuracy.        <br>\n",
    "How was I  found a weight matrix $W$ and bias $B$ ? <br> \n",
    "Just I used the LogisticRegression from scikit-learn and took the coefficients, but our purpose is to understand the way   of finding the weight $W$ and bias $\\vec b$ . \n",
    "</h1>\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"3\" color='#310959'>  \n",
    "    &nbsp;&nbsp; The loss function wich we will use for Softmax regression is called Cross-Entropy Loss or Sofmax Loss and it is defined as \n",
    " <font>    \n",
    "  <br> <br>   \n",
    "  <font size=\"5\" color='#4a3e20' >\n",
    "    $$ (2) L(W,b)=-\\sum_i^m\\sum_j^k y_{ij} \\log (p(Z)_{ij})$$ <br> <br>  \n",
    "  </font>\n",
    "  <font size=\"2\" color='#310959' > \n",
    "       $\\;\\;\\;\\;\\;\\;$Where $m$ is a count of records , k count of classes $y_{ij}$ is label values , $p_{ij}=\\phi_{softmax}(Z)_{ij}$ are  $Y'_{ij}$ predicited class  values, $W$ is weight matrix and $b$ bias. \n",
    "    </font>  \n",
    "  <br> <br>  \n",
    " <font size=\"3\" color='#310959'>  \n",
    "  &nbsp;&nbsp;  Why will we use these Loss function is a question which brings by itself much reading and investigations.Here <a href='https://daodavid.github.io/Machine-Learning/pages/html/ML/logistic-regression/Cross-entropy%20function.Investigation%20and%20gradient%20descent.html'>cross-entropy</a> there are some examples and investigations related to Minimum cross-entropy used in  Sigmoid regression. That can give you some intuitions.<br>\n",
    "Out goal is to mimimaize the eq.(2) in order to find best estimators   $w_{ij}\\in W$ and $b_j$ given \n",
    " a training data $X$  and a label data $Y$. <br>\n",
    "    To do that we will apply well known Gradient descent as a optimization model. \n",
    "    Note that, the eq.(2) is the function of all weights $w_{ij}$, bias $b_j$ all training data X and label data Y.        \n",
    " <font>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"3\" color='#310959'>  \n",
    " &nbsp;&nbsp;  Gradient descent is defined as  : \n",
    "</font>\n",
    "<br>    \n",
    "<font size=\"3\" color='#4a3e20' >   \n",
    "    $$(2) \\;  \\; \\;  \\; \\; \\; \\;\\begin{matrix} w_{ij} = w_{ij} - \\lambda \\nabla w_{ij}L(W,b) \\\\  \\\\ b_{j} = b_j - \\lambda\\nabla b_{j}L(W,b)\n",
    "       \\end{matrix} $$ \n",
    " </font>\n",
    "  <font size=\"3\" color='#310959'>  \n",
    " &nbsp;&nbsp;  $\\lambda$ is learning rate (step size)  : \n",
    "</font>\n",
    "</h1>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"3\" color='#310959'>  \n",
    "&nbsp;&nbsp;  Plug gradient descent into eq.(1) : \n",
    "</font>\n",
    "    <br>\n",
    "    <font size=\"3\" color='#4a3e20' >   \n",
    "     $$(3) \\;  \\; \\;  \\; \\; \\; \\;\n",
    "   \\begin{matrix} \\nabla w_{ij}L(W,b)   &=  -\\frac{\\partial}{\\partial w_{ij}}\\Big(\\sum_k^m\\sum_n^n y_{mn} \\log {p_{mn}}\\Big) \\\\  \\\\\n",
    "     \\nabla b_{j}L(W,b) & =- \\frac{\\partial}{\\partial b_{j}}\\Big(\\sum_k^m\\sum_n^n y_{mn} \\log{p_{mn}}\\Big)\n",
    "       \\end{matrix} $$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>\n",
    " <font size=\"3\" color='#310959'>  \n",
    "   Before we take up  with  $\\nabla w_{ij}L(W,b)$. We gonna introduce some math technics which will make our work easier.\n",
    "   <br>\n",
    "   For simplicity in the summation process of indices, we will introduce a <a href='http://physics.csusb.edu/~prenteln/notes/vc_notes.pdf'>Kronecker symbol</a> .<br>   <br>\n",
    "</font>\n",
    "<font size=\"3\" color='#4a3e20' >  \n",
    "       $$\\delta_{ij} =    \n",
    "         \\begin{equation}\n",
    "   \\begin{Bmatrix} \n",
    "   1 & if \\; i=j  \\\\\n",
    "   0 & if \\; i\\ne j  \n",
    "    \\end{Bmatrix} \n",
    "\\end{equation}$$\n",
    "  <br><br>\n",
    "          $$ \\delta_{ij} = \\begin{bmatrix} 1 & 0 & 0  \\\\ 0 & 1 & 0 \\\\  0 & 0 & 1 \\end{bmatrix}$$\n",
    "   </font>\n",
    "<font size=\"3\" color='#310959'>  \n",
    "   In many places in the coming sum operations over indexes we will miss the $\\sum$ symbol, just it will be avoided(hidden) according to the .<a href='https://en.wikipedia.org/wiki/Einstein_notation'>Einstein summation convention</a> .<br>\n",
    "       For example, the equation. \n",
    "</font>\n",
    "</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>\n",
    "<font size=\"2\" color='#4a3e20' >   \n",
    "   $$z_{ij} = \\sum_p^3 x_{ip} w_{jp} + b_j $$ \n",
    "</font>\n",
    "<font size=\"3\" color='#310959'>  \n",
    "     by applying the  Enstein convetion we could rewrite it as : <br>\n",
    "</font>\n",
    "</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>\n",
    "<font size=\"3\" color='#4a3e20' >   \n",
    "   $$z_{ij} = x_{ip} w_{jp} + b_j$$  \n",
    "</font>\n",
    "<font size=\"3\" color='#310959'>  \n",
    "     The sign $\\sum_p^3$ is miss.The sumation over p  is implied(by default) because p is repeated twice.Every time when there are repeatable indices that is the indicator for exist of $\\sum$  which is just missing(The sum sign  is not written).\n",
    "</font>\n",
    "</h5>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<font size=\"3\" color='#310959' >  \n",
    "    &nbsp;&nbsp; Now we gonna to resolve the eqs.(3) : <br>\n",
    "</font>    \n",
    " </h1>  \n",
    " <br>\n",
    " <h5>\n",
    "    <font size=\"3\" color='#310959' >  \n",
    "    &nbsp;&nbsp;  Be ready for a hard part !!! : <br>\n",
    "</font> \n",
    "   <br> \n",
    "     <font size=\"2\" color='#310959' >  \n",
    "    &nbsp;&nbsp;If you do not familiar with indices calculation of Vector and Tensor Analysis, you can skip this part. <br>\n",
    "</font> \n",
    "</h5>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"4\" color='#4a3e20' >    \n",
    " &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;$\\frac{\\partial L}{\\partial w_{ij}}=-\\frac{\\partial}{\\partial w_{ij}}\\Big(\\sum_k\\sum_n y_{mn} \\log {p_{mn}}\\Big)$\n",
    " $=\\sum_k\\sum_n y_{mn}\\frac{\\partial \\log {p_{mn}}}{\\partial w_{ij}}$ \n",
    "  </font>\n",
    "</h1>     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"4\" color='#4a3e20' >    \n",
    " &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;$=-\\sum_m\\sum_n\\frac{y_{mn}}{p_{mn}}\\frac{\\partial  p_{mn}}{\\partial w_{ij}} $ \n",
    "            $=\\sum_m\\sum_n \\frac{y_{mn}}{p_{mn}}\\frac{\\partial  p_{mn}}{\\partial z_{vp}}\\frac{\\partial  z_{vp}}{\\partial w_{ij}} $\n",
    " </font>\n",
    "</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h6>\n",
    "  <font size=\"3\" color='#310959' >  \n",
    "      &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "     since  $z_{vp} = f(w_{pi}) \\Rightarrow \\frac{\\partial  z_{vp}}{\\partial w_{ij}} = 0\\;$ if $\\; p\\ne i$ then  we can  write $ \\frac{\\partial  z_{vp}}{\\partial w_{ij}}=\\delta_{pi}\\frac{\\partial  z_{vp}}{\\partial w_{ij}}$ <br>\n",
    "      &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "      and  $p_{mn} = f(z_{mv}) \\Rightarrow \\frac{\\partial  z_{vp}}{\\partial w_{ij}} = 0$ if $ m\\ne v $ then  we  can  write  $\\frac{\\partial p_{mn}}{\\partial z_{vp}}=\\delta_{mv}\\frac{\\partial  p_{mn}}{\\partial z_{vp}}  $ <br>\n",
    "       &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "       by replacing in our equation we will achieve \n",
    "    </font>\n",
    "</h6>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"4\" color='#4a3e20' >  \n",
    "     &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;$ =-\\sum_m\\sum_n \\frac{y_{mn}}{p_{mn}}\\delta_{mv}\\frac{\\partial  p_{mn}}{\\partial  z_{vp}} \\delta_{pi}\\frac{\\partial  z_{vp}}{\\partial w_{ij}} $\n",
    "    </font>\n",
    "</h1>     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>\n",
    "  <font size=\"3\" color='#310959' >  \n",
    "      &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "      using common Kronicker $\\delta$ proprties   \n",
    "  </font>\n",
    "</h6>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"4\" color='#4a3e20' >  \n",
    "     &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;$=-\\sum_m\\sum_n \\frac{y_{mn}}{p_{mn}}\\delta_{mm}\\delta_{ii}\\frac{\\partial  p_{mn}}{\\partial z_{mi}}\\frac{\\partial  z_{mi}}{\\partial w_{ij}} $ $=-\\sum_m\\sum_n \\frac{y_{mn}}{p_{mn}}\\frac{\\partial  p_{mn}}{\\partial z_{mi}}\\frac{\\partial  z_{mi}}{\\partial w_{ij}} $ \n",
    "    </font>\n",
    "</h1>     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"4\" color='#4a3e20' >    \n",
    " &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;$\\frac{\\partial L}{\\partial w_{ij}}=-\\sum_m\\sum_n \\frac{y_{mn}}{p_{mn}}\\frac{\\partial  p_{mn}}{\\partial z_{mi}}\\frac{\\partial  z_{mi}}{\\partial w_{ij}}$ \n",
    "  </font>\n",
    "</h1>     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"3\" color='#310959' >  \n",
    "     &nbsp;&nbsp;\n",
    "     We've successfully reduced the count of sum operations, using Einstein's convention and Kronecker symbol.<br>\n",
    "     Let to focus on terms $\\frac{\\partial  p_{mn}}{\\partial z_{mi}}$ and $\\frac{\\partial  z_{mi}}{\\partial w_{ij}}$\n",
    "    </font>\n",
    "</h1>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"4\" color='#4a3e20' >  \n",
    "      &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;\n",
    "     $\\frac{\\partial  p_{mn}} {\\partial z_{mi}}=\\frac{\\partial\\frac { e^{z_{mn}} }{ \\sum_ke^{z_{mk}}} }{\\partial z_{mi}}$\n",
    "     $=\\frac{1}{(\\sum_ke^{z_{mk}})^2}\\times \\Big(\\frac{\\partial e^{z_{mn}} }{\\partial z_{mi}}\\times(\\sum_ke^{z_{mk}}) - e^{z_{mn}}\\times\\frac{\\partial (\\sum_ke^{z_{mk}})}{\\partial z_{mi}}  \\Big)$\n",
    "   </font>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"4\" color='#4a3e20' >  \n",
    "     &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;\n",
    "     $=\\frac{e^{z_{mn}}\\times\\frac{\\partial z_{mn}}{\\partial z_{mi}}}{\\sum_ke^{z_{mk}}} - \\frac{e^{z_{mn}}}{\\sum_ke^{z_{mk}}}\\times\\frac{ \\sum_k e^{z_{mk}} \\frac{ \\partial z_{mk}}{\\partial z_{mi}}}  {\\sum_ke^{z_{mk}}}$\n",
    "   </font>\n",
    "</h1>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>\n",
    "  <font size=\"3\" color='#310959' >  \n",
    "      &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "      since $\\frac{\\partial z_{mk}}{\\partial z_{mi}}=0$ if $k\\ne i\\;\\frac{\\partial z_{mk}}{\\partial z_{mi}}=1\\;ifk = i\\;\\Rightarrow \\frac{\\partial z_{mk}}{\\partial z_{mi}}=\\delta_{ki}$ \n",
    "  </font>\n",
    "</h6>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"4\" color='#4a3e20' >  \n",
    "     &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;\n",
    "     $=\\frac{e^{z_{mn}}\\times \\delta_{ni} }{\\sum_ke^{z_{mk}}} - \\frac{e^{z_{mn}}}{\\sum_ke^{z_{mk}}}\\times\\frac{ \\sum_k e^{z_{mk}}\\delta_{ki}}  {\\sum_ke^{z_{mk}}}$ \n",
    "      $=\\frac{e^{z_{mn}}\\times \\delta_{ni} }{\\sum_k e^{z_{mk}}} - \\frac{e^{z_{mn}}}{\\sum_ke^{z_{mk}}}\\times\\frac{ \\sum_k e^{z_{mk}}\\delta_{ki}}  {\\sum_ke^{z_{mk}}}$ \n",
    "   </font>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"4\" color='#4a3e20' >  \n",
    "      &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;\n",
    "      $=\\frac{e^{z_{mn}}\\times \\delta_{ni} }{\\sum_ke^{z_{mk}}} - \\frac{e^{z_{mn}}}{\\sum_ke^{z_{mk}}}\\times\\frac{ \\sum_k e^{z_{mi}}\\delta_{ii}}  {\\sum_ke^{z_{mk}}}$ \n",
    "          $=\\frac{e^{z_{mn}}\\times \\delta_{ni} }{\\sum_ke^{z_{mk}}} - \\frac{e^{z_{mn}}}{\\sum_ke^{z_{mk}}}\\times \\frac{ e^{z_{mi}}}{\\sum_ke^{z_{mk}}}$ <br><br>\n",
    "   </font>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>\n",
    "  <font size=\"3\" color='#310959' >  \n",
    "      &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "      using eq.(1) $\\Rightarrow$ $\\frac{ e^{z_{mn}} }{ \\sum_k e^{z_{mk} } }=p_{mn}$ and $\\frac{ e^{z_{mi}} }{ \\sum_k e^{z_{mk} } }=p_{mi}$ <br>\n",
    "      &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; plugining it\n",
    "  </font>\n",
    "</h6>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"4\" color='#4a3e20' >  \n",
    "      &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;\n",
    "       $=p_{mn}\\times \\delta_{ni} - p_{mn}\\times p_{mn}p_{mi}$ <br>\n",
    "        &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;\n",
    "          $=p_{mn}(\\delta_{ni} -  p_{mi})$ \n",
    "   </font>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"4\" color='#4a3e20' >  \n",
    "      &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;\n",
    "     $$4) \\; \\; \\; \\; \\frac{\\partial  p_{mn}} {\\partial z_{mi}}=p_{mn}(\\delta_{ni} -  p_{mi})$$\n",
    "   </font>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"4\" color='#4a3e20' >    \n",
    " &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;\n",
    "     $\\frac{\\partial  z_{mi}}{\\partial w_{ij}} =\\frac{\\partial ( \\sum_k  x_{mk}w_{ki})}{\\partial w_{ij}}= \\frac{ \\sum_k z_{mi} x_{mk}\\partial w_{ki}}{\\partial w_{ij}}$ \n",
    "  </font>\n",
    "</h1>     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>\n",
    "  <font size=\"3\" color='#310959' >  \n",
    "      &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "      $\\frac{\\partial w_{ki}}{\\partial w_{ij}} = \\delta_{kj}$ only a direct verification can  proof it\n",
    "  </font>\n",
    "</h6>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"4\" color='#4a3e20' >  \n",
    "      &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;\n",
    "     $=  \\sum_k  x_{mk} \\delta_{kj} =\\sum_k  x_{mj}\\delta_{jj}=x_{mj}$  \n",
    "   </font>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"4\" color='#4a3e20' >  \n",
    "      $$(5) \\;\\;\\;\\;\\;\\frac{\\partial  z_{mi}}{\\partial w_{ij}} = x_{mj} $$   \n",
    "   </font>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"3\" color='#310959' >  \n",
    "     &nbsp;&nbsp;\n",
    "     Plug in the eq.(4) and eq.(5) in :\n",
    "    </font>\n",
    "</h1>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"4\" color='#4a3e20' >    \n",
    " &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;$\\frac{\\partial L}{\\partial w_{ij}}=-\\sum_m\\sum_n \\frac{y_{mn}}{p_{mn}}\\frac{\\partial  p_{mn}}{\\partial z_{mi}}\\frac{\\partial  z_{mi}}{\\partial w_{ij}}$ \n",
    "  </font>\n",
    "</h1>     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"3\" color='#310959' >  \n",
    "     &nbsp;&nbsp;\n",
    "     We achieve :\n",
    "    </font>\n",
    "</h1>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"4\" color='#4a3e20' >    \n",
    " &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;$\\frac{\\partial L}{\\partial w_{ij}}=-\\sum_m\\sum_n \\frac{y_{mn}}{p_{mn}}p_{mn}(\\delta_{ni} -  p_{mi})x_{mj}=-y_{mn}(\\delta_{ni} -  p_{mi})x_{mj} $ \n",
    "  </font>\n",
    "</h1>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"4\" color='#4a3e20' >  \n",
    "      &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;\n",
    "     $ =-\\sum_m\\sum_n y_{mn}\\delta_{ni} + \\sum_m\\sum_n^n y_{mn} p_{mi}x_{mj} $ \n",
    "   </font>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1>\n",
    " <font size=\"4\" color='#4a3e20' >  \n",
    "      &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;\n",
    "    $ =-\\sum_k^m\\sum_n y_{mi}\\delta_{ii} + \\sum_m\\sum_n y_{mn} p_{mi}x_{mj} $ \n",
    "   </font>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1>\n",
    " <font size=\"4\" color='#4a3e20' >  \n",
    "      &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;\n",
    "   $ =-\\sum_k^m y_{mi} + \\sum_m\\sum_n y_{mn} p_{mi}x_{mj} $\n",
    "   </font>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>\n",
    "  <font size=\"3\" color='#310959' >  \n",
    "      &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "      $y_{m1}+ .y_{m2}.. +y_{mk}=1$   because $y_{mk}$ are class variables written as one hot label values (dummies)<br> \n",
    "      &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "      therefore they sum up to 1  $\\sum_n^k y_{mk}=1$  where $k$ is a number of classes\n",
    "  </font> \n",
    "</h6>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"4\" color='#4a3e20' >  \n",
    "      &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;\n",
    "  $ =-\\sum_k^m y_{mi}x_{mj} + \\sum_k^m 1. p_{mi}x_{mj} = \\sum_k^m 1. p_{mi}x_{mj}-\\sum_k^m y_{mi}x_{mj}$\n",
    "   </font>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"4\" color='#4a3e20' >  \n",
    "      &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;\n",
    "  $ = \\sum_k^m( p_{mi}-y_{mi})x_{mj}$\n",
    "   </font>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"3\" color='#310959' >  \n",
    "     &nbsp;&nbsp;\n",
    "      Maybe we've achieved the most important result: <br>\n",
    "    </font>\n",
    "</h1>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"5\" color='#4a3e20' >  \n",
    "     $$6) \\;\\;\\;\\;\\frac{\\partial L}{\\partial w_{ij}}=\\nabla w_{ij}L(W,b)= \\sum_k^m( p_{mi}-y_{mi})x_{mj}$$\n",
    "   </font>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"3\" color='#310959' >  \n",
    "     &nbsp;&nbsp;\n",
    "      We can apply absolutely the same schema for $\\nabla b_{i}L(W,b)$ even It is easier one and we will reach<br>\n",
    "    </font>\n",
    "</h1>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"5\" color='#4a3e20' >  \n",
    "     $$7)\\;\\;\\;\\;\\nabla b_{i}L(W,b)= \\sum_k^m( p_{mi}-y_{mi})$$\n",
    "   </font>\n",
    "</h1>\n",
    "\n",
    "</h6>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"3\" color='#310959' >  \n",
    "     &nbsp;&nbsp;\n",
    "        Althought the eq.(6) seems so simple and elegant  it is written is tenzor form not in matrix one therefor  its implementation becomes more dificult, espesialy when we want to use our lovely library numpy.But we can write the equation\n",
    "     in matrix form :\n",
    "    </font>\n",
    "</h1>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"3\" color='#4a3e20' >  \n",
    "      &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;\n",
    " $\\nabla_W L = \\begin{bmatrix} \\nabla_{w_{11}} L  & \\nabla_{w_{12}}L &\\nabla_{ w_{13}}L \\\\\\nabla_{ w_{21}}L &\\nabla_{ w_{22}}L & \\nabla_{w_{23}}L \\\\\\nabla_{ w_{31}}L &\\nabla_{w_{32} }L &\\nabla_{ w_{33}}L \\end{bmatrix} $\n",
    "$=\\begin{bmatrix} p_{11} -y_{11}  &  p_{12}-y_{12}  & p_{13} -y_{13}\\\\  p_{21}-y_{12} &  p_{22} -y_{22}& p_{23} -y_{23}\\\\ ... & ... & ... \\\\ p_{n1}-y_{n1} & p_{n2}- y_{n2} &p_{n3}- y_{n3} \\end{bmatrix}$\n",
    "$\\begin{bmatrix} x_{11} &  x_{12} & x_{13} \\\\  x_{21} &  x_{22} & x_{23} \\\\ ... & ... & ... \\\\ x_{n1} &  x_{n2} & x_{n3} \\end{bmatrix}$\n",
    "   </font>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"6\" color='#4a3e20' >  \n",
    "      $$8)\\;\\;\\;\\;\\nabla_W L = (P-Y)^T.X$$\n",
    "   </font>\n",
    "</h1>\n",
    "<h1>\n",
    " <font size=\"6\" color='#4a3e20' >  \n",
    "      $$9)\\;\\;\\;\\;\\nabla_B L = (P-Y)$$\n",
    "   </font>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>\n",
    "  <font size=\"3\" color='#310959' >  \n",
    "      &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "      The eq(8) can be proved only with direct verification, you can try to  proof it !!! See why this is correct!!!\n",
    "  </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"3\" color='#310959' >  \n",
    "     &nbsp;&nbsp;\n",
    "       The eq(8) and eq(9)  have been written in matrix form.The  result is  surprisingly simple and so easy for implementation.<br>       If we plug the eq(8) and (9) in eq(2) we ahieve our final result : \n",
    "    </font>\n",
    "</h1>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"3\" color='#310959'>  \n",
    " &nbsp;&nbsp;  Our main training algorithm look like this  : \n",
    "</font>\n",
    "\n",
    "\n",
    "</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<font size=\"5\" color='#4a3e20' > \n",
    "   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "  $(10) \\;  \\; \\;  \\; \\; \\; \\;  W= W -\\lambda(P-y)^T.X  $ <br> <br>\n",
    "   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "  $(11) \\;  \\; \\;  \\; \\; \\; \\;  B = B -\\lambda(P-Y)^T    $\n",
    "</font>      \n",
    "<h1> \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"3\" color='#310959' >  \n",
    "     &nbsp;&nbsp;\n",
    "      The implentation will be :\n",
    "       $ W = W - l*(Y-softmax(W,b,X,Y))^T.dot(X)$ <br>\n",
    "       $ B = B - l*(Y-softmax(W,b,X,Y))$ <br>\n",
    "    </font>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_gradient(X,y,max_iter=999,learning_rate=0.1):\n",
    "    innitial_value =5  \n",
    "    \n",
    "    bias = np.full((y.shape[1],),innitial_value)\n",
    "    W = np.full((X.shape[1], y.shape[1]), innitial_value)\n",
    "    m = X.shape[0] \n",
    "    P = softmax(X,W,bias)\n",
    "    for i in range(1000):\n",
    "        P = softmax(X,W,bias)\n",
    "        B = P-y\n",
    "        w_delta =(1/m)*(learning_rate*B.T.dot(X))\n",
    "        b_delta = (1/m)*learning_rate*np.sum(B, axis=0)\n",
    "        if np.isnan(w_delta).any() or np.isnan(b_delta).any():\n",
    "            print('or fuck')\n",
    "            break;   \n",
    "        #print('bias ' ,bias)      \n",
    "        W = W - w_delta\n",
    "        bias = bias - b_delta\n",
    "        \n",
    "    print('bias ' ,bias)    \n",
    "    print('W ' ,W) \n",
    "    return W,bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias  [4.96244088 9.96270683 0.07485229]\n",
      "W  [[5.23885761 4.39193655 5.63613753]\n",
      " [4.43993901 4.67474259 3.62560059]\n",
      " [5.32120337 5.93332086 5.73826188]]\n"
     ]
    }
   ],
   "source": [
    "W,b = perform_gradient(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "B= np.array(  [ 5.28361925 ,10.08490948 ,-0.36852873])\n",
    "W = np.array( [[5.08196969 ,5.03709761 ,5.15405716],\n",
    " [4.24027399, 4.67333502 ,3.818242  ],\n",
    " [5.67775632, 5.28956737, 6.02770084]])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original data label Y :  \n",
      "[[0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n",
      "\n",
      "predicted data label Y' :\n",
      "[[0.31 0.65 0.04]\n",
      " [0.22 0.   0.78]\n",
      " [0.55 0.22 0.23]\n",
      " [0.51 0.07 0.42]\n",
      " [0.22 0.   0.77]\n",
      " [0.53 0.1  0.37]\n",
      " [0.5  0.07 0.43]\n",
      " [0.19 0.   0.81]\n",
      " [0.23 0.75 0.02]\n",
      " [0.24 0.   0.75]\n",
      " [0.22 0.   0.78]\n",
      " [0.16 0.83 0.01]\n",
      " [0.33 0.01 0.65]\n",
      " [0.52 0.09 0.4 ]\n",
      " [0.16 0.   0.84]\n",
      " [0.21 0.   0.79]\n",
      " [0.23 0.   0.77]\n",
      " [0.25 0.01 0.75]\n",
      " [0.55 0.16 0.29]]\n"
     ]
    }
   ],
   "source": [
    "print('original data label Y :  ')\n",
    "print(y_train[1:20])\n",
    "print('')\n",
    "print(\"predicted data label Y' :\")\n",
    "print(np.around(softmax(X_train,W,b)[1:20],2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
