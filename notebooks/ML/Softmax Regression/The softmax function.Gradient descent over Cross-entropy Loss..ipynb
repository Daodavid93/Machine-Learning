{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "   <font color='#45101c'>\n",
    "       The softmax function.Gradient descent over Cross-entropy Loss.\n",
    "    </font>\n",
    "</h1> \n",
    "<h3>\n",
    "    <font color='#26080f'>\n",
    "      (More Math Less Code)\n",
    "      \n",
    "  </font>\n",
    "</h3>    \n",
    " <h5>\n",
    "    <br> <br> <br>\n",
    "    <font color='blue'>\n",
    "     author : daodeiv (David Stankov) \n",
    "     </font>\n",
    "  </h5>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "   <font size=\"3\" color='#263a61'>\n",
    "      .\n",
    "    </font>\n",
    "</h2> \n",
    "<h5>\n",
    "\n",
    "   <font size=\"3\" color='#263a61' >\n",
    "    $\\;\\;\\;\\;$  What is the Softmax function and how it works? <br>\n",
    "   $\\;\\;\\;\\;$ Which function is used  as  Loss Error  in Softmax regression? <br> <br>\n",
    "    This notebook has a mathematical proof and derivation of the formula for the Gradient Descent algorithm which is applied to cross-  entropy loss or so-called Softmax Loss in order to fit the estimator parameters.The notebook  contains a simply   implentation of Gradient descent. This paper is focused on math issues related to Multiclass classification used in Logistic regression and  Neural networks as well.\n",
    "    </font>     \n",
    "</h5>    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "\n",
    "   <br>\n",
    "   <font size=\"2\" color='#263a61'>\n",
    "       $\\; \\; $   The standart softmax function $\\sigma: \\; \\Re^k \\; \\rightarrow \\; \\Re^k $ is defined by formula : <br>\n",
    "    \n",
    "\n",
    "</font>   \n",
    "  <font size=\"5\" color='#454214' > $$\\sigma(z)_i= \\frac{e^z_i}{\\sum_{j=1}^n e^{z}_{j}}$$  \n",
    "</font> \n",
    "<br>\n",
    "\n",
    "<font size=\"3\" color='#263a61' > \n",
    "    The softmax function  takes as an input a vector $\\vec z$ with $K$ number of component, and normalized it into  a  probability distribution consisting of  $K$ number of probabilities proportional to exponentials of input values.That is, prior to applying softmax function some vector components could be negative or greater than one and  might not sum up to 1.\n",
    "Furthermore more the larger input components correspond to larger probabilities. \n",
    "</font>    \n",
    "<br> <br>     \n",
    " </h1>     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "   <font color='#45101c'>\n",
    "      .\n",
    "    </font>\n",
    "</h2> \n",
    "\n",
    "\n",
    " <h6> <font size=\"2\" color='#263a61' >  $\\; \\; $ Before we getting deeper into the above equation, we gonna generate our learning data. The data consist of records contains different types of vehicle together with their physical sizes. Since our purpose is to diving into the concepts of Softmax (not to show great predictions) the dataset is very simple and we would be able to do prediction using only the power of our brain without any ML algorithms. The simplicity of the dataset will help us to easier understand the math concepts behind softmax easier. \n",
    "    </font> \n",
    "  </h6>\n",
    "<br>\n",
    "<h5>\n",
    "<font size=\"2\" color='#263a61' > \n",
    "        Let to generate the training data : <br>\n",
    "</font>    \n",
    "</h5>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_size</th>\n",
       "      <th>y_size</th>\n",
       "      <th>z_size</th>\n",
       "      <th>label_Bus</th>\n",
       "      <th>label_Car</th>\n",
       "      <th>label_Tractor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.200743</td>\n",
       "      <td>4.131306</td>\n",
       "      <td>4.255998</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.149941</td>\n",
       "      <td>4.838548</td>\n",
       "      <td>4.218311</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.787533</td>\n",
       "      <td>4.986217</td>\n",
       "      <td>4.901542</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.506791</td>\n",
       "      <td>4.950477</td>\n",
       "      <td>4.686618</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.832195</td>\n",
       "      <td>4.577499</td>\n",
       "      <td>4.156847</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.421212</td>\n",
       "      <td>1.615876</td>\n",
       "      <td>1.537319</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.544604</td>\n",
       "      <td>1.663653</td>\n",
       "      <td>1.425982</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.896910</td>\n",
       "      <td>1.798983</td>\n",
       "      <td>1.442927</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.203042</td>\n",
       "      <td>1.960941</td>\n",
       "      <td>1.309985</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.145273</td>\n",
       "      <td>1.200511</td>\n",
       "      <td>1.637442</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      x_size    y_size    z_size  label_Bus  label_Car  label_Tractor\n",
       "0   4.200743  4.131306  4.255998          0          0              1\n",
       "1   4.149941  4.838548  4.218311          0          0              1\n",
       "2   4.787533  4.986217  4.901542          0          0              1\n",
       "3   4.506791  4.950477  4.686618          0          0              1\n",
       "4   4.832195  4.577499  4.156847          0          0              1\n",
       "..       ...       ...       ...        ...        ...            ...\n",
       "25  1.421212  1.615876  1.537319          0          1              0\n",
       "26  1.544604  1.663653  1.425982          0          1              0\n",
       "27  1.896910  1.798983  1.442927          0          1              0\n",
       "28  1.203042  1.960941  1.309985          0          1              0\n",
       "29  1.145273  1.200511  1.637442          0          1              0\n",
       "\n",
       "[90 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generate  records refer to tractor label with random physical size between [4,5] \n",
    "tractor_dataframe= pd.DataFrame(data=np.random.random((30, 3))+4,columns = ['x_size','y_size','z_size'])\n",
    "tractor_dataframe['label'] =pd.DataFrame( np.full((1,30 ), 'Tractor').T)\n",
    "\n",
    "#generate  records refer to car label with random physical size between [1,2]  \n",
    "car_dataframe= pd.DataFrame(data=np.random.random((30, 3)) + 1,columns = ['x_size','y_size','z_size'])\n",
    "car_dataframe['label'] =pd.DataFrame( np.full((1,30 ), 'Car').T)\n",
    "\n",
    "#generate  records refer to car label with random physical size between [2,3]  \n",
    "bus_dataframe= pd.DataFrame(data=np.random.random((30, 3))+2,columns = ['x_size','y_size','z_size'])\n",
    "bus_dataframe['label'] =pd.DataFrame( np.full((1,30 ), 'Bus').T)\n",
    "\n",
    "# joint each data frame into  one\n",
    "data = tractor_dataframe.append(bus_dataframe).append(car_dataframe)\n",
    "data = pd.get_dummies(data) \n",
    "data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> <br>\n",
    "<h7>\n",
    "<font size=\"2\" color='#263a61' > \n",
    "The label values label_car,label_bus, and label_tractor are represented as one-hot encoding variable (dummies) in order to be convenient for mathematical manipulation or just to be used in mathematical equations.\n",
    "    </font>\n",
    "</h7>    \n",
    "<br><br>    \n",
    "\n",
    "<h5>\n",
    "<font size=\"3\" color='#263a61' > \n",
    "       Let to see the scatter plot of data.  \n",
    "</font>    \n",
    "</h5>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x19637f118c8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(tractor_dataframe['x_size'],tractor_dataframe['y_size'],label='Tractor',color='blue')\n",
    "plt.scatter(car_dataframe['x_size'],tractor_dataframe['y_size'],label='Car',color='red')\n",
    "plt.scatter(bus_dataframe['x_size'],tractor_dataframe['y_size'],label='Bus',color='yellow')\n",
    "plt.xlabel('x size')\n",
    "plt.ylabel('y size')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x train (n,m) (60, 3)\n",
      "shape of y label (n,k) (60, 3)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.array(data.drop(['label_Bus','label_Car','label_Tractor'], axis = 1)) # gets the target label variables\n",
    "y_train = np.array(data[['label_Bus','label_Car','label_Tractor']]) # gets feature variables \n",
    "X_train, X_test, y_train, y_test = train_test_split(x_train,y_train, test_size=0.33, random_state=42) #separats into test and train samples \n",
    "\n",
    "print('shape of x train (n,m)', X_train.shape)\n",
    "print('shape of y label (n,k)', y_train.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<br> <br>\n",
    "<font size=\"2\" color='#263a61' > \n",
    "$\\; \\; \\; $In softmax regression we replace sigmoid function by so-called softmax function \n",
    "          $\\phi_{softmax (.)}$. Where we difine the $z$ input as : \n",
    " </font> \n",
    " <br> <br>\n",
    " <font size=\"3\" color='#454214'>\n",
    "     $$ z = w_1 x_1 + ...+w_nx_n +b = \\sum_l^mw_lx_l +b = W^T +b$$\n",
    " </font>  \n",
    "</h1> \n",
    "\n",
    "<h5>\n",
    " <font size=\"2\" color='#263a61' > \n",
    "$\\; \\; \\;\\; \\; \\; \\; \\; \\;\\; \\; \\;\\; \\; \\; \\; \\; \\;\\; \\; \\;\\; \\; \\; \\; \\; \\;\\; \\;\\; \\; \\; \\; \\; \\;\\; \\; \\;\\; \\; \\; \\; \\; \\;$  where $W$ is the weight vector, $X$ is is the feature vector of 1 training example, and $b$ is bias unit (intercept)  \n",
    " </font>\n",
    " <br><br>\n",
    "</h5>\n",
    "<br> <br>\n",
    "<h1>\n",
    "<font size=\"2\" color='#263a61' > \n",
    " $\\;\\;\\;$Now,softmax function computes the probability that this training example $x^{(i)}$ belongs to class j given the weight $w$ and  input $z^{(i)}$. <br>\n",
    " So we compute the probability $p(y=j | x^{(i)};W_j)$ for each class label in j=1,...,k..Note the normalization term in the denominator which causes these class probabilities to sum up one. Our training sample consist of $(n_{samples} ,m_{features})$ = (60,3) and label consist of  are ( k_{classes} ) =  (3)\n",
    "therefore  the weight matrix is $m\\times k = 9$ or  $(m_{features},k_{classes}(probabilities) = (3 ,3)$ dimensional. <br> \n",
    "To compute the net input $z$, we mutiply feature matrix with weight matrix  $(n_{samples} ,m_{features})\\times (m_{features} ,k_{classes})\\; (60,3)\\times(3,3) $ wich yields a $60 \\times 3 $ to wich we then add the bias unit.Mathematicaly look like this:\n",
    " </font> \n",
    " </h1>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> <br>\n",
    "<h7>\n",
    "<font size=\"2\" color='#263a61' > \n",
    "$\\; \\; \\; $ weight matrix  $W$ and bias (intercept) $B$ $X$ training data: \n",
    "</font>\n",
    "    \n",
    "<br> <br>\n",
    "<font size=\"2\" color='#454214' >\n",
    "   $\\;\\;\\;\\;\\; W= \\begin{bmatrix} weight_1\\rightarrow class \\; 1(bus) \\\\ weight_2\\rightarrow class\\;  2(car) \\;   \\\\ weight_3\\rightarrow class \\;3(tractor) \\;  \\end{bmatrix} =\n",
    "    \\begin{bmatrix} \\vec w_1 \\\\  \\vec w_2\\  \\\\ \\vec w_3  \\end{bmatrix} =\\begin{bmatrix} w_{11} & w_{12} & w_{13} \\\\ w_{21} & w_{22} & w_{23} \\\\ w_{31} & w_{32} & w_{33} \\end{bmatrix}  $ <br> <br> <br>\n",
    "     $\\;\\;\\;\\;\\; B= \\begin{bmatrix}  b_1 \\\\  b_2\\  \\\\  b_3  \\end{bmatrix}\\;\\;\\;  X = \\begin{bmatrix} x_{11} &  x_{12} & x_{13} \\\\  x_{21} &  x_{22} & x_{23} \\\\ ... & ... & ... \\\\ x_{n1} &  x_{n2} & x_{n3} \\end{bmatrix}$\n",
    " <font>   \n",
    "    \n",
    " </h7>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font size=\"2\" color='#263a61' > \n",
    "$\\; \\; \\; $ and net input Z: \n",
    "</font> \n",
    "\n",
    "<font size=\"2\" color='#454214' >\n",
    "        $$ z_{ij} = \\sum_l^3 x_{ip} w_{jp} + b_j $$\n",
    "</font>\n",
    "<br>\n",
    " <font size=\"2\" color='#263a61' > \n",
    "$\\; \\; \\; $  written in matrix form : \n",
    "    <br>\n",
    "</font> \n",
    "<font size=\"3\" color='#454214' >\n",
    "     $$ Z = XW^{T} +b $$ <br> <br>\n",
    "</font>     \n",
    "<br>\n",
    "<font size=\"2\" color='#454214' >\n",
    "     $$ Z = \n",
    "    \\begin{bmatrix} x_{11} &  x_{12} & x_{13} \\\\  x_{21} &  x_{22} & x_{23} \\\\ ... & ... & ... \\\\ x_{n1} &  x_{n2} & x_{n3} \\end{bmatrix} \\times \\begin{bmatrix} w_{11} & w_{21} & w_{31}  \\\\ w_{12} & w_{22} & w_{32} \\\\  w_{13} & w_{23} & w_{33}\\end{bmatrix} + \\begin{bmatrix}  b_1 \\\\  b_2\\  \\\\  b_3  \\end{bmatrix}  =   \\begin{bmatrix} z_{11} &  z_{12} & z_{13} \\\\  z_{21} &  z_{32} & z_{33} \\\\ ... & ... & ...  \\\\ z_{n1} &  z_{n2} & z_{n3}  \\end{bmatrix} $$ <br>\n",
    "</font>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h7>\n",
    "<font size=\"2\" color='#263a61' > \n",
    "    $\\; \\; $In softmax regression settings, we are interested in multi-class classification and so the label y can take many values rather then only two. Thus in our training  we have $k_i\\ in {1,2,3}$.\n",
    "Given a test input x, we want our hypotesis to estimate the probability  $P(y=k|x_i)$ (y=k_i), i.e prediction of one label ,accoring to one samle(row) x_i) for each value of $k = {1,2,3}$ i.e we want to estimate the probability of class label taking on each of the $K$ different posible values. Thus, our hypotesis will output a K-dimensianal (3) vectors (whose element sum up to 1) giving us our K estimated probabilities. Concretely, our score(hypotesis) function $f(x,W)$ takes the form : <br>\n",
    "  </font> \n",
    "  <br>\n",
    "</h7>\n",
    " <font size=\"2\" color='#454214'>  \n",
    "   $$ f(x|W) = \\begin{bmatrix} P(y=1 |x;W) \\\\  P(y=2 |x;W)  \\\\ ... \\\\ P(y=n |x;W)  \\end{bmatrix} = \n",
    " \\frac{1}{\\sum_{j=1}^k exp(W_j^Tx)} = \\begin{bmatrix} exp(W_1^Tx) \\\\  exp(W_2^Tx)  \\\\ ... \\\\ exp(W_n^Tx)  \\end{bmatrix}$$\n",
    " </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h5>\n",
    " <font size=\"2\" color='#263a61' > \n",
    "$\\; \\; \\; \\; \\; \\;\\; \\; \\;\\; \\; \\;\\; \\; \\;\\; \\; \\;$ according to our generated dataset: \n",
    "</font> \n",
    "</h5>\n",
    "\n",
    "<font size=\"2\" color='#454214'>  \n",
    "$$ f(x|W) = P = \\begin{bmatrix} P(y=1 |x_{1j} ; w_{1j}) &  P(y=2 |x_{2j} ; w_{2j})  & P(y=3 |x_{1j} ; w_{3j}) \n",
    "        \\\\ ... &  ...  & ...\n",
    "        \\\\ P(y=1 |x_{nj} ; w_{1j}) &  P(y=2 |x_{2j} ; w_{2j})  & P(y=3 |x_{1j} ; w_{3j})\n",
    "        \\end{bmatrix} = \n",
    "        \\begin{bmatrix} \\frac{e^{z_{11}}}{\\sum_{1j}e^{z_{1j}}} & \\frac{e^{z_{12}}}{\\sum_{1j}e^{z_{1j}}} & \\frac{e^{z_{13}}}{\\sum_{1j}e^{z_{1j}}} \n",
    "        \\\\   \\\\ ... & ... & ...  \\\\\n",
    "        \\\\    \\frac{e^{k_{n1}}}{\\sum_{j}^3e^{z_{nj}}} & \\frac{e^{z_{n2}}}{\\sum_{j}^3e^{z_{nj}}} & \\frac{e^{z_{n3}}}{\\sum_{j}^3e^{z_{nj}}}\\end{bmatrix} $$ \n",
    "</font>        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h7>\n",
    "<font size=\"2\" color='#263a61' > \n",
    "        $\\;\\;\\;$The elements of $P$ we will called predictors $p_{ij}$ wich can be interpretated as probability i_th row element\n",
    "        to be j_th categorical value.If $p_{1,2}$ means what is the probability that record one belongs to the label_ bus?\n",
    "        Ofcourse, $p_{ij} \\in [0,1]$ <br>\n",
    "        Another way to express $p_{ij}$ : <br>  <br> \n",
    "        For example, the probability record one $X_1$ from dataset to be from class label $Y_2$ can be calculated in this way :\n",
    "         </font> \n",
    "      <br> \n",
    "</h7>      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\" color='#454214'>  \n",
    "      $$softmax(Z)_{12}=P(y=2 |x_{1j};\\; w_{2j})=  \\;p_{12}\\;  = \\frac{ e^{ ^{z_{12}} } }{ \\sum_p^3 e^{z_{1p}}}=\\frac{ e^{ (^{\\sum_v^3 x_{1v}.w_{vj} + b_v }} )}{ \\sum_p^3 e^{ ^ ({\\sum_v^3 x_{1v}.w_{2v}} + b_2})}$$\n",
    "  </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>\n",
    "<font size=\"2\" color='#263a61' > \n",
    "     Let to see how an above considering can be applied concretely in our training dataset :\n",
    "             First, let to define a weight vector $W$ <br> <br>\n",
    "   \n",
    "</font>   \n",
    " </h5>    \n",
    " </h7>\n",
    "    <font color='#454214'>\n",
    " $$W = \\begin{bmatrix} w_{11} & w_{12} & w_{13} \\\\ w_{21} & w_{22} & w_{23} \\\\ w_{31} & w_{32} & w_{33} \\end{bmatrix}  =\n",
    "            \\begin{bmatrix} 0.75776811 & 0.6690284 & -0.02646755 \\\\\n",
    " -2.94775864 &  -3.06439323 & -2.14043833\\\\\n",
    "  2.18999053 & 2.39536483 &  2.16690588 \\end{bmatrix}$$\n",
    "  <br> <br>\n",
    "    $$B = \\begin{bmatrix}  b_1 \\\\  b_2\\  \\\\  b_3  \\end{bmatrix} = \\begin{bmatrix}  -0.15425504 \\\\ 18.81781451 \\\\ -18.66355947 \\end{bmatrix} $$ <br> <br>\n",
    "  </font> \n",
    " </h7>\n",
    " \n",
    " <h5>\n",
    "  <font size=\"2\" color='#263a61' > \n",
    "     I've prepared weight vector $W$ and bias $B$ in advance, how? We will see later. <br> <br>\n",
    "</font> \n",
    "   </h5> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.array([[ 0.75776811,  0.6690284 , -0.02646755],\n",
    "              [-2.94775864, -3.06439323, -2.14043833],\n",
    "              [ 2.18999053 , 2.39536483,  2.16690588]]) # define a weight matrix\n",
    "\n",
    "B = intercept = np.array([ -0.15425504 , 18.81781451, -18.66355947])  #bias vector or intercept "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sofmax(X_data, W_weight, Bias):\n",
    "    '''\n",
    "    softmax function  \n",
    "    takes :\n",
    "    X = training data\n",
    "    W =  weight matrix\n",
    "    b = bias vector (intercept) \n",
    "    return :\n",
    "      softmax for every z unit e^{k_ij}/(e^{k_i1}+e^{k_i2}+e^{k_i3})\n",
    "    '''\n",
    "    #dot product between X_data matrix  and tranposed Weight_ matrix added to Bias  gives matrix each z_ij\n",
    "    Z = X_data.dot(W.T)+B \n",
    "    \n",
    "    #return matrix cosist of exponentials Z input net matrix  s_i \n",
    "    exp_z = np.exp(X_data.dot(W.T)+B)\n",
    "    \n",
    "    #array with sum_i=Sum_k (e^z_{ik})\n",
    "    sums=np.sum(exp_z, axis=1) \n",
    "    \n",
    "    #return softmax(Z)_{ij}\n",
    "    return (exp_z.T/sums).T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original data label Y :  \n",
      "[[0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]]\n",
      "\n",
      "predicted data label Y' :\n",
      "[[0.02 0.98 0.  ]\n",
      " [0.01 0.   0.99]\n",
      " [0.98 0.   0.02]\n",
      " [0.97 0.03 0.  ]\n",
      " [0.01 0.   0.99]\n",
      " [0.98 0.01 0.01]\n",
      " [0.96 0.04 0.  ]\n",
      " [0.   0.   1.  ]\n",
      " [0.03 0.97 0.  ]]\n"
     ]
    }
   ],
   "source": [
    "print('original data label Y :  ')\n",
    "print(y_train[1:10])\n",
    "print('')\n",
    "print(\"predicted data label Y' :\")\n",
    "print(np.around(sofmax(X_train,W,intercept)[1:10],2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h7>\n",
    "<font size=\"2\" color='#263a61' > \n",
    "     From $Y'$ output let to consider the first row : <br> <br>\n",
    "            $Y'_{11} = 0.01 \\rightarrow  $ has  0.1% chance that row one belongs to class 1(BUS) <br> \n",
    "            $Y'_{12} = 0.98 \\rightarrow  $ has 98% chance that row one belongs to class 2(CAR) <br> \n",
    "            $Y'_{13} = 0.01 \\rightarrow  $ has 0.1% chance that row one belongs to class 3(TRACTOR) <br> <br>\n",
    "            That means record $X_1$ belongs to class Car, because has the 90% chance, which is the largest. <br> <br> <br>\n",
    "      If we compare     $Y$ and $Y'$ we will notice that our prediction has 100% accuracy. <br>\n",
    "But, how have I  found a weight matrix $W$ and bias &B&.I just used the LogisticRegression from scikit-learn, and took the coefficients, but now we gonna define a Loss function ant its optimization in order to find the weght $W$ and bias $\\vec b$. \n",
    "         </font>     \n",
    "</h7>         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h7>\n",
    " <font size=\"2\" color='#263a61' > \n",
    "     <br> <br>\n",
    "     $\\;\\;\\;$ Now we are going to define a loss function for :\n",
    "     </font>\n",
    "     <font color='#454214'>\n",
    "            $$(1) \\; \\; \\; \\; \\; \\;p_{ij}=softmax(Z)_{ij}=\\frac{e^{z_{ij}}}{\\sum_l^k e^{z_{il}}} $$ <br>\n",
    "            $$\\; \\; \\;z_{ij} = \\sum_p^3 x_{ip} w_{jp} + b_j $$\n",
    "   </font>\n",
    "   <br>\n",
    "   <font size=\"3\" color='#263a61' > \n",
    "   $\\;\\;\\;$ The loss function for Softmax regression is called Cross-Entropy Loss or Sofmax Loss and it is defined like that:\n",
    "   <br>\n",
    "   <br>\n",
    "   <font size =\"4\" color='#45101c'>\n",
    "    $$ (2) \\;  \\; \\;L(W,b)=-\\sum_i^n\\sum_j^k y_{ij} \\log (sofmax(Z)_{ij})$$ <br> <br>\n",
    "     </font>\n",
    "   </h7>\n",
    "   <h5>\n",
    "   <font size=\"2\" color='#263a61' > \n",
    "       $\\;\\;\\;\\;\\;\\;$Where n is a count of records , $y_{ij}$ is label value of i_th record j_th class, $softmax(Z)_{ij}$ predicited label value  $Y'_{ij}$ and $W$ is weight matrix and $b$ bias. \n",
    "    </font>\n",
    "    </h5>\n",
    "    <h7>\n",
    "       <br> <br>\n",
    "    <font size=\"2\" color='#263a61' > \n",
    "     Our purpose in to fit parameters  $w_{ij}\\in W$ and $b_j$ according to \n",
    " a training data $X$  and a label data $Y$ in order to make the best prediction.To do that we will apply Gradient descent as a regression technique over Softmax loss function.The eq.(2) is the function of all weights $w_{ij}$, bias $b_j$ all training data x and label data y.     \n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h7>\n",
    " <br> <br>\n",
    "        <font size=\"2\" color='#263a61' >\n",
    "            First, We gonna introduce some math technics which will make our work easier, and we will find derivates of some function which will be extremely useful in the process of finding of $\\nabla L(W,b)$. <br> <br>\n",
    "            For simplicity in the summation process of indices, we will introduce a <a href='http://physics.csusb.edu/~prenteln/notes/vc_notes.pdf'>Kronecker symbol</a> .<br>\n",
    "    </font>    \n",
    "</h7> \n",
    "\n",
    " <h3>\n",
    "     <font size=\"2\" color='#454214'>\n",
    "       $$\\delta_{ij} =    \n",
    "         \\begin{equation}\n",
    "   \\begin{Bmatrix} \n",
    "   1 & if \\; i=j  \\\\\n",
    "   0 & if \\; i\\ne j  \n",
    "    \\end{Bmatrix} \n",
    "\\end{equation}$$\n",
    "  <br><br>\n",
    "          $$ \\delta_{ij} = \\begin{bmatrix} 1 & 0 & 0  \\\\ 0 & 1 & 0 \\\\  0 & 0 & 1 \\end{bmatrix}$$\n",
    "  </font>\n",
    " </h3>    \n",
    " <h7>\n",
    " <br> <br>\n",
    "        <font size=\"2\" color='#263a61' >\n",
    "           In many places in the coming sum operations over indexes we will miss the $\\sum$ symbol, just it will be avoided(hidden) according to the .<a href='https://en.wikipedia.org/wiki/Einstein_notation'>Einstein summation convention</a> .<br>\n",
    "       For example, the equation. \n",
    "    </font>\n",
    "    <font size=\"2\" color='#454214'>\n",
    "       $$z_{ij} = \\sum_p^3 x_{ip} w_{jp} + b_j $$ \n",
    " </font>   \n",
    "  <font size=\"2\" color='#263a61' >\n",
    "by applying the  Enstein convetion we could rewrite it as : <br>\n",
    "  </font>\n",
    "  <br>\n",
    "  <font size=\"2\" color='#454214'>\n",
    "         $$z_{ij} = x_{ip} w_{jp} + b_j$$  \n",
    "   </font>     \n",
    "   <br>\n",
    "   <font size=\"2\" color='#263a61' >\n",
    "   The sign $\\sum_p^3$ is miss.The sumation over p  is implied(by default) because p is repeated twice.Every time when there are repeatable indices that is the indicator for exist of $\\sum$  which is just missing(The sum sign  is not written).\n",
    "    </font>    \n",
    "</h7> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h7>\n",
    " <font size=\"2\" color='#263a61' >\n",
    "  Our goal is to apply Gradient descent to eq.(2), it is defined as :\n",
    "     \n",
    " </font>\n",
    "  <br> <br>\n",
    "   <font size =\"4\" color='#45101c'>\n",
    "    $$(2) \\;  \\; \\;  \\; \\; \\; \\;\n",
    "   \\begin{matrix} w_{ij} = w_{ij} - \\nabla w_{ij}L(W,b) \\\\  \\\\ b_{j} = b_j - \\nabla b_{j}L(W,b)\n",
    "       \\end{matrix} $$\n",
    "   \n",
    "  </font> \n",
    "</h7> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h7>\n",
    " <font size=\"2\" color='#263a61' >\n",
    "  Our goal is to apply Gradient descent to eq.(2), it is defined as :\n",
    "     \n",
    " </font>\n",
    "  <br> <br>\n",
    "   <font size =\"4\" color='#45101c'>\n",
    "    $$(3) \\;  \\; \\;  \\; \\; \\; \\;\n",
    "   \\begin{matrix} \\nabla w_{ij}L(W,b)   &=  -\\frac{\\partial}{\\partial w_{ij}}\\Big(\\sum_k^m\\sum_n^n y_{mn} \\log {p_{mn}}\\Big) \\\\  \\\\\n",
    "     \\nabla b_{j}L(W,b) & =- \\frac{\\partial}{\\partial b_{j}}\\Big(\\sum_k^m\\sum_n^n y_{mn} \\log{p_{mn}}\\Big)\n",
    "       \\end{matrix} $$\n",
    "   \n",
    "  </font> \n",
    "</h7> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font size=\"2\" color='#263a61' >\n",
    "        $$ \\;\\;\\; where\\; p_{mn} = softmax(Z)_{mn}$$\n",
    "   </font>\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>\n",
    " <font size=\"3\" color='#263a61' >\n",
    "  Be ready for a hard part !!!\n",
    "     \n",
    " </font>\n",
    "</h5> \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"2\" color='#263a61' >\n",
    "   Now we gonna to resolve the eqs.(3) : <br>\n",
    "     \n",
    "     \n",
    " </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "     <font size=\"4\" color='#454214'>\n",
    "         $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\frac{\\partial L}{\\partial w_{ij}}=-\\frac{\\partial}{\\partial w_{ij}}\\Big(\\sum_k^m\\sum_n^n y_{mn} \\log {p_{mn}}\\Big)$\n",
    "   <br><br>\n",
    "   \n",
    "   $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=\\sum_k^m\\sum_n^n y_{mn}\\frac{\\partial \\log {p_{mn}}}{\\partial w_{ij}} $ \n",
    "   \n",
    "   <br><br>\n",
    "   $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=\\sum_k^m\\sum_n^n\\frac{y_{mn}}{p_{mn}}\\frac{\\partial  p_{mn}}{\\partial w_{ij}} $\n",
    "   <br><br>\n",
    "   $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=\\sum_k^m\\sum_n^n \\frac{y_{mn}}{p_{mn}}\\frac{\\partial  p_{mn}}{\\partial z_{vp}}\\frac{\\partial  z_{vp}}{\\partial w_{ij}} $\n",
    "   </font>\n",
    "   <br> <br>\n",
    "   <font size=\"3\" color='#263a61'> $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;since \\; z_{vp} = f(w_{pi}) \\Rightarrow \\frac{\\partial  z_{vp}}{\\partial w_{ij}} = 0\\; if \\; p\\ne i \\;\\; we \\; can \\; write \\; \\frac{\\partial  z_{vp}}{\\partial w_{ij}}=\\delta_{pi}\\frac{\\partial  z_{vp}}{\\partial w_{ij}}$\n",
    "    $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;and \\; p_{mn} = f(z_{mv}) \\Rightarrow \\frac{\\partial  z_{vp}}{\\partial w_{ij}} = 0\\; if \\; m\\ne v \\;\\; we \\; can \\; write \\; \\frac{\\partial  p_{mn}}{\\partial z_{vp}}=\\delta_{mv}\\frac{\\partial  p_{mn}}{\\partial z_{vp}}  $ <br>\n",
    "     $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;replacing\\; in \\;equation\\; we\\; achieve  $\n",
    " </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "     <font size=\"4\" color='#454214'>\n",
    "         $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=\\sum_k^m\\sum_n^n \\frac{y_{mn}}{p_{mn}}\\delta_{mv}\\frac{\\partial  p_{mn}}{\\partial  z_{vp}} \\delta_{pi}\\frac{\\partial  z_{vp}}{\\partial w_{ij}} $ <br> <br>\n",
    "         </font>\n",
    "         <font size=\"3\" color='#263a61'>\n",
    "    $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; using\\; \\delta\\; proprties\\; we\\; can\\; write$ </font>\n",
    "    <font size=\"4\" color='#454214'> <br> <br>\n",
    "     $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=\\sum_k^m\\sum_n^n \\frac{y_{mn}}{p_{mn}}\\delta_{mm}\\delta_{ii}\\frac{\\partial  p_{mn}}{\\partial z_{mi}}\\frac{\\partial  z_{mi}}{\\partial w_{ij}} $ </font> <br> <br>\n",
    "    <font size=\"3\" color='#263a61'>\n",
    "    $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; since\\; there \\;is\\; no \\sum \\;respect \\;to \\; i \\;and \\;m \\; we\\; can\\; remove\\;\\delta_{ii}\\;and\\; \\delta_{mm}$ </font>\n",
    "    <font size=\"4\" color='#454214'> <br> <br>\n",
    "     $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=\\sum_k^m\\sum_n^n \\frac{y_{mn}}{p_{mn}}\\frac{\\partial  p_{mn}}{\\partial z_{mi}}\\frac{\\partial  z_{mi}}{\\partial w_{ij}} $ </font> <br> <br>\n",
    "  </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    " <font size=\"2\" color='#263a61' >\n",
    "   Let to focus on $\\frac{\\partial  p_{mn}}{\\partial z_{mi}}$ and $\\frac{\\partial  z_{mi}}{\\partial w_{ij}}$ <br>\n",
    "  </font>\n",
    "</h1> \n",
    "<br>\n",
    "<h4>\n",
    "     <font size=\"3\" color='#454214'>\n",
    "          $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\frac{\\partial  p_{mn}} {\\partial z_{mi}}=\\frac{\\partial\\frac { e^{z_{mn}} }{ \\sum_ke^{z_{mk}}} }{\\partial z_{mi}}$ <br> <br>\n",
    "          $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=\\frac{1}{(\\sum_ke^{z_{mk}})^2}\\times \\Big(\\frac{\\partial e^{z_{mn}} }{\\partial z_{mi}}\\times(\\sum_ke^{z_{mk}}) - e^{z_{mn}}\\times\\frac{\\partial (\\sum_ke^{z_{mk}})}{\\partial z_{mi}}  \\Big)$ <br> <br>\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "     <font size=\"4\" color='#454214'>\n",
    "          $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=\\frac{e^{z_{mn}}\\times\\frac{\\partial z_{mn}}{\\partial z_{mi}}}{\\sum_ke^{z_{mk}}} - \\frac{e^{z_{mn}}}{\\sum_ke^{z_{mk}}}\\times\\frac{ \\sum_k e^{z_{mk}} \\frac{ \\partial z_{mk}}{\\partial z_{mi}}}  {\\sum_ke^{z_{mk}}}$\n",
    "   </font>\n",
    "</h4>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\" color='#263a61'>\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; since\\; \\frac{\\partial z_{mk}}{\\partial z_{mi}}=0\\;if\\;k\\ne i\\;\\frac{\\partial z_{mk}}{\\partial z_{mi}}=1\\;ifk = i\\;\\Rightarrow \\frac{\\partial z_{mk}}{\\partial z_{mi}}=\\delta_{ki}$ </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "     <font size=\"4\" color='#454214'>\n",
    "          $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=\\frac{e^{z_{mn}}\\times \\delta_{ni} }{\\sum_ke^{z_{mk}}} - \\frac{e^{z_{mn}}}{\\sum_ke^{z_{mk}}}\\times\\frac{ \\sum_k e^{z_{mk}}\\delta_{ki}}  {\\sum_ke^{z_{mk}}}$ <br><br>\n",
    "          $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=\\frac{e^{z_{mn}}\\times \\delta_{ni} }{\\sum_k e^{z_{mk}}} - \\frac{e^{z_{mn}}}{\\sum_ke^{z_{mk}}}\\times\\frac{ \\sum_k e^{z_{mk}}\\delta_{ki}}  {\\sum_ke^{z_{mk}}}$ <br><br>\n",
    "          $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=\\frac{e^{z_{mn}}\\times \\delta_{ni} }{\\sum_ke^{z_{mk}}} - \\frac{e^{z_{mn}}}{\\sum_ke^{z_{mk}}}\\times\\frac{ \\sum_k e^{z_{mi}}\\delta_{ii}}  {\\sum_ke^{z_{mk}}}$ <br><br>\n",
    "          $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=\\frac{e^{z_{mn}}\\times \\delta_{ni} }{\\sum_ke^{z_{mk}}} - \\frac{e^{z_{mn}}}{\\sum_ke^{z_{mk}}}\\times \\frac{ e^{z_{mi}}}{\\sum_ke^{z_{mk}}}$ <br><br>\n",
    "   </font>\n",
    "</h4>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\" color='#263a61'>\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; from \\;eq.(1)\\;\\Rightarrow\\; \\frac{ e^{z_{mn}} }{ \\sum_k e^{z_{mk} } }=p_{mn} \\;and \\;frac{ e^{z_{mi}} }{ \\sum_k e^{z_{mk} } }=p_{mi}  $</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "     <font size=\"4\" color='#454214'>\n",
    "          $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=p_{mn}\\times \\delta_{ni} - p_{mn}\\times p_{mn}p_{mi}$ <br><br>\n",
    "          $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=p_{mn}(\\delta_{ni} -  p_{mi})$ <br><br> <br>\n",
    "          <font size =\"5\" color='#45101c'>\n",
    "               $$(3) \\;\\;\\;\\;\\;\\frac{\\partial  p_{mn}} {\\partial z_{mi}} = p_{mn}(\\delta_{ni} -  p_{mi}) $$\n",
    "  </font>     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "     <font size=\"5\" color='#454214'>\n",
    " $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\frac{\\partial  z_{mi}}{\\partial w_{ij}} =\\frac{\\partial ( \\sum_k  x_{mk}w_{ki})}{\\partial w_{ij}}= \\frac{ \\sum_k z_{mi} x_{mk}\\partial w_{ki}}{\\partial w_{ij}}$ <br>\n",
    " $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=  \\sum_k  x_{mk}\\frac{\\partial w_{ki}}{\\partial w_{ij}} $ <br>\n",
    " </font>\n",
    "<font size=\"3\" color='#263a61'>\n",
    " $!!!\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;  \\frac{\\partial w_{ki}}{\\partial w_{ij}} = \\delta_{kj}\\;only\\;direct\\;verification\\;can\\; proof \\;it$</font> <br>\n",
    "<h4>\n",
    "     <font size=\"4\" color='#454214'> \n",
    "          $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=  \\sum_k  x_{mk}\\delta_{kj} =\\sum_k  x_{mj}\\delta_{jj}=x_{mj}  $ <br>\n",
    "  </h4>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    " <font size =\"5\" color='#45101c'>\n",
    "               $$(4) \\;\\;\\;\\;\\;\\frac{\\partial  z_{mi}}{\\partial w_{ij}} = x_{mj} $$   \n",
    "</h4>        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h7>\n",
    " <font size=\"2\" color='#263a61' >\n",
    "  Replacing eq.(3) and eq.(4) in $\\frac{\\partial L}{\\partial w_{ij}}=\\sum_k^m\\sum_n^n y_{mn}p_{mn}\\frac{\\partial  p_{mn}}{\\partial z_{mi}}\\frac{\\partial  z_{mi}}{\\partial w_{ij}}$ we achieve : \n",
    "    </font>\n",
    " <h7>   \n",
    "  <h4>\n",
    "     <font size=\"3\" color='#454214'>\n",
    "         $\\frac{\\partial L}{\\partial w_{ij}}=\\sum_k^m\\sum_n \\frac{y_{mn}}{p_{mn}}\\frac{\\partial  p_{mn}}{\\partial z_{mi}}\\frac{\\partial  z_{mi}}{\\partial w_{ij}}=\\sum_k^m\\sum_n^n \\frac{y_{mn}}{p_{mn}}p_{mn}(\\delta_{ni} -  p_{mi})x_{mj}$<br><br>\n",
    "     $ =\\sum_k^m\\sum_n y_{mn}(\\delta_{ni} -  p_{mi})x_{mj} $\n",
    "     $ =\\sum_k^m\\sum_n y_{mn}\\delta_{ni} - \\sum_k^m\\sum_n^n y_{mn} p_{mi}x_{mj} $ <br> <br>\n",
    "      $ =\\sum_k^m\\sum_n y_{mi}\\delta_{ii} - \\sum_k^m\\sum_n y_{mn} p_{mi}x_{mj} $ <br> <br>\n",
    "        $ =\\sum_k^m y_{mi} - \\sum_k^m\\sum_n y_{mn} p_{mi}x_{mj} $\n",
    "   </font>     \n",
    "<font size=\"3\" color='#263a61'> <br> <br>\n",
    " $!!!\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;y_{m1}+ ... +y_{mk}=1 \\;\\; because \\;y_{mk}\\; are\\; dummies \\;variable\\;and\\;their\\; z\\;\\sum_m y_{mk}\\;is\\;always\\;equal\\;to\\;1\\;by\\;deff.$</font> <br>\n",
    "<h4>    \n",
    " <font size=\"3\" color='#454214'> \n",
    "       $ =- (\\sum_k^m y_{mi} - \\sum_k^m 1. p_{mi}x_{mj} )= $\n",
    "       $ =+ \\sum_m  (p_{mi} - y_{mi} )x_{mj} )= $"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
